{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DocQA","text":"<p>https://lone17.github.io/docqa/</p> <p> </p> <p>Ask questions on your documents.</p> <p>This repo contains various tools for creating a document QA app from your text file to a RAG chatbot.</p>"},{"location":"#installation","title":"Installation","text":"<ul> <li><code>Require python &gt;= 3.10</code></li> <li>Get the source code</li> </ul> <pre><code># clone the repo (with a submodule)\ngit clone --recurse-submodules https://github.com/lone17/docqa.git\ncd docqa\n</code></pre> <ul> <li>It is recommended to create a virtual environment</li> </ul> <pre><code>python -m venv env\n. env/bin/activate\n</code></pre> <ul> <li>First let's install marker (following its instructions)</li> </ul> <pre><code>cd marker\n#  Install ghostscript &gt; 9.55 by following https://ghostscript.readthedocs.io/en/latest/Install.html\nscripts/install/ghostscript_install.sh\n# install other requirements\ncat scripts/install/apt-requirements.txt | xargs sudo apt-get install -y\npip install .\n</code></pre> <ul> <li>Then install docqa</li> </ul> <pre><code>cd ..\npip install -e .[dev]\n</code></pre>"},{"location":"#try-the-demo","title":"Try the Demo","text":"<p>This repo contains a demo for the whole pipeline for a QA chatbot on Generative Agents based on the information in this paper.</p> <p></p>"},{"location":"#from-source","title":"From source","text":"<p>Before playing with the demo, please populate your key and secrets in the <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=...\nOPENAI_MODEL=...\nOPENAI_SEED=...\nWANDB_API_KEY=...\n</code></pre> <p>All the scripts for the full pipeline as well as generated artifacts are in the <code>demo</code> folder.</p> <ul> <li><code>create_dataset.py</code>: This script handles the full data processing pipeline:</li> <li>parse the pdf file</li> <li>convert it to markdown</li> <li>chunk the content preserving structural content</li> <li>generate question-answers pairs</li> <li>prepare data for other steps: fine-tuning OpenAI models, adding to vector-stores.</li> <li><code>finetune_openai.py</code>: As the name suggests, this scripts is used to fine-tune the OpenAI model   using the data generated in <code>create_dataset.py</code>.</li> <li>Also includes Wandb logging.</li> <li><code>pipeline.py</code>: Declares the QA pipeline with semantic retrieval using ChromaDB.</li> </ul> <p>The <code>main.py</code> script is the endpoint for running the backend app:</p> <pre><code>python main.py\n</code></pre> <p>And to run the front end:</p> <pre><code>streamlit run frontend.py\n</code></pre>"},{"location":"#from-docker","title":"From Docker","text":"<p>Alternatively, you can get the image from Docker Hub here.</p> <pre><code>docker pull el117/docqa\ndocker run --rm -p 8000:8000 -e OPENAI_API_KEY=&lt;...&gt; el117/docqa\n</code></pre> <p>Note that the docker does not contain the front end. To run it you can simply do:</p> <pre><code>pip install streamlit\nstreamlit run frontend.py\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#data-pipeline","title":"Data pipeline","text":"<p>The diagram blow describes the data cycle. Result from each steps can be found at docqa/demo/data/generative_agent.</p> <pre><code>flowchart TB\n    subgraph pdf-to-md[PDF to Markdown]\n        direction RL\n        pdf[PDF] --&gt; raw-md(raw markdown)\n        raw-md --&gt; tidied-md([tidied markdown])\n    end\n\n    subgraph create-dataset[Create Dataset]\n        tidied-md --&gt; sections([Markdown sections])\n        sections --&gt; doc-tree([doc tree])\n        doc-tree --&gt; top-lv([top-level sections])\n        doc-tree --&gt; chunks([section-bounded\\nchunks])\n        top-lv --&gt; top-lv-qa([top-level sections\\nQA pairs])\n        top-lv-qa --&gt; finetune-data([fine-tuning data])\n        chunks --&gt; chunks-qa([chunk QA pairs])\n    end\n\n\n        finetune-data --&gt; lm{{language model}}\n\n        top-lv-qa --&gt; vector-store[(vector store)]\n        chunks-qa --&gt; vector-store</code></pre>"},{"location":"#app","title":"App","text":"<p>The diagram below describe the app's internal working, from receiving a question to answering it.</p> <pre><code>flowchart LR\n    query(query) --&gt; retriever\n\n    subgraph retriever[Semantic Retriever]\n        direction LR\n        doc-tree([doc tree])\n        vector-store[(vector store)]\n        emb{{embedding model}}\n    end\n\n    retriever --&gt; ref([references])\n\n    ref --&gt; thresh{similarity &gt; threshold}\n    query --&gt; thresh\n\n    thresh -- true --&gt; answer(((answer)))\n\n    thresh -- false --&gt; answerer\n    query --&gt; answerer\n    answerer --&gt; answer\n\n    subgraph answerer[AnswerGenerator]\n        lm{{language model}}\n    end</code></pre>"},{"location":"reference/","title":"API Reference","text":"<ul> <li>Core<ul> <li>Chunking</li> <li>Data Generation</li> <li>Data Validation</li> <li>Doc Tree</li> <li>Markdown</li> <li>Retrieval</li> </ul> </li> <li>Demo<ul> <li>Config</li> <li>Create Dataset</li> <li>Finetune Openai</li> <li>Pipeline</li> </ul> </li> <li>Types</li> </ul>"},{"location":"reference/types/","title":"Types","text":""},{"location":"reference/types/#types.ChatRequest","title":"ChatRequest","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a chat request.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message from user.</p> required <code>openai_model</code> <code>str</code> <p>The OpenAI model to use. Defaults to \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\".</p> required <code>temperature</code> <code>float</code> <p>The temperature of the model's output. Higher values make the output more random, while lower values make it more focused and deterministic. Defaults to 1.0.</p> required <code>certainty_threshold</code> <code>float</code> <p>The threshold for considering a question as certain. Defaults to 0.9.</p> required <code>uncertainty_threshold</code> <code>float</code> <p>The threshold for considering a question as uncertain. Defaults to 0.6.</p> required Source code in <code>docqa/types.py</code> <pre><code>class ChatRequest(BaseModel):\n    \"\"\"\n    Represents a chat request.\n\n    Args:\n        message (str): The message from user.\n        openai_model (str, optional): The OpenAI model to use. Defaults to\n            \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\".\n        temperature (float, optional): The temperature of the model's output.\n            Higher values make the output more random, while lower values make it\n            more focused and deterministic. Defaults to 1.0.\n        certainty_threshold (float, optional): The threshold for considering a\n            question as certain. Defaults to 0.9.\n        uncertainty_threshold (float, optional): The threshold for considering a\n            question as uncertain. Defaults to 0.6.\n    \"\"\"\n\n    message: str\n    openai_model: str = \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\"\n    temperature: float = 1.0\n    certainty_threshold: float = 0.9\n    uncertainty_threshold: float = 0.6\n</code></pre>"},{"location":"reference/types/#types.RetrievalReference","title":"RetrievalReference","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a reference.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the reference.</p> required <code>content</code> <code>str</code> <p>The content of the reference.</p> required Source code in <code>docqa/types.py</code> <pre><code>class RetrievalReference(BaseModel):\n    \"\"\"\n    Represents a reference.\n\n    Args:\n        source (str): The source of the reference.\n        content (str): The content of the reference.\n    \"\"\"\n\n    source: str\n    content: str\n</code></pre>"},{"location":"reference/types/#types.PipelineOutput","title":"PipelineOutput","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents the output of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>str</code> <p>The answer to the question.</p> required <code>references</code> <code>list[RetrievalReference]</code> <p>A list of RetrievalReference objects containing the related references.</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata associated with the answer.</p> required Source code in <code>docqa/types.py</code> <pre><code>class PipelineOutput(BaseModel):\n    \"\"\"\n    Represents the output of the pipeline.\n\n    Args:\n        answer (str): The answer to the question.\n        references (list[RetrievalReference]): A list of RetrievalReference objects\n            containing the related references.\n        metadata (dict): Additional metadata associated with the answer.\n    \"\"\"\n\n    answer: str\n    references: list[RetrievalReference]\n    metadata: dict | None = None\n</code></pre>"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/core/chunking/","title":"Chunking","text":""},{"location":"reference/core/chunking/#core.chunking.chunk_content","title":"chunk_content","text":"<pre><code>chunk_content(content, single_threshold=100, composite_threshold=200)\n</code></pre> <p>Generate a list of content chunks from a given string. The function will only split at a new line and never in the middle of a sentence. Which means it tries its best to preserve the structure of the original text.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The input string to be chunked.</p> required <code>single_threshold</code> <code>int</code> <p>The minimum length of a single chunk. Defaults to 100.</p> <code>100</code> <code>composite_threshold</code> <code>int</code> <p>The maximum length of a composite chunk. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of content chunks.</p> Description <ul> <li>This function takes a string <code>content</code> and splits it into smaller chunks based on the specified thresholds.</li> <li>It first splits the string into parts using the newline and carriage return characters as delimiters.</li> <li>It then iterates over each part and checks if the length of the part exceeds the <code>single_threshold</code>.</li> <li>If it does, it is considered a paragraph and added as a separate chunk.</li> <li>If the length of the current chunk exceeds the <code>composite_threshold</code>, it is also added as a separate chunk.</li> <li>Finally, the function returns a list of all the generated chunks.</li> </ul> Example <p><pre><code>content = (\n    # long paragraph\n    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit ... \\n\"\n    \"A quick brown fox jumps over the lazy dog.\n)\nchunk_content(content)\n</code></pre> <pre><code>['Lorem ipsum dolor sit amet, consectetur adipiscing elit ...',\n'A quick brown fox jumps over the lazy dog.']\n</code></pre></p> Source code in <code>docqa/core/chunking.py</code> <pre><code>def chunk_content(\n    content: str, single_threshold: int = 100, composite_threshold: int = 200\n) -&gt; list[str]:\n    \"\"\"\n    Generate a list of content chunks from a given string.\n    The function will only split at a new line and never in the middle of a sentence.\n    Which means it tries its best to preserve the structure of the original text.\n\n    Args:\n        content (str): The input string to be chunked.\n        single_threshold (int, optional): The minimum length of a single chunk.\n            Defaults to 100.\n        composite_threshold (int, optional): The maximum length of a composite chunk.\n            Defaults to 200.\n\n    Returns:\n        list[str]: A list of content chunks.\n\n    Description:\n        - This function takes a string `content` and splits it into smaller chunks based\n        on the specified thresholds.\n        - It first splits the string into parts using the newline and carriage return\n        characters as delimiters.\n        - It then iterates over each part and checks if the length of the part exceeds\n        the `single_threshold`.\n        - If it does, it is considered a paragraph and added as a separate chunk.\n        - If the length of the current chunk exceeds the `composite_threshold`, it is\n        also added as a separate chunk.\n        - Finally, the function returns a list of all the generated chunks.\n\n    Example:\n        ```python\n        content = (\n            # long paragraph\n            \"Lorem ipsum dolor sit amet, consectetur adipiscing elit ... \\\\n\"\n            \"A quick brown fox jumps over the lazy dog.\n        )\n        chunk_content(content)\n        ```\n        ```bash\n        ['Lorem ipsum dolor sit amet, consectetur adipiscing elit ...',\n        'A quick brown fox jumps over the lazy dog.']\n        ```\n    \"\"\"\n    if not content:\n        return []\n\n    parts = re.split(r\"[\\n\\r]+\", content)\n\n    chunks = []\n    cur_chunk: deque = deque([])\n    cur_length = 0\n\n    for text in parts:\n        text_length = len(text.split())\n        # if found a text big enough, this could be a paragraph\n        if text_length &gt; single_threshold:\n            # add the current chunk\n            if cur_chunk:\n                chunks.append(\"\\n\".join(cur_chunk))\n            cur_chunk = deque([])\n            cur_length = 0\n            # then add the found paragraph\n            chunks.append(text)\n        else:\n            # extend the current chunk\n            cur_chunk.append(text)\n            cur_length += text_length\n            # if the current chunk is big enough\n            if cur_length &gt; composite_threshold:\n                chunks.append(\"\\n\".join(cur_chunk))\n                # then shorten it down from the left\n                while cur_length &gt; single_threshold:\n                    discard = cur_chunk.popleft()\n                    cur_length -= len(discard.split())\n\n    # the last chunk\n    if cur_chunk:\n        chunks.append(\"\\n\".join(cur_chunk))\n\n    return chunks\n</code></pre>"},{"location":"reference/core/chunking/#core.chunking.chunk_size_stats","title":"chunk_size_stats","text":"<pre><code>chunk_size_stats(sections)\n</code></pre> <p>Calculates the statistics of the chunk sizes in the given list of sections.</p> Description <p>This function calculates the statistics of the chunk sizes in the given list of sections. It iterates through each section and splits the content into paragraphs using \"\\n\\n\" as the delimiter. It then calculates the length of each paragraph by splitting it into words and stores them in the <code>paragraph_lengths</code> list. After that, it filters out the paragraph lengths that are less than or equal to 100.</p> <p>Next, it prints the average paragraph length by calculating the sum of all paragraph lengths and dividing it by the number of paragraph lengths. It then prints the 90th percentile paragraph length by sorting the paragraph lengths in ascending order and selecting the index that corresponds to 90% of the length of the list.</p> <p>The function then initializes an empty dictionary <code>sections_details</code> to store the details of each section. It iterates through each section and checks if the header matches any of the predefined keywords. If it does, it initializes an empty list <code>chunks</code>, otherwise it calls the <code>chunk_content</code> function to chunk the content and assigns the result to <code>chunks</code>. It then adds the details of the section to the <code>sections_details</code> dictionary.</p> <p>Finally, it prints the total number of chunks by summing the lengths of the <code>chunks</code> list for each section in <code>sections_details</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples containing a header and content for each section. The content is a string.</p> required Source code in <code>docqa/core/chunking.py</code> <pre><code>def chunk_size_stats(sections: list[tuple[str, str]]):\n    \"\"\"Calculates the statistics of the chunk sizes in the given list of sections.\n\n    Description:\n        This function calculates the statistics of the chunk sizes in the given list of\n        sections. It iterates through each section and splits the content into\n        paragraphs using \"\\\\n\\\\n\" as the delimiter. It then calculates the length of\n        each paragraph by splitting it into words and stores them in the\n        `paragraph_lengths` list. After that, it filters out the paragraph lengths\n        that are less than or equal to 100.\n\n        Next, it prints the average paragraph length by calculating the sum of all\n        paragraph lengths and dividing it by the number of paragraph lengths. It then\n        prints the 90th percentile paragraph length by sorting the paragraph lengths in\n        ascending order and selecting the index that corresponds to 90% of the length\n        of the list.\n\n        The function then initializes an empty dictionary `sections_details` to store\n        the details of each section. It iterates through each section and checks if the\n        header matches any of the predefined keywords. If it does, it initializes an\n        empty list `chunks`, otherwise it calls the `chunk_content` function to chunk\n        the content and assigns the result to `chunks`. It then adds the details of the\n        section to the `sections_details` dictionary.\n\n        Finally, it prints the total number of chunks by summing the lengths of the\n        `chunks` list for each section in `sections_details`.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples containing a header and\n            content for each section. The content is a string.\n\n    \"\"\"\n    paragraph_lengths = []\n    for header, content in sections:\n        paragraphs = content.split(\"\\n\\n\")\n        paragraph_lengths.extend([len(p.split()) for p in paragraphs])\n\n    paragraph_lengths = [length for length in paragraph_lengths if length &gt; 100]\n    print(\"average paragraph length:\", sum(paragraph_lengths) / len(paragraph_lengths))\n    print(\n        \"90 percentile paragraph length:\",\n        sorted(paragraph_lengths)[int(len(paragraph_lengths) * 0.9)],\n    )\n\n    sections_details = {}\n    for header, content in sections:\n        if re.sub(r\"[^a-zA-Z0-9]\", \"\", header.lower()) in (\n            \"reference\",\n            \"references\",\n            \"acknowledgement\",\n            \"acknowledgements\",\n        ):\n            chunks = []\n        else:\n            chunks = chunk_content(content)\n        sections_details[header] = {\n            \"content\": content,\n            \"chunks\": chunks,\n        }\n    print(\n        \"Total number of chunks:\",\n        sum(len(sec[\"chunks\"]) for sec in sections_details.values()),\n    )\n</code></pre>"},{"location":"reference/core/data_generation/","title":"Data generation","text":""},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator","title":"QAPairGenerator","text":"<p>             Bases: <code>BaseModel</code></p> <p>Generates questions and answers for sections and subsections of a document.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The API key for OpenAI.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required Source code in <code>docqa/core/data_generation.py</code> <pre><code>class QAPairGenerator(BaseModel):\n    \"\"\"\n    Generates questions and answers for sections and subsections of a document.\n\n    Args:\n        openai_key (str): The API key for OpenAI.\n        openai_model (str): The name of the OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    output_format: str = (\n        \"Present your questions along with the detailed answers in the following JSON\"\n        ' format: [{\"question\": str, \"answer\": str}, ...].'\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def question_types(self) -&gt; dict[str, dict[str, str]]:\n        return {\n            \"sparse\": {\n                \"system_message\": (\n                    \"You are a professional examiner. Your job is to give questions to\"\n                    \" test people's understanding of a given document.\"\n                ),\n                \"instruction\": (\n                    \"You are given a document, your goal is:\\n- construct a list of\"\n                    \" different complex questions that can be answered based **solely**\"\n                    \" on the given text.\\n- make sure to cover all of the topics\"\n                    \" described in the document.\\n- include the answer for each\"\n                    \" question, the answers should be as detailed as\"\n                    \" possible.\\n\"\n                ),\n            },\n            \"dense\": {\n                \"system_message\": \"You are a top university professor.\",\n                \"instruction\": (\n                    \"You are a top university professor. You have the below text and\"\n                    \" you want to test the student's understanding of it. If you can\"\n                    \" only ask {num_questions} question(s) but must cover all of the\"\n                    \" content and the answer(s) to those questions must contain\"\n                    \" **solely** the information presented in the given text, what\"\n                    \" would you ask?\\n\"\n                ),\n            },\n        }\n\n    @staticmethod\n    def sanitize_output_format(output: dict | list) -&gt; list[dict]:\n        \"\"\"This static method takes in an `output` of type `dict` or `list` and returns\n            a sanitized `list[dict]` output.\n\n        Args:\n            output (dict | list): The input `output` that needs to be sanitized.\n\n        Returns:\n            list[dict]: The sanitized output as a list of dictionaries.\n\n        Raises:\n            ValueError: If the `output` format is invalid.\n\n        \"\"\"\n        if isinstance(output, dict):\n            if list(output.keys()) == [\"questions\"]:\n                if isinstance(output[\"questions\"], list):\n                    output = output[\"questions\"]\n            else:\n                output = [output]\n        elif isinstance(output, list):\n            if list(output[0].keys()) == [\"questions\"]:\n                output = output[0][\"questions\"]\n        else:\n            raise ValueError(f\"Invalid output format: {type(output)}\")\n\n        return output  # type: ignore[return-value]\n\n    def process(\n        self,\n        document: str,\n        temperature: float = 1.0,\n        question_type: str = \"dense\",\n        num_questions: int = 5,\n    ) -&gt; tuple[list[dict[str, str]], list[dict]]:\n        \"\"\"\n        Process the given document to generate a list of questions and answers.\n\n        Args:\n            document (str): The text document to process.\n            temperature (float, optional): The temperature parameter for controlling\n                the randomness of the output. Defaults to 1.0.\n            question_type (str, optional): The type of questions to generate.\n                Defaults to \"dense\".\n            num_questions (int, optional): The number of questions to generate.\n                Defaults to 5.\n\n        Returns:\n            tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of\n                questions and answers and a list of metadata.\n\n        Raises:\n            ValueError: If an invalid question type is provided.\n        \"\"\"\n        if question_type not in self.question_types:\n            raise ValueError(f\"Invalid question type: {question_type}\")\n\n        system_message = self.question_types[question_type][\"system_message\"]\n        instruction = self.question_types[question_type][\"instruction\"].format(\n            num_questions=num_questions\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\n                \"role\": \"user\",\n                \"content\": instruction + self.output_format,\n            },\n            {\"role\": \"user\", \"content\": \"\\nHere is the given text:\\n\\n\" + document},\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n            response_format={\"type\": \"json_object\"},\n        )\n        text = response.choices[0].message.content.strip()\n\n        output = json.loads(text)\n        output = self.sanitize_output_format(output)\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = response.usage.model_dump()\n\n        return output, metadata  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator.sanitize_output_format","title":"sanitize_output_format  <code>staticmethod</code>","text":"<pre><code>sanitize_output_format(output)\n</code></pre> <p>This static method takes in an <code>output</code> of type <code>dict</code> or <code>list</code> and returns     a sanitized <code>list[dict]</code> output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>dict | list</code> <p>The input <code>output</code> that needs to be sanitized.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The sanitized output as a list of dictionaries.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>output</code> format is invalid.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>@staticmethod\ndef sanitize_output_format(output: dict | list) -&gt; list[dict]:\n    \"\"\"This static method takes in an `output` of type `dict` or `list` and returns\n        a sanitized `list[dict]` output.\n\n    Args:\n        output (dict | list): The input `output` that needs to be sanitized.\n\n    Returns:\n        list[dict]: The sanitized output as a list of dictionaries.\n\n    Raises:\n        ValueError: If the `output` format is invalid.\n\n    \"\"\"\n    if isinstance(output, dict):\n        if list(output.keys()) == [\"questions\"]:\n            if isinstance(output[\"questions\"], list):\n                output = output[\"questions\"]\n        else:\n            output = [output]\n    elif isinstance(output, list):\n        if list(output[0].keys()) == [\"questions\"]:\n            output = output[0][\"questions\"]\n    else:\n        raise ValueError(f\"Invalid output format: {type(output)}\")\n\n    return output  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator.process","title":"process","text":"<pre><code>process(document, temperature=1.0, question_type='dense', num_questions=5)\n</code></pre> <p>Process the given document to generate a list of questions and answers.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>str</code> <p>The text document to process.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling the randomness of the output. Defaults to 1.0.</p> <code>1.0</code> <code>question_type</code> <code>str</code> <p>The type of questions to generate. Defaults to \"dense\".</p> <code>'dense'</code> <code>num_questions</code> <code>int</code> <p>The number of questions to generate. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple[list[dict[str, str]], list[dict]]</code> <p>tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of questions and answers and a list of metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid question type is provided.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def process(\n    self,\n    document: str,\n    temperature: float = 1.0,\n    question_type: str = \"dense\",\n    num_questions: int = 5,\n) -&gt; tuple[list[dict[str, str]], list[dict]]:\n    \"\"\"\n    Process the given document to generate a list of questions and answers.\n\n    Args:\n        document (str): The text document to process.\n        temperature (float, optional): The temperature parameter for controlling\n            the randomness of the output. Defaults to 1.0.\n        question_type (str, optional): The type of questions to generate.\n            Defaults to \"dense\".\n        num_questions (int, optional): The number of questions to generate.\n            Defaults to 5.\n\n    Returns:\n        tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of\n            questions and answers and a list of metadata.\n\n    Raises:\n        ValueError: If an invalid question type is provided.\n    \"\"\"\n    if question_type not in self.question_types:\n        raise ValueError(f\"Invalid question type: {question_type}\")\n\n    system_message = self.question_types[question_type][\"system_message\"]\n    instruction = self.question_types[question_type][\"instruction\"].format(\n        num_questions=num_questions\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\n            \"role\": \"user\",\n            \"content\": instruction + self.output_format,\n        },\n        {\"role\": \"user\", \"content\": \"\\nHere is the given text:\\n\\n\" + document},\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n        response_format={\"type\": \"json_object\"},\n    )\n    text = response.choices[0].message.content.strip()\n\n    output = json.loads(text)\n    output = self.sanitize_output_format(output)\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = response.usage.model_dump()\n\n    return output, metadata  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.AnswerGenerator","title":"AnswerGenerator","text":"<p>             Bases: <code>BaseModel</code></p> <p>Generate an answer to a question based on a reference.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The OpenAI API key.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required Source code in <code>docqa/core/data_generation.py</code> <pre><code>class AnswerGenerator(BaseModel):\n    \"\"\"Generate an answer to a question based on a reference.\n\n    Args:\n        openai_key (str): The OpenAI API key.\n        openai_model (str): The name of the OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    system_message: str = (\n        \"You are a trusted factual chatbot. You always answer questions based strictly\"\n        \" on the provided reference.\"\n    )\n    instruction: str = (\n        \"Reference(s):\\n\\n{reference}\\n\\nStrictly according to the provided\"\n        \" reference(s), give an answer as detailed as possible to the following\"\n        \" question: {question}\"\n    )\n\n    def process(\n        self,\n        question: str,\n        reference: str,\n        temperature: float = 1.0,\n    ) -&gt; tuple[str, dict]:\n        \"\"\"\n        Process the given question and generate a response using the OpenAI model.\n\n        Parameters:\n            question (str): The question to be processed.\n            reference (str): The reference string for the instruction.\n            temperature (float, optional): The temperature parameter for generating the\n                response. Higher values (e.g., 1.0) make the output more random, while\n                lower values (e.g., 0.2) make it more focused and deterministic.\n                Defaults to 1.0.\n\n        Returns:\n            Tuple[str, dict]: A tuple containing the generated answer and metadata.\n\n        Output dict structure:\n            - answer (str): The generated answer as a string.\n            - metadata (dict): Additional metadata about the response.\n                - finish_reason (str): The reason why the completion finished.\n                - usage (dict): Usage statistics of the completion.\n                    - completed_tokens (int): The number of tokens used for\n                        completion.\n                    - prompt_tokens (int): The number of tokens used for the prompt.\n                    - total_tokens (int): The total number of tokens used.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_message},\n            {\n                \"role\": \"user\",\n                \"content\": self.instruction.format(\n                    reference=reference, question=question\n                ),\n            },\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n        )\n        answer = response.choices[0].message.content.strip()\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = {\n            \"completed_tokens\": response.usage.completion_tokens,\n            \"prompt_tokens\": response.usage.prompt_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n        return answer, metadata\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.AnswerGenerator.process","title":"process","text":"<pre><code>process(question, reference, temperature=1.0)\n</code></pre> <p>Process the given question and generate a response using the OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to be processed.</p> required <code>reference</code> <code>str</code> <p>The reference string for the instruction.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for generating the response. Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple[str, dict]</code> <p>Tuple[str, dict]: A tuple containing the generated answer and metadata.</p> Output dict structure <ul> <li>answer (str): The generated answer as a string.</li> <li>metadata (dict): Additional metadata about the response.<ul> <li>finish_reason (str): The reason why the completion finished.</li> <li>usage (dict): Usage statistics of the completion.<ul> <li>completed_tokens (int): The number of tokens used for     completion.</li> <li>prompt_tokens (int): The number of tokens used for the prompt.</li> <li>total_tokens (int): The total number of tokens used.</li> </ul> </li> </ul> </li> </ul> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def process(\n    self,\n    question: str,\n    reference: str,\n    temperature: float = 1.0,\n) -&gt; tuple[str, dict]:\n    \"\"\"\n    Process the given question and generate a response using the OpenAI model.\n\n    Parameters:\n        question (str): The question to be processed.\n        reference (str): The reference string for the instruction.\n        temperature (float, optional): The temperature parameter for generating the\n            response. Higher values (e.g., 1.0) make the output more random, while\n            lower values (e.g., 0.2) make it more focused and deterministic.\n            Defaults to 1.0.\n\n    Returns:\n        Tuple[str, dict]: A tuple containing the generated answer and metadata.\n\n    Output dict structure:\n        - answer (str): The generated answer as a string.\n        - metadata (dict): Additional metadata about the response.\n            - finish_reason (str): The reason why the completion finished.\n            - usage (dict): Usage statistics of the completion.\n                - completed_tokens (int): The number of tokens used for\n                    completion.\n                - prompt_tokens (int): The number of tokens used for the prompt.\n                - total_tokens (int): The total number of tokens used.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.system_message},\n        {\n            \"role\": \"user\",\n            \"content\": self.instruction.format(\n                reference=reference, question=question\n            ),\n        },\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n    )\n    answer = response.choices[0].message.content.strip()\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = {\n        \"completed_tokens\": response.usage.completion_tokens,\n        \"prompt_tokens\": response.usage.prompt_tokens,\n        \"total_tokens\": response.usage.total_tokens,\n    }\n\n    return answer, metadata\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.generate_top_sections_questions","title":"generate_top_sections_questions","text":"<pre><code>generate_top_sections_questions(doc_tree, output_file, openai_key='', openai_model='', seed=42, temperature=1.0)\n</code></pre> <p>Generate the top sections with questions based on the provided document tree.</p> <p>Parameters:</p> Name Type Description Default <code>doc_tree</code> <code>dict</code> <p>The document tree representing the sections of the document.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the top sections with uestions will be saved.</p> required <code>openai_key</code> <code>str</code> <p>The OpenAI API key. Defaults to an empty string.</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The OpenAI model to use for question generation. Defaults to an empty string.</p> <code>''</code> <code>seed</code> <code>int</code> <p>The seed value for random number generation. Defaults to 42.</p> <code>42</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for question generation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The top sections with questions.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def generate_top_sections_questions(\n    doc_tree: dict,\n    output_file: Path,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    temperature: float = 1.0,\n) -&gt; dict:\n    \"\"\"\n    Generate the top sections with questions based on the provided document tree.\n\n    Args:\n        doc_tree (dict): The document tree representing the sections of the document.\n        output_file (Path): The path to the output file where the top sections with\n            uestions will be saved.\n        openai_key (str, optional): The OpenAI API key. Defaults to an empty string.\n        openai_model (str, optional): The OpenAI model to use for question generation.\n            Defaults to an empty string.\n        seed (int, optional): The seed value for random number generation.\n            Defaults to 42.\n        temperature (float, optional): The temperature parameter for question\n            generation.\n            Defaults to 1.0.\n\n    Returns:\n        dict: The top sections with questions.\n    \"\"\"\n    output_file = Path(output_file)\n    if output_file.exists():\n        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n            top_sections_with_questions = json.load(f)\n        return top_sections_with_questions\n\n    qa_gen = QAPairGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        seed=seed,\n    )\n\n    top_sections_with_questions = {}\n    if doc_tree[\"text\"]:\n        top_sections_with_questions[\"\"] = {\n            \"text\": doc_tree[\"text\"],\n            \"chunks_count\": len(chunk_content(doc_tree[\"text\"])),\n        }\n\n    for section in doc_tree.get(\"child_sections\", []):\n        full_text = get_section_full_text(section)\n        top_sections_with_questions[section[\"heading\"]] = {\n            \"text\": full_text,\n            \"chunks_count\": len(chunk_content(full_text)),\n        }\n\n    for heading, section in top_sections_with_questions.items():\n        print(f\"Generating questions for {heading}\")\n        dense_questions, _ = qa_gen.process(\n            section[\"text\"],\n            question_type=\"dense\",\n            num_questions=section[\"chunks_count\"],\n            temperature=temperature,\n        )\n        sparse_questions, _ = qa_gen.process(\n            section[\"text\"],\n            question_type=\"sparse\",\n            temperature=temperature,\n        )\n        top_sections_with_questions[heading][\"dense_questions\"] = dense_questions\n        top_sections_with_questions[heading][\"sparse_questions\"] = sparse_questions\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(top_sections_with_questions, f, indent=4, ensure_ascii=False)\n\n    return top_sections_with_questions\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.generate_long_answers_for_sections_questions","title":"generate_long_answers_for_sections_questions","text":"<pre><code>generate_long_answers_for_sections_questions(sections_with_questions, output_file, openai_key='', openai_model='', seed=42, temperature=1.0)\n</code></pre> <p>Generate long answers for sections' questions.</p> <p>Parameters:</p> Name Type Description Default <code>sections_with_questions</code> <code>dict</code> <p>A dictionary containing sections with their corresponding questions.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the generated long answers will be stored.</p> required <code>openai_key</code> <code>str</code> <p>The API key for OpenAI. Defaults to an empty string.</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use. Defaults to an empty string.</p> <code>''</code> <code>seed</code> <code>int</code> <p>The seed value for random number generation. Defaults to 42.</p> <code>42</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for generating answers. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing sections with their corresponding questions and generated long answers.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def generate_long_answers_for_sections_questions(\n    sections_with_questions: dict,\n    output_file: Path,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    temperature: float = 1.0,\n) -&gt; dict:\n    \"\"\"\n    Generate long answers for sections' questions.\n\n    Args:\n        sections_with_questions (dict): A dictionary containing sections with their\n            corresponding questions.\n        output_file (Path): The path to the output file where the generated long answers\n            will be stored.\n        openai_key (str, optional): The API key for OpenAI. Defaults to an empty string.\n        openai_model (str, optional): The name of the OpenAI model to use. Defaults to\n            an empty string.\n        seed (int, optional): The seed value for random number generation.\n            Defaults to 42.\n        temperature (float, optional): The temperature parameter for generating answers.\n            Defaults to 1.0.\n\n    Returns:\n        dict: A dictionary containing sections with their corresponding questions and\n            generated long answers.\n    \"\"\"\n    output_file = Path(output_file)\n    if output_file.exists():\n        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n            sections_with_questions_and_long_answers = json.load(f)\n        return sections_with_questions_and_long_answers\n\n    answer_gen = AnswerGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        seed=seed,\n    )\n    for heading, section in sections_with_questions.items():\n        print(f\"Generating long answers for dense questions of {heading}\")\n        reference = f\"===\\n[source: {heading}]\\n{section['text']}\\n===\\n\"\n        import pdb\n\n        pdb.set_trace()\n        for question in section[\"dense_questions\"]:\n            answer, _ = answer_gen.process(\n                question=question[\"question\"],\n                reference=reference,\n                temperature=temperature,\n            )\n            question[\"long_answer\"] = answer\n\n        print(f\"Generating long answers for sparse questions of {heading}\")\n        for question in section[\"sparse_questions\"]:\n            answer, _ = answer_gen.process(\n                question=question[\"question\"],\n                reference=reference,\n                temperature=temperature,\n            )\n            question[\"long_answer\"] = answer\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(sections_with_questions, f, indent=4, ensure_ascii=False)\n\n    return sections_with_questions\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.make_simple_sample_for_openai","title":"make_simple_sample_for_openai","text":"<pre><code>make_simple_sample_for_openai(question, answer)\n</code></pre> <p>Generates a simple sample for OpenAI chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The user's question.</p> required <code>answer</code> <code>str</code> <p>The assistant's answer.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the chat conversation sample.</p> Example <p>make_simple_sample_for_openai(\"What is the capital of France?\", \"Paris\")</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def make_simple_sample_for_openai(question: str, answer: str) -&gt; dict:\n    \"\"\"\n    Generates a simple sample for OpenAI chat conversation.\n\n    Args:\n        question (str): The user's question.\n        answer (str): The assistant's answer.\n\n    Returns:\n        dict: A dictionary containing the chat conversation sample.\n\n    Example:\n        make_simple_sample_for_openai(\"What is the capital of France?\", \"Paris\")\n    \"\"\"\n    return {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a trusted factual chatbot that only answers questions\"\n                    \" about generative agents.\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n    }\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.make_instruction_sample_for_openai","title":"make_instruction_sample_for_openai","text":"<pre><code>make_instruction_sample_for_openai(question, answer, references)\n</code></pre> <p>Generates a function comment for the given function body.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to be used in the instruction.</p> required <code>answer</code> <code>str</code> <p>The answer to be used in the instruction.</p> required <code>references</code> <code>list[str]</code> <p>A list of reference texts to be included in the instruction.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The generated function comment in the form of a dictionary.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def make_instruction_sample_for_openai(\n    question: str, answer: str, references: list[str]\n) -&gt; dict:\n    \"\"\"\n    Generates a function comment for the given function body.\n\n    Args:\n        question (str): The question to be used in the instruction.\n        answer (str): The answer to be used in the instruction.\n        references (list[str]): A list of reference texts to be included in the\n            instruction.\n\n    Returns:\n        dict: The generated function comment in the form of a dictionary.\n    \"\"\"\n    reference_text = \"\\n\\n\".join([\"===\\n\" + ref + \"\\n===\" for ref in references])\n    system_message = AnswerGenerator.model_fields[\"system_message\"].default\n    instruction = AnswerGenerator.model_fields[\"instruction\"].default\n    instruction = instruction.format(reference=reference_text, question=question)\n    return {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": system_message,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": instruction,\n            },\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n    }\n</code></pre>"},{"location":"reference/core/data_validation/","title":"Data validation","text":"<p>Source: https://cookbook.openai.com/examples/chat_finetuning_data_prep</p>"},{"location":"reference/core/doc_tree/","title":"Doc tree","text":""},{"location":"reference/core/doc_tree/#core.doc_tree.build_doc_tree_from_markdown","title":"build_doc_tree_from_markdown","text":"<pre><code>build_doc_tree_from_markdown(text)\n</code></pre> <p>Takes a string representation of a markdown file as input. Finds the highest level of heading and splits the text into sections accordingly. Returns a list of tuples, each containing the section title and section content.</p> <pre><code>{\n    \"heading\": \"Section 1\",\n    \"text\": \"Section 1 opening text\",\n    \"child_sections\": [\n        {\n            \"heading\": \"Section 1.1\",\n            \"text\": \"Section 1.1 opening text\",\n            \"child_sections\": [\n                ...\n            ]\n        },\n        ...\n    ]\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The content of a markdown file as a single string.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the tree structure of the markdown file.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def build_doc_tree_from_markdown(\n    text: str,\n) -&gt; dict:\n    \"\"\"\n    Takes a string representation of a markdown file as input.\n    Finds the highest level of heading and splits the text into sections accordingly.\n    Returns a list of tuples, each containing the section title and section content.\n\n    ```python\n    {\n        \"heading\": \"Section 1\",\n        \"text\": \"Section 1 opening text\",\n        \"child_sections\": [\n            {\n                \"heading\": \"Section 1.1\",\n                \"text\": \"Section 1.1 opening text\",\n                \"child_sections\": [\n                    ...\n                ]\n            },\n            ...\n        ]\n    }\n    ```\n\n    Args:\n        text (str): The content of a markdown file as a single string.\n\n    Returns:\n        dict: A dictionary containing the tree structure of the markdown file.\n\n    \"\"\"\n    lines = text.strip().split(\"\\n\")\n\n    # Find the highest heading level\n    highest_heading_level = find_highest_markdown_heading_level(lines)\n\n    # If there are no headings, return the text as a single section\n    if highest_heading_level is None:\n        return {\"heading\": \"\", \"text\": text}\n\n    # Construct the heading prefix for splitting\n    headings_prefix = (\"#\" * highest_heading_level) + \" \"\n\n    n = len(lines)\n    i = 0\n    opening_text_lines = []\n    while i &lt; n and not lines[i].startswith(headings_prefix):\n        opening_text_lines.append(lines[i])\n        i += 1\n\n    root = {\n        \"heading\": \"\",\n        \"text\": \"\\n\".join(opening_text_lines).strip(),\n        \"child_sections\": [],\n    }\n\n    current_section_title = \"\"\n    current_section_lines: list[str] = []\n\n    # Split the text at the highest heading level\n    while i &lt; n:\n        line = lines[i]\n        # Check if the line starts with the highest heading level prefix\n        if line.startswith(headings_prefix):\n            # If the current_section is not empty, add it to the sections list\n            if len(current_section_lines) &gt; 0:\n                current_section_body = \"\\n\".join(current_section_lines).strip()\n                child_section = build_doc_tree_from_markdown(current_section_body)\n                child_section[\"heading\"] = current_section_title\n                root[\"child_sections\"].append(child_section)  # type: ignore\n\n            # Update the current_section_title and clear the current_section\n            current_section_title = line.strip()\n            current_section_lines = []\n        else:\n            # Add the line to the current_section\n            current_section_lines.append(line)\n        i += 1\n\n    # Add the last section to the sections list (if not empty)\n    if len(current_section_lines) &gt; 0:\n        current_section_body = \"\\n\".join(current_section_lines).strip()\n        child_section = build_doc_tree_from_markdown(current_section_body)\n        child_section[\"heading\"] = current_section_title\n        root[\"child_sections\"].append(child_section)  # type: ignore[attr-defined]\n\n    return root\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.build_doc_tree_from_pdf","title":"build_doc_tree_from_pdf","text":"<pre><code>build_doc_tree_from_pdf(input_file, output_dir)\n</code></pre> <p>Generate a document tree from a PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>The path to the input PDF file.</p> required <code>output_dir</code> <code>Path</code> <p>The directory where the output files will be saved.</p> required Notes <ul> <li>The function first checks if the marker output file exists in the output     directory.</li> <li>If the marker output file exists, it reads the content of the file.</li> <li>If the marker output file does not exist, it converts the input PDF file to     markdown using the <code>pdf_to_markdown</code> function.</li> <li>The function then checks if the tidy text sections file exists in the output     directory.</li> <li>If the tidy text sections file exists, it reads the content of the file.</li> <li>If the tidy text sections file does not exist, it builds a document tree from     the marker markdown content using the <code>build_doc_tree_from_markdown</code>     function.</li> <li>The function flattens the document tree using the <code>flatten_doc_tree</code> function.</li> <li>It preprocesses the sections using the <code>preprocess_sections</code> function.</li> <li>The function then tidies the markdown sections and retrieves the metadata     using the <code>tidy_markdown_sections</code> function.</li> <li>Finally, it saves the tidy text sections to a file, writes the tidy markdown     content to a file, and saves the metadata to a file.</li> </ul> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The final document tree generated from the PDF.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the marker output file or tidy text sections file does not exist.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def build_doc_tree_from_pdf(input_file: Path, output_dir: Path) -&gt; dict:\n    \"\"\"\n    Generate a document tree from a PDF file.\n\n    Args:\n        input_file (Path): The path to the input PDF file.\n        output_dir (Path): The directory where the output files will be saved.\n\n    Notes:\n        - The function first checks if the marker output file exists in the output\n            directory.\n        - If the marker output file exists, it reads the content of the file.\n        - If the marker output file does not exist, it converts the input PDF file to\n            markdown using the `pdf_to_markdown` function.\n        - The function then checks if the tidy text sections file exists in the output\n            directory.\n        - If the tidy text sections file exists, it reads the content of the file.\n        - If the tidy text sections file does not exist, it builds a document tree from\n            the marker markdown content using the `build_doc_tree_from_markdown`\n            function.\n        - The function flattens the document tree using the `flatten_doc_tree` function.\n        - It preprocesses the sections using the `preprocess_sections` function.\n        - The function then tidies the markdown sections and retrieves the metadata\n            using the `tidy_markdown_sections` function.\n        - Finally, it saves the tidy text sections to a file, writes the tidy markdown\n            content to a file, and saves the metadata to a file.\n\n    Returns:\n        dict: The final document tree generated from the PDF.\n\n    Raises:\n        FileNotFoundError: If the marker output file or tidy text sections file does\n            not exist.\n    \"\"\"\n    marker_output_file = output_dir / \"marker_output.md\"\n\n    if marker_output_file.exists():\n        with open(marker_output_file, \"r\", encoding=\"utf-8\") as f:\n            marker_markdown = f.read()\n    else:\n        cache_dir = output_dir / \"pdf_to_markdown_cache/\"\n        marker_markdown = pdf_to_markdown(\n            input_file, marker_output_file, cache_dir=cache_dir\n        )\n\n    tidy_text_sections_file = output_dir / \"tidy_text_sections.json\"\n    tidy_markdown_file = output_dir / \"tidy_output.md\"\n\n    if tidy_text_sections_file.exists():\n        with open(tidy_text_sections_file, \"r\", encoding=\"utf-8\") as f:\n            tidy_text_sections = json.load(f)\n        tidy_markdown = \"\\n\\n\".join(tidy_text_sections)\n    else:\n        doc_tree = build_doc_tree_from_markdown(marker_markdown)\n        sections = flatten_doc_tree(doc_tree)\n        sections = preprocess_sections(sections)\n        tidy_sections, all_metadata = tidy_markdown_sections(\n            sections,\n            openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n            openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n            seed=int(os.getenv(\"SEED\", 42)),\n        )\n\n        tidy_text_sections = [\n            f\"{header.strip()}\\n\\n{content.strip()}\".strip()\n            for header, content in tidy_sections\n        ]\n        tidy_markdown = \"\\n\\n\".join(tidy_text_sections)\n\n        with open(tidy_text_sections_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(tidy_text_sections, f, indent=4)\n\n        with open(tidy_markdown_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(tidy_markdown)\n\n        with open(output_dir / \"tidy_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(all_metadata, f, indent=4)\n\n        print(\n            \"total completion tokens:\",\n            sum([m.get(\"usage\", {}).get(\"total_tokens\", 0) for m in all_metadata]),\n        )\n        print(\n            \"total prompt tokens:\",\n            sum([m.get(\"usage\", {}).get(\"prompt_tokens\", 0) for m in all_metadata]),\n        )\n        print(\n            \"total completed tokens:\",\n            sum([m.get(\"usage\", {}).get(\"completed_tokens\", 0) for m in all_metadata]),\n        )\n\n    final_doc_tree = build_doc_tree_from_markdown(tidy_markdown)\n    doc_tree_file = output_dir / \"doc_tree.json\"\n    with open(doc_tree_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_doc_tree, f, indent=4)\n\n    return final_doc_tree\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.flatten_doc_tree","title":"flatten_doc_tree","text":"<pre><code>flatten_doc_tree(root)\n</code></pre> <p>Recursively flattens a nested dictionary representing a document tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>dict</code> <p>The root node of the document tree.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of tuples representing the flattened document tree. Each tuple contains a heading and its corresponding text.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def flatten_doc_tree(root: dict) -&gt; list:\n    \"\"\"\n    Recursively flattens a nested dictionary representing a document tree.\n\n    Parameters:\n        root (dict): The root node of the document tree.\n\n    Returns:\n        list: A list of tuples representing the flattened document tree. Each tuple\n            contains a heading and its corresponding text.\n    \"\"\"\n    if root[\"heading\"] or root[\"text\"]:\n        sections = [(root[\"heading\"], root[\"text\"])]\n    else:\n        sections = []\n    for section in root.get(\"child_sections\", []):\n        sections.extend(flatten_doc_tree(section))\n    return sections\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.get_section_full_text","title":"get_section_full_text","text":"<pre><code>get_section_full_text(section)\n</code></pre> <p>Retrieves the full text of a section from a given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>section</code> <code>dict</code> <p>The section to retrieve the full text from.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full text of the section.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def get_section_full_text(section: dict) -&gt; str:\n    \"\"\"\n    Retrieves the full text of a section from a given dictionary.\n\n    Args:\n        section (dict): The section to retrieve the full text from.\n\n    Returns:\n        str: The full text of the section.\n    \"\"\"\n    flatten_sections = flatten_doc_tree(section)\n    text_sections = [\n        f\"{header.strip()}\\n\\n{content.strip()}\".strip()\n        for header, content in flatten_sections\n    ]\n\n    full_text = \"\\n\\n\".join(text_sections)\n\n    return full_text\n</code></pre>"},{"location":"reference/core/markdown/","title":"Markdown","text":""},{"location":"reference/core/markdown/#core.markdown.MarkdownTidier","title":"MarkdownTidier","text":"<p>             Bases: <code>BaseModel</code></p> <p>Tidies the given markdown text using OpenAI's model.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The OpenAI API key.</p> required <code>openai_model</code> <code>str</code> <p>The OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required <code>system_message</code> <code>str</code> <p>The system message for the OpenAI model.</p> required <code>instruction</code> <code>str</code> <p>The instruction for the OpenAI model.</p> required <code>api_client</code> <code>OpenAI</code> <p>The OpenAI client.</p> required Source code in <code>docqa/core/markdown.py</code> <pre><code>class MarkdownTidier(BaseModel):\n    \"\"\"\n    Tidies the given markdown text using OpenAI's model.\n\n    Args:\n        openai_key (str): The OpenAI API key.\n        openai_model (str): The OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n        system_message (str, optional): The system message for the OpenAI model.\n        instruction (str, optional): The instruction for the OpenAI model.\n        api_client (OpenAI): The OpenAI client.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n    system_message: str = (\n        \"You are a professional editor. Your job is to reconstruct the broken markdown\"\n        \" text.\"\n    )\n    instruction: str = (\n        \"You are given a markdown text which was converted from pdf and thus has \"\n        \"mixed-up sentences and paragraphs structure, your job is:\\n\"\n        \"- reconstruct the text with proper sentences and paragraphs.\\n\"\n        \"- keep the headings unchanged.\\n\"\n        \"- keep the original content verbatim.\\n\"\n        \"- discard unrelated text.\\n\"\n        \"Answer with **only the reconstructed text**  and nothing else.\\n\"\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    def process(self, markdown_text: str, temperature: float = 0.7) -&gt; tuple[str, dict]:\n        \"\"\"\n        Generates a response to a given markdown text using the OpenAI chat model.\n\n        Args:\n            markdown_text (str): The input markdown text to generate a response for.\n            temperature (float, optional): The temperature of the model's output.\n                Higher values make the output more random, while lower values make it\n                more focused and deterministic. Defaults to 0.7.\n\n        Returns:\n            str: The generated response text.\n            dict: Metadata about the completion process, including the finish reason\n                and token usage.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; process(\"Hello, how are you?\")\n            ```\n            ```shel\n            (\n                'I am fine, thank you!',\n                {\n                    'finish_reason': 'stop',\n                    'usage': {\n                        'completed_tokens': 48,\n                        'prompt_tokens': 6,\n                        'total_tokens': 54\n                    }\n                }\n            )\n            ```\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_message},\n            {\"role\": \"user\", \"content\": self.instruction},\n            {\"role\": \"user\", \"content\": markdown_text},\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n        )\n\n        text = response.choices[0].message.content.strip()\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = {\n            \"completed_tokens\": response.usage.completion_tokens,\n            \"prompt_tokens\": response.usage.prompt_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n        return text, metadata\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.MarkdownTidier.process","title":"process","text":"<pre><code>process(markdown_text, temperature=0.7)\n</code></pre> <p>Generates a response to a given markdown text using the OpenAI chat model.</p> <p>Parameters:</p> Name Type Description Default <code>markdown_text</code> <code>str</code> <p>The input markdown text to generate a response for.</p> required <code>temperature</code> <code>float</code> <p>The temperature of the model's output. Higher values make the output more random, while lower values make it more focused and deterministic. Defaults to 0.7.</p> <code>0.7</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response text.</p> <code>dict</code> <code>dict</code> <p>Metadata about the completion process, including the finish reason and token usage.</p> Example <p><pre><code>&gt;&gt;&gt; process(\"Hello, how are you?\")\n</code></pre> <pre><code>(\n    'I am fine, thank you!',\n    {\n        'finish_reason': 'stop',\n        'usage': {\n            'completed_tokens': 48,\n            'prompt_tokens': 6,\n            'total_tokens': 54\n        }\n    }\n)\n</code></pre></p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def process(self, markdown_text: str, temperature: float = 0.7) -&gt; tuple[str, dict]:\n    \"\"\"\n    Generates a response to a given markdown text using the OpenAI chat model.\n\n    Args:\n        markdown_text (str): The input markdown text to generate a response for.\n        temperature (float, optional): The temperature of the model's output.\n            Higher values make the output more random, while lower values make it\n            more focused and deterministic. Defaults to 0.7.\n\n    Returns:\n        str: The generated response text.\n        dict: Metadata about the completion process, including the finish reason\n            and token usage.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; process(\"Hello, how are you?\")\n        ```\n        ```shel\n        (\n            'I am fine, thank you!',\n            {\n                'finish_reason': 'stop',\n                'usage': {\n                    'completed_tokens': 48,\n                    'prompt_tokens': 6,\n                    'total_tokens': 54\n                }\n            }\n        )\n        ```\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.system_message},\n        {\"role\": \"user\", \"content\": self.instruction},\n        {\"role\": \"user\", \"content\": markdown_text},\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n    )\n\n    text = response.choices[0].message.content.strip()\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = {\n        \"completed_tokens\": response.usage.completion_tokens,\n        \"prompt_tokens\": response.usage.prompt_tokens,\n        \"total_tokens\": response.usage.total_tokens,\n    }\n\n    return text, metadata\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.find_highest_markdown_heading_level","title":"find_highest_markdown_heading_level","text":"<pre><code>find_highest_markdown_heading_level(lines)\n</code></pre> <p>Takes a list of lines representing a markdown file as input. Finds the highest level of heading and returns it as an integer. Returns None if the text contains no headings.</p> Source <p>https://github.com/nestordemeure/question_extractor/blob/main/question_extractor/markdown.py</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list of str</code> <p>A list of lines in the markdown file.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>int | None: The highest heading level as an integer, or None if no headings are found.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def find_highest_markdown_heading_level(lines: list[str]) -&gt; int | None:\n    \"\"\"\n    Takes a list of lines representing a markdown file as input.\n    Finds the highest level of heading and returns it as an integer.\n    Returns None if the text contains no headings.\n\n    Source:\n        https://github.com/nestordemeure/question_extractor/blob/main/question_extractor/markdown.py\n\n    Args:\n        lines (list of str): A list of lines in the markdown file.\n\n    Returns:\n        int | None: The highest heading level as an integer, or None if no headings\n            are found.\n    \"\"\"\n    highest_heading_level = None\n    code_section = False\n\n    # Iterate through the lines in the markdown file\n    for line in lines:\n        \"\"\"\n        Check code section e.g.:\n            ```bash\n            # Trace an IP packet between two Pods\n            antctl trace-packet -S ns1/pod1 -D ns2/pod2\n            # Trace a Service request from a local Pod\n            antctl trace-packet -S ns1/pod1 -D ns2/svc2 -f \"tcp,tcp_dst=80\"\n            # Trace the Service reply packet (assuming \"ns2/pod2\" is the Service\n            # backend Pod)\n            antctl trace-packet -D ns1/pod1 -S ns2/pod2 -f \"tcp,tcp_src=80\"\n            # Trace an IP packet from a Pod to gateway port\n            antctl trace-packet -S ns1/pod1 -D antrea-gw0\n            # Trace a UDP packet from a Pod to an IP address\n            antctl trace-packet -S ns1/pod1 -D 10.1.2.3 -f udp,udp_dst=1234\n            # Trace a UDP packet from an IP address to a Pod\n            antctl trace-packet -D ns1/pod1 -S 10.1.2.3 -f udp,udp_src=1234\n            ```\n        Here # is a code comment not the md level symbole\n        \"\"\"\n        if line.startswith(\"```\"):\n            code_section = not code_section\n        # Check if the line starts with a heading\n        if line.startswith(\"#\") and not code_section:\n            # Calculate the heading level based on the number of '#' characters\n            current_heading_level = len(line.split()[0])\n\n            # Update the highest_heading_level if it is None or if the current_heading_\n            # level is higher\n            if (highest_heading_level is None) or (\n                current_heading_level &lt; highest_heading_level\n            ):\n                highest_heading_level = current_heading_level\n\n    return highest_heading_level\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.pdf_to_markdown","title":"pdf_to_markdown","text":"<pre><code>pdf_to_markdown(pdf_file, output_file, max_pages=None, parallel_factor=1, cache_dir=Path('.cache/pdf_to_markdown/'))\n</code></pre> <p>Converts a PDF file to Markdown format and saves the result to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_file</code> <code>Path</code> <p>The path to the PDF file to be converted.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the converted Markdown will be saved.</p> required <code>max_pages</code> <code>int | None</code> <p>The maximum number of pages to convert. Defaults to None.</p> <code>None</code> <code>parallel_factor</code> <code>int</code> <p>The number of parallel processes to use for conversion. Defaults to 1.</p> <code>1</code> <code>cache_dir</code> <code>Path</code> <p>The directory to use for caching the conversion</p> <code>Path('.cache/pdf_to_markdown/')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The converted Markdown text.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def pdf_to_markdown(\n    pdf_file: Path,\n    output_file: Path,\n    max_pages: int | None = None,\n    parallel_factor: int = 1,\n    cache_dir: Path = Path(\".cache/pdf_to_markdown/\"),\n) -&gt; str:\n    \"\"\"\n    Converts a PDF file to Markdown format and saves the result to an output file.\n\n    Args:\n        pdf_file (Path): The path to the PDF file to be converted.\n        output_file (Path): The path to the output file where the converted Markdown\n            will be saved.\n        max_pages (int | None, optional): The maximum number of pages to convert.\n            Defaults to None.\n        parallel_factor (int, optional): The number of parallel processes to use for\n            conversion. Defaults to 1.\n        cache_dir (Path, optional): The directory to use for caching the conversion\n\n    Returns:\n        str: The converted Markdown text.\n    \"\"\"\n    markdown_text, metadata = convert_single_pdf(\n        pdf_file,\n        model_lst=load_all_models(),\n        max_pages=max_pages,\n        parallel_factor=parallel_factor,\n        cache_dir=cache_dir,\n    )\n\n    if output_file is not None:\n        output_file = Path(output_file)\n        output_file.parent.mkdir(exist_ok=True, parents=True)\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(markdown_text)\n\n    return markdown_text\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.filter_empty_sections","title":"filter_empty_sections","text":"<pre><code>filter_empty_sections(sections)\n</code></pre> <p>Filters out empty sections from a list of tuples.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing sections, where each tuple contains a header (str) and content (str).</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples representing non-empty sections, where each tuple contains a header (str) and content (str).</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def filter_empty_sections(sections: list[tuple[str, str]]) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Filters out empty sections from a list of tuples.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing sections, where\n            each tuple contains a header (str) and content (str).\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples representing non-empty sections, where\n            each tuple contains a header (str) and content (str).\n    \"\"\"\n    return [(header, content) for header, content in sections if header or content]\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.merge_abstract_with_previous_sections","title":"merge_abstract_with_previous_sections","text":"<pre><code>merge_abstract_with_previous_sections(sections)\n</code></pre> <p>If found an Abstract section then assume it's a research paper and merge it with all previous sections, this is because the authors section might have more column thus messes up the parsed order</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing sections, where each tuple contains a header (str) and content (str).</p> required <p>Returns:</p> Type Description <p>list[tuple[str, str]]: A list of tuples representing merged sections, where each tuple contains a header (str) and content (str).</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def merge_abstract_with_previous_sections(sections: list[tuple[str, str]]):\n    \"\"\"\n    If found an Abstract section then assume it's a research paper and merge it with\n    all previous sections, this is because the authors section might have more column\n    thus messes up the parsed order\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing sections, where\n            each tuple contains a header (str) and content (str).\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples representing merged sections, where\n            each tuple contains a header (str) and content (str).\n    \"\"\"\n\n    if len(sections) &lt; 2:\n        return sections\n\n    first_header = sections[0][0]\n    text_sections = [sections[0][1]]\n\n    for i in range(1, len(sections)):\n        header, content = sections[i]\n        current_text = f\"{header}\\n\\n{content}\"\n        text_sections.append(current_text)\n        if re.sub(r\"[^a-zA-Z]\", \"\", header).lower() == \"abstract\":\n            combined_text = \"\\n\\n\".join(text_sections)\n            return [(first_header, combined_text)] + sections[i + 1 :]\n\n    return sections\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.preprocess_sections","title":"preprocess_sections","text":"<pre><code>preprocess_sections(sections)\n</code></pre> <p>Preprocesses the given list of sections by filtering out any empty sections and merging any abstract sections with their previous sections.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>List[Tuple[str, str]]</code> <p>A list of tuples representing sections. Each tuple contains two strings: the title of the section and the content of the section.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List[Tuple[str, str]]: A list of tuples representing the preprocessed sections. Each tuple contains two strings: the title of the section and the content of the section.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def preprocess_sections(sections: list[tuple[str, str]]) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Preprocesses the given list of sections by filtering out any empty sections and\n    merging any abstract sections with their previous sections.\n\n    Args:\n        sections (List[Tuple[str, str]]): A list of tuples representing sections.\n            Each tuple contains two strings: the title of the section and the content\n            of the section.\n\n    Returns:\n        List[Tuple[str, str]]: A list of tuples representing the preprocessed sections.\n            Each tuple contains two strings: the title of the section and the content\n            of the section.\n    \"\"\"\n    sections = filter_empty_sections(sections)\n    sections = merge_abstract_with_previous_sections(sections)\n\n    return sections\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.text_similarity_score","title":"text_similarity_score","text":"<pre><code>text_similarity_score(text1, text2)\n</code></pre> <p>Compute the similarity score between two texts.</p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>The first text.</p> required <code>text2</code> <code>str</code> <p>The second text.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The similarity score between the two texts.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def text_similarity_score(text1: str, text2: str) -&gt; float:\n    \"\"\"\n    Compute the similarity score between two texts.\n\n    Args:\n        text1 (str): The first text.\n        text2 (str): The second text.\n\n    Returns:\n        float: The similarity score between the two texts.\n    \"\"\"\n    # remove special characters\n    text1 = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text1.lower())\n    text2 = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text2.lower())\n\n    # split into words, the words are sorted to allow shuffling of content\n    text1_words = sorted(text1.split())\n    text2_words = sorted(text2.split())\n\n    return 1 - editdistance.eval(text1_words, text2_words) / (\n        max(len(text1_words), len(text2_words)) + 1e-6\n    )\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.header_similarity_score","title":"header_similarity_score","text":"<pre><code>header_similarity_score(header1, header2)\n</code></pre> <p>Calculate the similarity score between two headers.</p> <p>Parameters:</p> Name Type Description Default <code>header1</code> <code>str</code> <p>The first header.</p> required <code>header2</code> <code>str</code> <p>The second header.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The similarity score between the two headers.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def header_similarity_score(header1: str, header2: str) -&gt; float:\n    \"\"\"\n    Calculate the similarity score between two headers.\n\n    Parameters:\n        header1 (str): The first header.\n        header2 (str): The second header.\n\n    Returns:\n        float: The similarity score between the two headers.\n    \"\"\"\n    return 1 - editdistance.eval(header1, header2) / (\n        max(len(header1), len(header2)) + 1e-6\n    )\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.preserve_content","title":"preserve_content","text":"<pre><code>preserve_content(header, old_content, new_text, header_similarity_threshold=0.7, content_similarity_threshold=0.8)\n</code></pre> <p>Calculate the similarity between the given header and new header using a threshold. If the similarity score is above the threshold, the new text still contains the header, so the content after the header is extracted as the new content. If the similarity score is below the threshold, the new text does not contain the header, so the entire new text is considered as the new content. Calculate the similarity between the old content and new content using a threshold. If the similarity score is above the threshold, the content is considered preserved and the new content along with its similarity score is returned. If the similarity score is below the threshold, the content has been modified too much and the old content along with its similarity score is returned.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>str</code> <p>The header of the old text.</p> required <code>old_content</code> <code>str</code> <p>The content of the old text.</p> required <code>new_text</code> <code>str</code> <p>The new text.</p> required <code>header_similarity_threshold</code> <code>float</code> <p>The threshold for header similarity. Defaults to 0.7.</p> <code>0.7</code> <code>content_similarity_threshold</code> <code>float</code> <p>The threshold for content similarity. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[str, float]</code> <p>tuple[str, float]: A tuple containing the new content and its similarity score.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def preserve_content(\n    header: str,\n    old_content: str,\n    new_text: str,\n    header_similarity_threshold: float = 0.7,\n    content_similarity_threshold: float = 0.8,\n) -&gt; tuple[str, float]:\n    \"\"\"\n    Calculate the similarity between the given header and new header using a threshold.\n    If the similarity score is above the threshold, the new text still contains the\n    header, so the content after the header is extracted as the new content.\n    If the similarity score is below the threshold, the new text does not contain the\n    header, so the entire new text is considered as the new content.\n    Calculate the similarity between the old content and new content using a threshold.\n    If the similarity score is above the threshold, the content is considered preserved\n    and the new content along with its similarity score is returned.\n    If the similarity score is below the threshold, the content has been modified too\n    much and the old content along with its similarity score is returned.\n\n    Args:\n        header (str): The header of the old text.\n        old_content (str): The content of the old text.\n        new_text (str): The new text.\n        header_similarity_threshold (float, optional): The threshold for header\n            similarity. Defaults to 0.7.\n        content_similarity_threshold (float, optional): The threshold for content\n            similarity. Defaults to 0.8.\n\n    Returns:\n        tuple[str, float]: A tuple containing the new content and its similarity score.\n    \"\"\"\n    parts = new_text.split(\"\\n\")\n    new_header = parts[0]\n\n    header_similarity = header_similarity_score(header, new_header)\n    if header_similarity &gt;= header_similarity_threshold:\n        # new text still contains header\n        new_content = \"\\n\".join(parts[1:])\n    else:\n        # new text does not contain header\n        new_content = new_text\n\n    content_similarity = text_similarity_score(old_content, new_content)\n    if content_similarity &gt;= content_similarity_threshold:\n        # content is still preserved\n        return new_content, content_similarity\n    else:\n        # content has been modified too much\n        return old_content, content_similarity\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.tidy_markdown_sections","title":"tidy_markdown_sections","text":"<pre><code>tidy_markdown_sections(sections, max_length=4096, openai_key='', openai_model='', seed=42, header_similarity_threshold=0.7, content_similarity_threshold=0.8)\n</code></pre> <p>Tidies up sections of markdown text by splitting them into header and content, and then processing each section using the MarkdownTidier class. It takes a list of tuples representing the sections, where each tuple contains a header and content. The function also accepts optional parameters such as the maximum length of the tidied sections, the OpenAI API key, the OpenAI model to use, a seed value for reproducibility, and thresholds for header and content similarity.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing the sections of markdown text. Each tuple contains a header and content.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the tidied sections. Defaults to 4096.</p> <code>4096</code> <code>openai_key</code> <code>str</code> <p>The OpenAI API key. Defaults to \"\".</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The OpenAI model to use. Defaults to \"\".</p> <code>''</code> <code>seed</code> <code>int</code> <p>A seed value for reproducibility. Defaults to 42.</p> <code>42</code> <code>header_similarity_threshold</code> <code>float</code> <p>The threshold for header similarity. Defaults to 0.7.</p> <code>0.7</code> <code>content_similarity_threshold</code> <code>float</code> <p>The threshold for content similarity. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[list[tuple[str, str]], list[dict]]</code> <p>tuple[list[tuple[str, str]], list[dict]]: A tuple containing the tidied sections and a list of metadata for each section.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def tidy_markdown_sections(\n    sections: list[tuple[str, str]],\n    max_length: int = 4096,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    header_similarity_threshold: float = 0.7,\n    content_similarity_threshold: float = 0.8,\n) -&gt; tuple[list[tuple[str, str]], list[dict]]:\n    \"\"\"\n    Tidies up sections of markdown text by splitting them into header and content, and\n    then processing each section using the MarkdownTidier class. It takes a list of\n    tuples representing the sections, where each tuple contains a header and\n    content. The function also accepts optional parameters such as the maximum\n    length of the tidied sections, the OpenAI API key, the OpenAI model to use, a\n    seed value for reproducibility, and thresholds for header and content similarity.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing the sections of\n            markdown text. Each tuple contains a header and content.\n        max_length (int, optional): The maximum length of the tidied sections.\n            Defaults to 4096.\n        openai_key (str, optional): The OpenAI API key. Defaults to \"\".\n        openai_model (str, optional): The OpenAI model to use. Defaults to \"\".\n        seed (int, optional): A seed value for reproducibility. Defaults to 42.\n        header_similarity_threshold (float, optional): The threshold for header\n            similarity. Defaults to 0.7.\n        content_similarity_threshold (float, optional): The threshold for content\n            similarity. Defaults to 0.8.\n\n    Returns:\n        tuple[list[tuple[str, str]], list[dict]]: A tuple containing the tidied\n            sections and a list of metadata for each section.\n    \"\"\"\n    tidier = MarkdownTidier(openai_key=openai_key, openai_model=openai_model, seed=seed)\n    encoding = tiktoken.encoding_for_model(tidier.openai_model)\n\n    tidy_sections = []\n    all_metadata: list[dict] = []\n    for header, content in sections:\n        print(\"Tidying:\", header)\n        section_text = f\"{header}\\n\\n{content}\"\n        if len(encoding.encode(section_text)) &gt; max_length:\n            tidy_sections.append((header, content))\n            all_metadata.append({})\n        else:\n            new_section_text, metadata = tidier.process(section_text)\n            new_content, similarty = preserve_content(\n                header,\n                content,\n                new_section_text,\n                header_similarity_threshold=header_similarity_threshold,\n                content_similarity_threshold=content_similarity_threshold,\n            )\n            print(\"\\tcontent similarity:\", similarty)\n            tidy_sections.append((header, new_content))\n            all_metadata.append(metadata)\n\n    return tidy_sections, all_metadata\n</code></pre>"},{"location":"reference/core/retrieval/","title":"Retrieval","text":""},{"location":"reference/core/retrieval/#core.retrieval.SemanticRetriever","title":"SemanticRetriever","text":"<p>             Bases: <code>BaseModel</code></p> <p>SemanticRetriever class for retrieving documents based on embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>Any</code> <p>The embedding model used to encode the corpus.</p> required <code>vector_db</code> <code>Collection</code> <p>The Chroma vector database.</p> required Source code in <code>docqa/core/retrieval.py</code> <pre><code>class SemanticRetriever(BaseModel):\n    \"\"\"\n    SemanticRetriever class for retrieving documents based on embeddings.\n\n    Args:\n        embedding_model (Any): The embedding model used to encode the corpus.\n        vector_db (chromadb.Collection): The Chroma vector database.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    embedding_model: AnglE\n    vector_db: chromadb.Collection\n\n    def process(\n        self, query: str, top_k: int, metadata_filter: dict | None = None\n    ) -&gt; list[dict]:\n        \"\"\"\n        Process the given query to retrieve the top-k results from the vector database.\n\n        Args:\n            query (str): The query string.\n            top_k (int): The number of results to retrieve.\n            metadata_filter (dict | None, optional): A dictionary specifying metadata\n                filters. Defaults to None.\n\n        Returns:\n            list[dict]: The list of retrieved results.\n        \"\"\"\n        query_embeddings = self.embedding_model.encode({\"text\": query})\n\n        results = self.vector_db.query(\n            query_embeddings=query_embeddings, n_results=top_k, where=metadata_filter\n        )\n\n        output = []\n        for i in range(len(results[\"ids\"][0])):\n            score = 1 - results[\"distances\"][0][i]\n            document = results[\"documents\"][0][i]\n            metadata = results[\"metadatas\"][0][i]\n            output.append({\"score\": score, \"document\": document, \"metadata\": metadata})\n\n        return output\n</code></pre>"},{"location":"reference/core/retrieval/#core.retrieval.SemanticRetriever.process","title":"process","text":"<pre><code>process(query, top_k, metadata_filter=None)\n</code></pre> <p>Process the given query to retrieve the top-k results from the vector database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query string.</p> required <code>top_k</code> <code>int</code> <p>The number of results to retrieve.</p> required <code>metadata_filter</code> <code>dict | None</code> <p>A dictionary specifying metadata filters. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The list of retrieved results.</p> Source code in <code>docqa/core/retrieval.py</code> <pre><code>def process(\n    self, query: str, top_k: int, metadata_filter: dict | None = None\n) -&gt; list[dict]:\n    \"\"\"\n    Process the given query to retrieve the top-k results from the vector database.\n\n    Args:\n        query (str): The query string.\n        top_k (int): The number of results to retrieve.\n        metadata_filter (dict | None, optional): A dictionary specifying metadata\n            filters. Defaults to None.\n\n    Returns:\n        list[dict]: The list of retrieved results.\n    \"\"\"\n    query_embeddings = self.embedding_model.encode({\"text\": query})\n\n    results = self.vector_db.query(\n        query_embeddings=query_embeddings, n_results=top_k, where=metadata_filter\n    )\n\n    output = []\n    for i in range(len(results[\"ids\"][0])):\n        score = 1 - results[\"distances\"][0][i]\n        document = results[\"documents\"][0][i]\n        metadata = results[\"metadatas\"][0][i]\n        output.append({\"score\": score, \"document\": document, \"metadata\": metadata})\n\n    return output\n</code></pre>"},{"location":"reference/demo/","title":"Demo","text":""},{"location":"reference/demo/config/","title":"Config","text":""},{"location":"reference/demo/create_dataset/","title":"Create dataset","text":""},{"location":"reference/demo/create_dataset/#demo.create_dataset.create_openai_dataset","title":"create_openai_dataset","text":"<pre><code>create_openai_dataset(sections_qa_data_flatten, section_type='main', question_type='dense', answer_type='long', prompt_type='instruction')\n</code></pre> <p>Generate a dataset for OpenAI based on the given sections QA data.</p> <p>Parameters:</p> Name Type Description Default <code>sections_qa_data_flatten</code> <code>dict</code> <p>A dictionary containing the flattened sections QA data.</p> required <code>section_type</code> <code>Literal['main', 'summary', 'metadata', 'extra']</code> <p>The type of section to include in the dataset. Defaults to \"main\".</p> <code>'main'</code> <code>question_type</code> <code>Literal['dense', 'sparse']</code> <p>The type of question to include in the dataset. Defaults to \"dense\".</p> <code>'dense'</code> <code>answer_type</code> <code>Literal['long', 'short']</code> <p>The type of answer to include in the dataset. Defaults to \"long\".</p> <code>'long'</code> <code>prompt_type</code> <code>Literal['instruction', 'simple']</code> <p>The type of prompt to use in the dataset. Defaults to \"instruction\".</p> <code>'instruction'</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The generated dataset for OpenAI.</p> Note <ul> <li>The dataset is generated based on the specified parameters.</li> <li>Only sections that exist in the sections QA data will be included in the dataset.</li> <li>For each section, the questions and answers are extracted based on the question type and answer type.</li> <li>Depending on the prompt type, different sample generation functions are used to create the samples.</li> <li>The dataset is a list of dictionaries, where each dictionary represents a sample.</li> </ul> Source code in <code>docqa/demo/create_dataset.py</code> <pre><code>def create_openai_dataset(\n    sections_qa_data_flatten: dict,\n    section_type: Literal[\n        \"main\", \"summary\", \"metadata\", \"extra\"\n    ] = \"main\",  # keys in SECTIONS\n    question_type: Literal[\"dense\", \"sparse\"] = \"dense\",\n    answer_type: Literal[\"long\", \"short\"] = \"long\",\n    prompt_type: Literal[\"instruction\", \"simple\"] = \"instruction\",\n) -&gt; list[dict]:\n    \"\"\"\n    Generate a dataset for OpenAI based on the given sections QA data.\n\n    Parameters:\n        sections_qa_data_flatten (dict): A dictionary containing the flattened sections\n            QA data.\n        section_type (Literal[\"main\", \"summary\", \"metadata\", \"extra\"], optional): The\n            type of section to include in the dataset. Defaults to \"main\".\n        question_type (Literal[\"dense\", \"sparse\"], optional): The type of question to\n            include in the dataset. Defaults to \"dense\".\n        answer_type (Literal[\"long\", \"short\"], optional): The type of answer to include\n            in the dataset. Defaults to \"long\".\n        prompt_type (Literal[\"instruction\", \"simple\"], optional): The type of prompt to\n            use in the dataset. Defaults to \"instruction\".\n\n    Returns:\n        list[dict]: The generated dataset for OpenAI.\n\n    Note:\n        - The dataset is generated based on the specified parameters.\n        - Only sections that exist in the sections QA data will be included in the\n        dataset.\n        - For each section, the questions and answers are extracted based on the\n        question type and answer type.\n        - Depending on the prompt type, different sample generation functions are used\n        to create the samples.\n        - The dataset is a list of dictionaries, where each dictionary represents a\n        sample.\n    \"\"\"\n    dataset = []\n    for heading in SECTIONS[section_type]:\n        if heading not in sections_qa_data_flatten:\n            continue\n        section = sections_qa_data_flatten[heading]\n        qa_list = section[f\"{question_type}_questions\"]\n        for item in qa_list:\n            question = item[\"question\"]\n            answer = item[\"answer\"] if answer_type == \"short\" else item[\"long_answer\"]\n            if prompt_type == \"simple\":\n                sample = make_simple_sample_for_openai(question, answer)\n            elif prompt_type == \"instruction\":\n                reference = f\"[source: {heading}]\\n{section['text']}\\n\"\n                sample = make_instruction_sample_for_openai(\n                    question=question,\n                    answer=answer,\n                    references=[reference],\n                )\n            dataset.append(sample)\n\n    return dataset\n</code></pre>"},{"location":"reference/demo/create_dataset/#demo.create_dataset.pdf_to_qa_data","title":"pdf_to_qa_data","text":"<pre><code>pdf_to_qa_data(output_dir, pdf_file)\n</code></pre> <p>Generates a QA data dictionary from a PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>The directory where the output files will be saved.</p> required <code>pdf_file</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The generated QA data dictionary.</p> Source code in <code>docqa/demo/create_dataset.py</code> <pre><code>def pdf_to_qa_data(output_dir: Path, pdf_file: Path) -&gt; dict:\n    \"\"\"\n    Generates a QA data dictionary from a PDF file.\n\n    Args:\n        output_dir (Path): The directory where the output files will be saved.\n        pdf_file (Path): The path to the PDF file.\n\n    Returns:\n        dict: The generated QA data dictionary.\n    \"\"\"\n    doc_tree_file = output_dir / \"doc_tree.json\"\n\n    if doc_tree_file.exists():\n        with open(doc_tree_file, \"r\", encoding=\"utf-8\") as f:\n            doc_tree = json.load(f)\n    else:\n        doc_tree = build_doc_tree_from_pdf(pdf_file, output_dir=output_dir)\n\n    top_sections_qa_data = generate_top_sections_questions(\n        doc_tree,\n        output_file=output_dir / \"top_sections_qa_data.json\",\n        openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n        openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n        seed=int(os.getenv(\"SEED\", 42)),\n        temperature=1.0,\n    )\n    top_sections_qa_data = generate_long_answers_for_sections_questions(\n        top_sections_qa_data,\n        output_file=output_dir / \"top_sections_qa_data_long_answers.json\",\n        openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n        openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n        seed=int(os.getenv(\"SEED\", 42)),\n        temperature=1.0,\n    )\n\n    return top_sections_qa_data\n</code></pre>"},{"location":"reference/demo/finetune_openai/","title":"Finetune openai","text":""},{"location":"reference/demo/pipeline/","title":"Pipeline","text":""},{"location":"reference/demo/pipeline/#demo.pipeline.Pipeline","title":"Pipeline","text":"<p>             Bases: <code>BaseModel</code></p> <p>Pipeline class for the demo.</p> <p>Parameters:</p> Name Type Description Default <code>retriever</code> <code>SemanticRetriever</code> <p>The semantic retriever.</p> required <code>answerer</code> <code>AnswerGenerator</code> <p>The answer generator.</p> required <code>sections_map</code> <code>dict</code> <p>The mapping of section headings to their content.</p> required Source code in <code>docqa/demo/pipeline.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"\n    Pipeline class for the demo.\n\n    Args:\n        retriever (SemanticRetriever): The semantic retriever.\n        answerer (AnswerGenerator): The answer generator.\n        sections_map (dict): The mapping of section headings to their content.\n    \"\"\"\n\n    retriever: SemanticRetriever\n    answerer: AnswerGenerator\n    sections_map: dict\n\n    def process(\n        self,\n        question: str,\n        certainty_threshold: float = 0.9,\n        uncertainty_threshold: float = 0.6,\n        temperature: float = 1.0,\n    ) -&gt; PipelineOutput:\n        \"\"\"\n        Processes a question and returns the answer along with related references and\n            metadata.\n\n        Args:\n            question (str): The question to process.\n            certainty_threshold (float, optional): The threshold for considering a\n                question as certain. Defaults to 0.9.\n            uncertainty_threshold (float, optional): The threshold for considering a\n                question as uncertain. Defaults to 0.6.\n            temperature (float, optional): The temperature parameter for generating the\n                answer. Defaults to 1.0.\n\n        Returns:\n            dict: A dictionary containing the answer, references, and metadata.\n\n        Output dict format:\n            - answer (str): The answer to the question.\n            - references (list): A list of dictionaries containing the related\n                references.\n                - source (str): The source of the reference.\n                - content (str): The content of the reference.\n            - metadata (dict): Additional metadata associated with the answer.\n        \"\"\"\n        similar_questions = self.retriever.process(\n            question, top_k=1, metadata_filter={\"type\": \"question\"}\n        )\n        question_similarity = similar_questions[0][\"score\"]\n\n        if question_similarity &gt; certainty_threshold:\n            related_section = similar_questions[0][\"metadata\"][\"source\"]\n            related_content = self.sections_map[related_section]\n            return PipelineOutput(\n                answer=similar_questions[0][\"metadata\"][\"answer\"],\n                references=[{\"source\": related_section, \"content\": related_content}],\n            )\n\n        related_chunks = self.retriever.process(\n            question, top_k=3, metadata_filter={\"type\": \"chunk\"}\n        )\n        chunks_similarity = np.mean([each[\"score\"] for each in related_chunks])\n\n        if (\n            question_similarity &lt; uncertainty_threshold\n            and chunks_similarity &lt; uncertainty_threshold\n        ):\n            references = []\n            references_text = \"No related references found.\"\n        elif question_similarity &gt;= chunks_similarity:\n            related_section = similar_questions[0][\"metadata\"][\"source\"]\n            related_content = self.sections_map[related_section]\n            references = [{\"source\": related_section, \"content\": related_content}]\n            references_text = f\"{related_section}\\n\\n{related_content}\"\n        else:\n            references = [\n                {\"source\": each[\"metadata\"][\"source\"], \"content\": each[\"document\"]}\n                for each in related_chunks\n            ]\n            references_text = (\"-\" * 6).join(\n                [\n                    f\"From: {each['source']}\\n...\\n{each['content']}\\n...\\n\"\n                    for each in references\n                ]\n            )\n\n        answer, metadata = self.answerer.process(\n            question, references_text, temperature=temperature\n        )\n\n        return PipelineOutput(\n            answer=answer,\n            references=[RetrievalReference.model_validate(each) for each in references],\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.Pipeline.process","title":"process","text":"<pre><code>process(question, certainty_threshold=0.9, uncertainty_threshold=0.6, temperature=1.0)\n</code></pre> <p>Processes a question and returns the answer along with related references and     metadata.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to process.</p> required <code>certainty_threshold</code> <code>float</code> <p>The threshold for considering a question as certain. Defaults to 0.9.</p> <code>0.9</code> <code>uncertainty_threshold</code> <code>float</code> <p>The threshold for considering a question as uncertain. Defaults to 0.6.</p> <code>0.6</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for generating the answer. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>PipelineOutput</code> <p>A dictionary containing the answer, references, and metadata.</p> Output dict format <ul> <li>answer (str): The answer to the question.</li> <li>references (list): A list of dictionaries containing the related     references.<ul> <li>source (str): The source of the reference.</li> <li>content (str): The content of the reference.</li> </ul> </li> <li>metadata (dict): Additional metadata associated with the answer.</li> </ul> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def process(\n    self,\n    question: str,\n    certainty_threshold: float = 0.9,\n    uncertainty_threshold: float = 0.6,\n    temperature: float = 1.0,\n) -&gt; PipelineOutput:\n    \"\"\"\n    Processes a question and returns the answer along with related references and\n        metadata.\n\n    Args:\n        question (str): The question to process.\n        certainty_threshold (float, optional): The threshold for considering a\n            question as certain. Defaults to 0.9.\n        uncertainty_threshold (float, optional): The threshold for considering a\n            question as uncertain. Defaults to 0.6.\n        temperature (float, optional): The temperature parameter for generating the\n            answer. Defaults to 1.0.\n\n    Returns:\n        dict: A dictionary containing the answer, references, and metadata.\n\n    Output dict format:\n        - answer (str): The answer to the question.\n        - references (list): A list of dictionaries containing the related\n            references.\n            - source (str): The source of the reference.\n            - content (str): The content of the reference.\n        - metadata (dict): Additional metadata associated with the answer.\n    \"\"\"\n    similar_questions = self.retriever.process(\n        question, top_k=1, metadata_filter={\"type\": \"question\"}\n    )\n    question_similarity = similar_questions[0][\"score\"]\n\n    if question_similarity &gt; certainty_threshold:\n        related_section = similar_questions[0][\"metadata\"][\"source\"]\n        related_content = self.sections_map[related_section]\n        return PipelineOutput(\n            answer=similar_questions[0][\"metadata\"][\"answer\"],\n            references=[{\"source\": related_section, \"content\": related_content}],\n        )\n\n    related_chunks = self.retriever.process(\n        question, top_k=3, metadata_filter={\"type\": \"chunk\"}\n    )\n    chunks_similarity = np.mean([each[\"score\"] for each in related_chunks])\n\n    if (\n        question_similarity &lt; uncertainty_threshold\n        and chunks_similarity &lt; uncertainty_threshold\n    ):\n        references = []\n        references_text = \"No related references found.\"\n    elif question_similarity &gt;= chunks_similarity:\n        related_section = similar_questions[0][\"metadata\"][\"source\"]\n        related_content = self.sections_map[related_section]\n        references = [{\"source\": related_section, \"content\": related_content}]\n        references_text = f\"{related_section}\\n\\n{related_content}\"\n    else:\n        references = [\n            {\"source\": each[\"metadata\"][\"source\"], \"content\": each[\"document\"]}\n            for each in related_chunks\n        ]\n        references_text = (\"-\" * 6).join(\n            [\n                f\"From: {each['source']}\\n...\\n{each['content']}\\n...\\n\"\n                for each in references\n            ]\n        )\n\n    answer, metadata = self.answerer.process(\n        question, references_text, temperature=temperature\n    )\n\n    return PipelineOutput(\n        answer=answer,\n        references=[RetrievalReference.model_validate(each) for each in references],\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.create_chroma_db","title":"create_chroma_db","text":"<pre><code>create_chroma_db(data_dir, db_dir, collection_name, embedding_model)\n</code></pre> <p>Creates a Chroma database given the data directory, database directory, collection     name, and embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The directory containing the data files.</p> required <code>db_dir</code> <code>Path</code> <p>The directory where the database will be created.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection in the database.</p> required <code>embedding_model</code> <code>Any</code> <p>The embedding model used to encode the corpus.</p> required <p>Returns:</p> Type Description <code>Collection</code> <p>chromadb.Collection: The created Chroma database.</p> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def create_chroma_db(\n    data_dir: Path, db_dir: Path, collection_name: str, embedding_model: Any\n) -&gt; chromadb.Collection:\n    \"\"\"\n    Creates a Chroma database given the data directory, database directory, collection\n        name, and embedding model.\n\n    Args:\n        data_dir (Path): The directory containing the data files.\n        db_dir (Path): The directory where the database will be created.\n        collection_name (str): The name of the collection in the database.\n        embedding_model (Any): The embedding model used to encode the corpus.\n\n    Returns:\n        chromadb.Collection: The created Chroma database.\n    \"\"\"\n    corpus = []\n    metadatas = []\n\n    top_sections_qa_data_file = data_dir / \"top_sections_qa_data.json\"\n    with open(top_sections_qa_data_file) as f:\n        qa_data = json.load(f)\n\n    allowed_sections = set(\n        sum([SECTIONS[section_type] for section_type in [\"main\", \"summary\"]], [])\n    )\n\n    for heading, section in qa_data.items():\n        if heading not in allowed_sections:\n            continue\n        for item in section[\"dense_questions\"] + section[\"sparse_questions\"]:\n            corpus.append(item[\"question\"])\n            metadatas.append(\n                {\"type\": \"question\", \"source\": heading, \"answer\": item[\"answer\"]}\n            )\n\n    doc_tree_file = data_dir / \"doc_tree.json\"\n    with open(doc_tree_file) as f:\n        doc_tree = json.load(f)\n    all_sections_map = {heading: text for heading, text in flatten_doc_tree(doc_tree)}\n\n    for heading in all_sections_map.keys():\n        if heading not in allowed_sections:\n            continue\n        section_chunks = chunk_content(all_sections_map[heading])\n        corpus.extend(section_chunks)\n        metadatas.extend([{\"type\": \"chunk\", \"source\": heading} for _ in section_chunks])\n\n    print(\"Creating chroma db...\")\n    client = chromadb.PersistentClient(\n        str(db_dir), Settings(anonymized_telemetry=False)\n    )\n    db = client.create_collection(\n        name=collection_name, metadata={\"hnsw:space\": \"cosine\"}\n    )\n    print(\"Finish creating chroma db.\")\n\n    print(\"Embedding corpus...\")\n    # one by one because my machine does not have much ram\n    corpus_embeddings = [embedding_model.encode({\"text\": each})[0] for each in corpus]\n    corpus_embeddings = np.vstack(corpus_embeddings)\n    print(\"Finish embedding corpus.\")\n\n    print(\"Populating chroma db...\")\n    db.add(\n        documents=corpus,\n        embeddings=corpus_embeddings,\n        metadatas=metadatas,\n        ids=[str(i) for i in range(len(corpus))],\n    )\n    print(\"Finish populating chroma db.\")\n\n    return db\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.get_pipeline","title":"get_pipeline","text":"<pre><code>get_pipeline(data_dir, openai_key, openai_model)\n</code></pre> <p>Initializes and returns a pipeline for processing text-based questions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The directory containing the data files.</p> required <code>openai_key</code> <code>str</code> <p>The API key for OpenAI.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>The initialized pipeline for processing text-based questions.</p> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def get_pipeline(data_dir: Path, openai_key: str, openai_model: str) -&gt; Pipeline:\n    \"\"\"\n    Initializes and returns a pipeline for processing text-based questions.\n\n    Args:\n        data_dir (Path): The directory containing the data files.\n        openai_key (str): The API key for OpenAI.\n        openai_model (str): The name of the OpenAI model to use.\n\n    Returns:\n        Pipeline: The initialized pipeline for processing text-based questions.\n    \"\"\"\n    doc_tree_file = data_dir / \"doc_tree.json\"\n    with open(doc_tree_file) as f:\n        doc_tree = json.load(f)\n    all_sections_map = {heading: text for heading, text in flatten_doc_tree(doc_tree)}\n\n    print(\"Loading embedding model...\")\n    embedding_model = AnglE.from_pretrained(\n        \"WhereIsAI/UAE-Large-V1\", pooling_strategy=\"cls\"\n    )\n    embedding_model.set_prompt(prompt=Prompts.C)\n    print(\"Finish loading embedding model.\")\n\n    db_dir = data_dir / \"chroma\"\n    db_collection_name = \"generative-agents\"\n\n    try:\n        chroma_client = chromadb.PersistentClient(\n            str(db_dir), Settings(anonymized_telemetry=False)\n        )\n        db = chroma_client.get_collection(name=db_collection_name)\n    except ValueError:\n        db = create_chroma_db(data_dir, db_dir, db_collection_name, embedding_model)\n\n    retriever = SemanticRetriever(embedding_model=embedding_model, vector_db=db)\n    answerer = AnswerGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        instruction=(\n            \"You will be answering questions about the paper called 'Generative\"\n            \" Agents'.\\nInstructions:\\n- Find some references in the paper that related\"\n            \" to the question.\\n- If you found related references, answer the question\"\n            \" as detailed as possible based strictly on that references you found.\\n-\"\n            \" If you can't answer the question using the references, say you can't find\"\n            \" sufficient information to answer the question.\\n- If the question is not\"\n            \" related to the references or there is no reference found, say the\"\n            \" question is irrelevant to the paper and answer the question as\"\n            \" a normal chatbot.\\n\\nReferences you found:\\n\\n{reference}\\n\\nQuestion:\"\n            \" {question}\\nAnswer:\"\n        ),\n    )\n\n    pipeline = Pipeline(\n        retriever=retriever,\n        answerer=answerer,\n        sections_map=all_sections_map,\n    )\n\n    return pipeline\n</code></pre>"}]}