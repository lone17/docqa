{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DocQA","text":"<p>Documentation</p> <p> </p> <p>Ask questions on your documents.</p> <p>This repo contains various tools for creating a document QA app from your text file to a RAG chatbot.</p>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Get the source code</li> </ul> <pre><code># clone the repo (with a submodule)\ngit clone --recurse-submodules https://github.com/lone17/docqa.git\ncd docqa\n</code></pre> <ul> <li>It is recommended to create a virtual environment</li> </ul> <pre><code>python -m venv env\n. env/bin/activate\n</code></pre> <ul> <li>First, let's install Marker (following its instructions)</li> </ul> <pre><code>cd marker\n#  Install ghostscript &gt; 9.55 by following https://ghostscript.readthedocs.io/en/latest/Install.html\nscripts/install/ghostscript_install.sh\n# install other requirements\ncat scripts/install/apt-requirements.txt | xargs sudo apt-get install -y\npip install .\n</code></pre> <ul> <li>Then install docqa</li> </ul> <pre><code>cd ..\npip install -e .[dev]\n</code></pre>"},{"location":"#demo","title":"Demo","text":"<p>This repo contains a demo for the whole pipeline for a QA chatbot on Generative Agents based on the information in this paper.</p> <p>For information about the development process, please refer to the technical report</p> <p></p>"},{"location":"#try-the-demo","title":"Try the Demo","text":""},{"location":"#from-source","title":"From source","text":"<p>In order to use this app, you need a OpenAI API key.</p> <p>Before playing with the demo, please populate your key and secrets in the <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=...\nOPENAI_MODEL=...\nOPENAI_SEED=...\nWANDB_API_KEY=... # only needed if you want to fine-tune the model and use WanDB\n</code></pre> <p>All the scripts for the full pipeline as well as generated artifacts are in the <code>demo</code> folder.</p> <ul> <li><code>create_dataset.py</code>: This script handles the full data processing pipeline:</li> <li>parse the pdf file</li> <li>convert it to markdown</li> <li>chunk the content preserving structural content</li> <li>generate question-answers pairs</li> <li>prepare data for other steps: fine-tuning OpenAI models, and adding to vector-stores.</li> <li><code>finetune_openai.py</code>: As the name suggests, this script is used to fine-tune the OpenAI model   using the data generated in <code>create_dataset.py</code>.</li> <li>Also includes Wandb logging.</li> <li><code>pipeline.py</code>: Declares the QA pipeline with semantic retrieval using ChromaDB.</li> </ul> <p>The <code>main.py</code> script is the endpoint for running the backend app:</p> <pre><code>python main.py\n</code></pre> <p>And to run the front end:</p> <pre><code>streamlit run frontend.py\n</code></pre>"},{"location":"#using-docker","title":"Using Docker","text":"<p>Alternatively, you can get the image from Docker Hub.</p> <pre><code>docker pull el117/docqa\ndocker run --rm -p 8000:8000 -e OPENAI_API_KEY=&lt;...&gt; el117/docqa\n</code></pre> <p>Note that the docker does not contain the front end. To run it you can simply do:</p> <pre><code>pip install streamlit\nstreamlit run frontend.py\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#data-pipeline","title":"Data Pipeline","text":"<p>The diagram below describes the data life cycle. Results from each step can be found at docqa/demo/data/generative_agent.</p> <pre><code>flowchart LR\n    subgraph pdf-to-md[PDF to Markdown]\n        direction RL\n        pdf[PDF] --&gt; raw-md(raw\\nmarkdown)\n        raw-md --&gt; tidied-md([tidied\\nmarkdown])\n    end\n\n    subgraph create-dataset[Create Dataset]\n        tidied-md --&gt; sections([markdown\\nsections])\n        sections --&gt; doc-tree([doc\\ntree])\n        doc-tree --&gt; top-lv([top-level\\nsections])\n        doc-tree --&gt; chunks([section-bounded\\nchunks])\n        top-lv --&gt; top-lv-qa([top-level sections\\nQA pairs])\n        top-lv-qa --&gt; finetune-data([fine-tuning\\ndata])\n    end\n\n\n        finetune-data --&gt; lm{{language\\nmodel}}\n\n        top-lv-qa --&gt; vector-store[(vector\\nstore)]\n        chunks ----&gt; vector-store</code></pre>"},{"location":"#app","title":"App","text":"<p>The diagram below describes the app's internal working, from receiving a question to answering it.</p> <pre><code>flowchart LR\n    query(query) --&gt; emb{{embedding\\nmodel}}\n\n    subgraph retriever[SemanticRetriever]\n        direction LR\n        vector-store[(vector\\nstore)]\n\n        emb --&gt; vector-store\n        vector-store --&gt; chunks([related\\nchunks])\n        vector-store --&gt; questions([similar\\nquestions])\n        questions --&gt; sections([related\\nsections])\n    end\n\n    sections --&gt; ref([references])\n    chunks --&gt; ref\n\n    query --&gt; thresh{similarity &gt; threshold}\n    questions --&gt; thresh\n\n    thresh -- true --&gt; answer(((answer &amp;\\nreferences)))\n    thresh -- false --&gt; answerer\n\n    ref --&gt; prompt(prompt)\n    query --&gt; prompt\n\n    subgraph answerer[AnswerGenerator]\n        direction LR\n        prompt --&gt; llm{{language\\nmodel}}\n    end\n\n    llm --&gt; answer\n    ref --&gt; answer</code></pre>"},{"location":"report/","title":"Technical Report","text":""},{"location":"report/#introduction","title":"Introduction","text":"<p>The goal of the project was to develop a program that let users query questions about the paper Generative Agents (the source document) through a chat interface.</p> <p>This document describes the process of development, with details on what has been experimented, how decisions were made, the results, and possible future improvements.</p> <p>The development can be divided into 3 major parts:</p> <ol> <li>Data processing: in which the paper was processed from a PDF file to a more structured and parsable format.</li> <li>Model development: in which the data extracted in the previous step was used to create the training set, which was then used to fine-tune the model.</li> <li>App development: in which the pipeline and logic of the chatbot were constructed using the models and data from previous steps.</li> </ol>"},{"location":"report/#development-process","title":"Development process","text":""},{"location":"report/#data-processing","title":"Data processing","text":"<p>Overview diagram of the data processing pipeline:</p> <pre><code>flowchart LR\n    subgraph pdf-to-md[PDF to Markdown]\n        direction RL\n        pdf[PDF] --&gt; raw-md(raw\\nmarkdown)\n        raw-md --&gt; tidied-md([tidied\\nmarkdown])\n    end\n\n    subgraph create-dataset[Create Dataset]\n        tidied-md --&gt; sections([markdown\\nsections])\n        sections --&gt; doc-tree([doc\\ntree])\n        doc-tree --&gt; top-lv([top-level\\nsections])\n        doc-tree --&gt; chunks([section-bounded\\nchunks])\n        top-lv --&gt; top-lv-qa([top-level sections\\nQA pairs])\n        top-lv-qa --&gt; finetune-data([fine-tuning\\ndata])\n    end\n\n\n        finetune-data --&gt; lm{{language\\nmodel}}\n\n        top-lv-qa --&gt; vector-store[(vector\\nstore)]\n        chunks ----&gt; vector-store</code></pre> <p>Implementation details</p> <p>The data processing pipeline is put together in the <code>docqa/demo/create_dataset.py</code> script.</p>"},{"location":"report/#pdf-to-markdown","title":"PDF to Markdown","text":""},{"location":"report/#why-markdown","title":"Why Markdown?","text":"<p>The first step is to parse the pdf file and extract the text from it. It is preferable that the extracted text preserves the structure of the paper. This is, however, not a trivial task as PDF is notoriously hard to parse and doesn't explicitly contain structural and scheming information.</p> <p>The target format of choice for this step is markdown. Markdown was chosen instead of others as it has a well-defined structure (unlike plain text) while still being readable and not over-expressive (like HTML or LaTex). It strikes a balance between structure and expressiveness.</p> <p>The Markdown format is chosen partly due to being a personal favourite.</p> <p>Note</p> <p>Note that, for the case of the Generative Agents paper, one can get the  LaTex source from Arxiv and parse it instead, which will yield better results. However, in the spirit of handling any PDF document, it was decided to parse the PDF file directly.</p>"},{"location":"report/#pdf-parsers","title":"PDF parsers","text":"<p>Meta has a powerful PDF parser called Nougat which was designed to work on academic documents, which is very suitable for the source document. However, experiments show that it runs quite slowly. It also tries to extract text from images which is a feature that is not needed for the purpose of this project and makes the output noisier for parsing.</p> <p>Speed up Nougat</p> <p>A possible improvement would be to disable the OCR feature of Nougat. For the scope of this project, this was not done due to time constraints.</p> <p>Thus, a different parser called Marker was used for the purpose of this project. It is significantly faster and produces less noise. However, it does not come without drawbacks. It uses Nougat internally along with other tools to handle different aspects of the parsing process. As a result, is a more complicated solution with many components and heuristics.</p> <p>Output</p> <p>The markdown output from Marker is stored in <code>marker_output.md</code>.</p> <p>Parser issues</p> <p>In the specific case of the source document, Marker produced 2 noteworthy problems:</p> <ul> <li>As it tries to detect the column layout of the document, it fails to correctly parse the first page of the document where the authors section has 3 columns while the Abstract has only 2. This results in mixed-up content on the first page. A possible reason for this might be that it failed to identify that the author section and the Abstract are separated, thus treating them as a single section with 2 columns.</li> <li>Near the end of its pipeline is a post-processing step that makes heavy use of heuristics to combine all the extracted text into sentences and paragraphs. This process tends to not preserve the structure of the document, and combined with the first problem, creates even noisier texts.</li> </ul> <p>To overcome the above problems, a fork was created to:</p> <ul> <li>Adjust the post-processing heuristic to prevent over-combining the text.</li> <li>Produce a less processed text so that it can be tidied later.</li> <li>Simplify the installation process.</li> </ul>"},{"location":"report/#post-processing-using-language-models","title":"Post-processing using Language Models","text":"<p>To process the loosely structured text from Marker, a language model (<code>gpt-3.5-turbo</code> in this case) was employed to tidy it up into a more structured format. Language models are suitable for this task due to their ability to understand the semantic and sentence structure of the natural language.</p> <p>To best preserve the structure, heading lines (begin with <code>#</code>) were extracted, and then, only the text in between 2 heading lines was sent to the language model for reconstruction, with some prompt engineering involved. Note that, an exception was made which combines the author section and the Abstract section to address the earlier-mentioned problem.</p> <p>However, since the model produces outputs with a similar length as the inputs, the input length can only be at most half of the model's context length. Thus, the amount of tokens sent to the model was limited to <code>4096</code>.</p> <p>The output of the model is then compared to the input using a similarity metric, this is done to minimize the chances that the model adds new content or produces an output that is too different from the input.</p> <p>Implementation details and output</p> <p>The logic of this procedure is implemented in the <code>tidy_markdown_sections</code> method, and the tidied output is stored at <code>tidy_output.md</code>.</p>"},{"location":"report/#model-development","title":"Model development","text":""},{"location":"report/#create-the-dataset","title":"Create the Dataset","text":"<p>After the previous step, the input of this step would be a markdown file instead of the loosely structure pdf. Thus, any markdown file can be used as input for this step onward, allowing for different data sources and formats.</p>"},{"location":"report/#document-tree","title":"Document tree","text":"<p>To better represent the document structure, a document tree is created from the markdown. A document tree is just a fancy way of saying that the hierarchy of the sections is represented in a nested manner. Which, in this case, is a JSON object.</p> <p>This was done by first extracting the heading lines, deciding the heading level based on the number of <code>#</code>, and then recursively constructing the document tree.</p> <p>Output</p> <p>The output JSON file is stored at <code>doc_tree.json</code>.</p>"},{"location":"report/#section-bounded-chunking","title":"Section-bounded Chunking","text":"<p>For language models, their context lengths are finite and fixed, thus too long of a context will not be utilized fully by the models. In vector stores, the texts are represented as dense vectors, thus the longer the text the harder is it to condense the content into a single vector. As such, when working with language models and vector stores, chunking is usually needed as using the entire document is not efficient.</p> <p>Throughout this project, chunking is used several times. The logic for chunking is as follows:</p> <ul> <li>The chunks will be bound to a single section, meaning, no chunk is allowed to span across multiple sections. This is done by performing chunking on each section's content instead of on the entire document.</li> <li>To create a chunk, the text in a single section is first split at double newline characters (<code>\\n\\n</code>). This is an attempt to divide the text into paragraph candidates.</li> <li> <p>Then, heuristics are used to combine the candidates:</p> </li> <li> <p>If a candidate is longer than <code>single_threshold</code> words, then it is considered a paragraph and will be added as a chunk.</p> </li> <li>Otherwise, candidates are combined until they are longer than <code>composite_threshold</code> words, then it will be added as a chunk.</li> </ul> <p>Implementation details</p> <p>This logic is implemented in the <code>chunk_content</code> function.</p> <p>The choices of <code>single_threshold</code> and <code>composite_threshold</code> are 100 and 200 respectively. This was chosen because the average paragraph in English is about 100 - 200 words.</p> <p>Using language models for chunking</p> <p>Alternatively, a language model can used to divide the text into semantic paragraphs. However, the content in the source document was already presented in short paragraphs and the parsing step preserves this structure quite well, thus the use of language models is not necessary.</p>"},{"location":"report/#question-answer-pairs","title":"Question-Answer Pairs","text":"<p>To fine-tune the model, a dataset in the form of question-answer pairs is needed. This step was done by providing a Language Model with a portion of the document and using it as the context for the model to generate questions and answers. It is important to make sure that the portions are semantically and structurally enclosed, meaning they must contain complete sentences describing complete ideas without being cut-off mid mid-context.</p> <p>Number of training data</p> <p>Per the requirements, the number of training data used for fine-tuning was limited to 100.</p>"},{"location":"report/#generating-questions","title":"Generating questions","text":"<p>It was observed that the lengths of the top-level sections fit well within the context length of the model, while still leaving plenty of room for the model's output. Thus, instead of using chunks, each entire top-level section is used for prompting the model to generate questions. Here, chunking is less desirable as whole sections are guaranteed to have semantic completeness which provides the model with better context for generation.</p> <p>The language model was provided with each section as context and was asked to generate a list of questions that can be based only on the given context and must cover all information of the context, along with answers to those questions. To avoid trivial and duplicate questions, The number of questions is limited to the number of chunks (as described above) that the text can be split into. The idea is if the text contains <code>k</code> paragraphs, then at most <code>k</code> questions corresponding to the content of each paragraph should be asked.</p> <p>Note</p> <p>All sections were used to generate questions except for the Reference section as it does not describe the main content.</p> <p>Prompts for generating questions</p> <p>The questions generation process was handled by the <code>generate_top_sections_questions</code> function, which uses the <code>QAPairGenerator</code> class.</p> <p>Two different prompts were employed, indicated by the parameter <code>question_type</code> with the value of either <code>sparse</code> or <code>dense</code>:</p> <ul> <li>The <code>sparse</code> prompt does not limit the number of questions. Questions generated using this prompt tend to be extractive and trivial, whose answers are one or a few verbatim sentences from the context.</li> <li>The <code>dense</code> prompt limits the number of questions to the calculated number of chunks, which forces the model to come up with \"denser\" questions. Questions generated using this prompt tend to be less simple and require more context to be answered.</li> </ul> <p>Potential approach for improving quantity and quality</p> <ul> <li>The chunks can be used to generate more focused questions. However, since the number of questions for fine-tuning is limited to 100, this was not necessary.</li> <li>Questions quality can be improved by asking the language model to generate more abstract questions such as questions for explanation and comparison. However, these types of questions would require some manual effort to validate. </li> <li>Negative samples (generation with irrelevant context) can be included to reduce hallucination and overconfidence.</li> <li>Another approach to generate more and better questions is to use cross-section context. For example, pick 2 chunks from different sections and ask the model to describe the connection between them. </li> <li>Questions that require reasoning could be generated by asking the model to: <ul> <li>generate the conclusion based on the methodology and experiment sections; </li> <li>design the experiments based on the claims; </li> <li>explain the results of the experiments; </li> <li>give criticism based on the claims and the experiment results. </li> </ul> </li> <li>One can combine multiple question-answer pairs to make one single conversion-like sample. However, this feels like cheating on the question count limit.</li> </ul>"},{"location":"report/#generating-longer-answers","title":"Generating longer answers","text":"<p>The answers generated from the above process were short and lacked details. This is not desirable for fine-tuning datasets as the model might learn to only give short answers during inference. To generate more detailed answers, the model was prompted with the generated questions one at a time, along with the corresponding section as context.</p> <p>Implementation details and output</p> <p>Long answers generation was implemented in the <code>generate_long_answers_for_sections_questions</code> method which uses the <code>AnswerGenerator</code> class. The output is stored in the <code>top_sections_qa_data_long_answers.json</code> file.</p>"},{"location":"report/#fine-tuning-with-openai","title":"Fine-tuning with OpenAI","text":""},{"location":"report/#data-construction","title":"Data construction","text":"<p>The training data was constructed using the generated question-answer pairs and with the following parameters:</p> <ul> <li> <p><code>section_type</code>: denotes which sections of the paper whose questions will be included in the dataset. For the source document, the sections are categorized into 4 types defined in the config.py file:</p> <ul> <li><code>main</code>: sections describing the main content.</li> <li><code>summary</code>: the summary sections (Abstract and Conclusion).</li> <li><code>metadata</code>: sections that do not describe the main content.</li> <li><code>extra</code>: sections that contain supplementary content.</li> </ul> </li> <li> <p><code>question_type</code>: either <code>sparse</code> or <code>dense</code>, as denoted in Generating questions.</p> </li> <li><code>answer_type</code>: either <code>long</code> or <code>short</code>, as denoted in Generating longer answers.</li> <li><code>prompt_type</code>: denotes the type of prompt that will be used:</li> </ul> <p>Defined in <code>make_simple_sample_for_openai</code>.</p> <ul> <li><code>simple</code>: intended for simple question-answer conversation, which expects the model to generate answers based on prior knowledge.</li> </ul> <p>Defined in <code>make_instruction_sample_for_openai</code>.</p> <ul> <li><code>instruction</code>: intended for instructed question-answer conversation (RAG-based), which contains relevant information to the question and expects the model to generate an answer based on the provided context.</li> </ul> <p>Actual data configuration</p> <p>Due to the limitation in the number of data, fine-tuning would not be sufficient to embed the document content into the model. Thus, it was decided that only a RAG-based strategy would be used in the final system.</p> <p>The final data was constructed using the following parameters:</p> <ul> <li>For the <code>train</code> set:<ul> <li>Only the <code>main</code> sections with <code>dense</code> questions and <code>long</code> answers were used to limit the number of questions to 100.</li> <li>Only the <code>instruction</code> prompt type was used since the intended system will be RAG-based.</li> </ul> </li> <li>For the <code>val</code> set:<ul> <li>Only the <code>summary</code> sections were used. The idea was any question generated from the summary would be answerable based on the main content.</li> <li>Both <code>dense</code> and <code>sparse</code> questions were used to increase the number of samples.</li> <li><code>long</code> answers and <code>instruction</code> prompt type were used so that the distribution is consistent with the <code>train</code> set.</li> </ul> </li> </ul> <p>Data generation for the source document was implemented in the <code>create_dataset.py</code> script.</p>"},{"location":"report/#model-fine-tuning","title":"Model Fine-tuning","text":"<p>The base model <code>gpt-3.5-turbo-1106</code> was fine-tuned using OpenAI's API for 3 epochs and batch size 1.</p> <p>Additionally, the training progress was logged to WandB: </p> <p>Implementation details and output</p> <p>The fine-tuning was implemented in the <code>finetune_openai.py</code> script. The resulting model is <code>ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r</code>.</p> <p>Since the training was handled by OpenAI, this project focused more on the data preparation process.</p>"},{"location":"report/#vector-store","title":"Vector-store","text":"<p>Retrieval frameworkd</p> <p>This project employed dense retrieval (embeddings similarity search) for the retrieval process. However, there are other options that can be explored depending on the use case:</p> <ul> <li>keyword-based (sparse retrieval)</li> <li>re-rank (also a type of dense retrieval)</li> <li>knowledge graph index (from LLamaIndex)</li> </ul>"},{"location":"report/#choosing-the-embedding-model","title":"Choosing the embedding model","text":"<p>For embedding tasks, an encoder model is more suitable than a decoder one such as GPT. Furthermore, encoder models are smaller compared to the scale of LLMs and can be run locally. According to the Massive Text Embedding Benchmark (MTEB) Leaderboard, there are many open-source options to choose from that produce state-of-the-art results.</p> <p>Implementation details</p> <p>In this project, the WhereIsAI/UAE-Large-V1 was used. This is a 335M parameter model at the size of only 1.34G which can fit most consumer-grade machines even without the need for a GPU. At the time of writing, this was ranked 3rd in the MTEB overall leaderboard, only coming after two 7B models.</p> <p>The downside of \"small\" encoder models like this is their small context length, which is only 512 tokens for the <code>UAE-Large-V1</code> model. This puts more emphasis on having an efficient strategy for chunking.</p>"},{"location":"report/#indexing-contents","title":"Indexing Contents","text":"<p>The vector store contains 2 types of entries:</p> <ul> <li><code>question</code> entries: the questions are used to create the embedding vectors and are stored along with the answers and the heading of the related section.</li> <li><code>chunk</code> entries: the chunks are used to create the embedding vector and are stored along with the heading of the related section.</li> </ul> <p>Implementation details</p> <p>ChromaDB was used for the vector store in this project. The DB creation process is implemented in the <code>create_chroma_db</code> function of the <code>pipeline.py</code> script.</p>"},{"location":"report/#rag-application","title":"RAG Application","text":"<p>The RAG application consists of 3 main parts: the pipeline containing the RAG logic, a backend with a chat endpoint, and a simple chat GUI.</p>"},{"location":"report/#rag-pipeline","title":"RAG Pipeline","text":"<p>The pipeline has a <code>retriever</code> that retrieves related content from the vector store and an <code>answerer</code> that generates the answer using the output of the <code>retriever</code> as the reference. Below is an overview diagram of the pipeline:</p> <pre><code>flowchart LR\n    query(query) --&gt; emb{{embedding\\nmodel}}\n\n    subgraph retriever[SemanticRetriever]\n        direction LR\n        vector-store[(vector\\nstore)]\n\n        emb --&gt; vector-store\n        vector-store --&gt; chunks([related\\nchunks])\n        vector-store --&gt; questions([similar\\nquestions])\n        questions --&gt; sections([related\\nsections])\n    end\n\n    sections --&gt; ref([references])\n    chunks --&gt; ref\n\n    query --&gt; thresh{similarity &gt; threshold}\n    questions --&gt; thresh\n\n    thresh -- true --&gt; answer(((answer &amp;\\nreferences)))\n    thresh -- false --&gt; answerer\n\n    ref --&gt; prompt(prompt)\n    query --&gt; prompt\n\n    subgraph answerer[AnswerGenerator]\n        direction LR\n        prompt --&gt; llm{{language\\nmodel}}\n    end\n\n    llm --&gt; answer\n    ref --&gt; answer</code></pre> <p>Implementation details</p> <p>The pipeline is implemented in the <code>pipeline.py</code> script.</p>"},{"location":"report/#retriever","title":"Retriever","text":"<p>The <code>retriever</code> uses the [embedding model] (#choosing-the-embedding-model) to compute the embedding of a user's query and then uses it to retrieve related content from the vector store.</p> <p>Some similar questions and related chunks are retrieved, and then depending on the similarity metrics, different behaviours are expected:</p> <ul> <li>If the most similar question has a similarity score higher than a threshold, its answer will be used as the final answer.</li> <li>If the most similar question has a similarity score lower than a threshold but higher than the average similarity score of retrieved chunks, then the content of the corresponding section will be used as the <code>reference</code>.</li> <li>If the most similar question has a similarity score lower than the average similarity score of retrieved chunks, then the content of those chunks will be used as the <code>reference</code>.</li> </ul>"},{"location":"report/#answerer","title":"Answerer","text":"<p>The <code>answerer</code> gets the user's query along the <code>reference</code> output from the <code>retriever</code> in a prompt template and send it to the fine-tuned language model to generate the answer.</p>"},{"location":"report/#chat-endpoint","title":"Chat endpoint","text":"<p>The backend was implemented using <code>Pydantic</code> and provides a <code>/chat</code> endpoint.</p> <p>The request scheme is defined by the <code>ChatRequest</code> class. It not only allows the user to send the query but also to specify which model to use, the temperature and the similarity threshold for picking references.</p> <p>The response scheme is defined by the <code>PipelineOutput</code> class. It contains the answer, the references, and optionally some related metadata.</p>"},{"location":"report/#chat-gui","title":"Chat GUI","text":"<p>A simple chat GUI implemented using <code>streamlit</code> was also included:</p> <p></p>"},{"location":"reference/","title":"API Reference","text":"<ul> <li>Core<ul> <li>Chunking</li> <li>Data Generation</li> <li>Data Validation</li> <li>Doc Tree</li> <li>Markdown</li> <li>Retrieval</li> </ul> </li> <li>Demo<ul> <li>Config</li> <li>Create Dataset</li> <li>Finetune Openai</li> <li>Pipeline</li> </ul> </li> <li>Types</li> </ul>"},{"location":"reference/types/","title":"Types","text":""},{"location":"reference/types/#types.ChatRequest","title":"ChatRequest","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a chat request.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message from user.</p> required <code>openai_model</code> <code>str</code> <p>The OpenAI model to use. Defaults to \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\".</p> required <code>temperature</code> <code>float</code> <p>The temperature of the model's output. Higher values make the output more random, while lower values make it more focused and deterministic. Defaults to 1.0.</p> required <code>certainty_threshold</code> <code>float</code> <p>The threshold for considering a question as certain. Defaults to 0.9.</p> required <code>uncertainty_threshold</code> <code>float</code> <p>The threshold for considering a question as uncertain. Defaults to 0.6.</p> required Source code in <code>docqa/types.py</code> <pre><code>class ChatRequest(BaseModel):\n    \"\"\"\n    Represents a chat request.\n\n    Args:\n        message (str): The message from user.\n        openai_model (str, optional): The OpenAI model to use. Defaults to\n            \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\".\n        temperature (float, optional): The temperature of the model's output.\n            Higher values make the output more random, while lower values make it\n            more focused and deterministic. Defaults to 1.0.\n        certainty_threshold (float, optional): The threshold for considering a\n            question as certain. Defaults to 0.9.\n        uncertainty_threshold (float, optional): The threshold for considering a\n            question as uncertain. Defaults to 0.6.\n    \"\"\"\n\n    message: str\n    openai_model: str = \"ft:gpt-3.5-turbo-1106:aitomatic-inc:gen-agent-v2:8dPNxr8r\"\n    temperature: float = 1.0\n    certainty_threshold: float = 0.9\n    uncertainty_threshold: float = 0.6\n</code></pre>"},{"location":"reference/types/#types.RetrievalReference","title":"RetrievalReference","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a reference.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the reference.</p> required <code>content</code> <code>str</code> <p>The content of the reference.</p> required Source code in <code>docqa/types.py</code> <pre><code>class RetrievalReference(BaseModel):\n    \"\"\"\n    Represents a reference.\n\n    Args:\n        source (str): The source of the reference.\n        content (str): The content of the reference.\n    \"\"\"\n\n    source: str\n    content: str\n</code></pre>"},{"location":"reference/types/#types.PipelineOutput","title":"PipelineOutput","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents the output of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>str</code> <p>The answer to the question.</p> required <code>references</code> <code>list[RetrievalReference]</code> <p>A list of RetrievalReference objects containing the related references.</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata associated with the answer.</p> required Source code in <code>docqa/types.py</code> <pre><code>class PipelineOutput(BaseModel):\n    \"\"\"\n    Represents the output of the pipeline.\n\n    Args:\n        answer (str): The answer to the question.\n        references (list[RetrievalReference]): A list of RetrievalReference objects\n            containing the related references.\n        metadata (dict): Additional metadata associated with the answer.\n    \"\"\"\n\n    answer: str\n    references: list[RetrievalReference]\n    metadata: dict | None = None\n</code></pre>"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/core/chunking/","title":"Chunking","text":""},{"location":"reference/core/chunking/#core.chunking.chunk_content","title":"chunk_content","text":"<pre><code>chunk_content(content, single_threshold=100, composite_threshold=200)\n</code></pre> <p>Generate a list of content chunks from a given string. The function will only split at a new line and never in the middle of a sentence. Which means it tries its best to preserve the structure of the original text.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The input string to be chunked.</p> required <code>single_threshold</code> <code>int</code> <p>The minimum length of a single chunk. Defaults to 100.</p> <code>100</code> <code>composite_threshold</code> <code>int</code> <p>The maximum length of a composite chunk. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of content chunks.</p> Description <ul> <li>This function takes a string <code>content</code> and splits it into smaller chunks based on the specified thresholds.</li> <li>It first splits the string into parts using the newline and carriage return characters as delimiters.</li> <li>It then iterates over each part and checks if the length of the part exceeds the <code>single_threshold</code>.</li> <li>If it does, it is considered a paragraph and added as a separate chunk.</li> <li>If the length of the current chunk exceeds the <code>composite_threshold</code>, it is also added as a separate chunk.</li> <li>Finally, the function returns a list of all the generated chunks.</li> </ul> Example <p><pre><code>content = (\n    # long paragraph\n    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit ... \\n\"\n    \"A quick brown fox jumps over the lazy dog.\n)\nchunk_content(content)\n</code></pre> <pre><code>['Lorem ipsum dolor sit amet, consectetur adipiscing elit ...',\n'A quick brown fox jumps over the lazy dog.']\n</code></pre></p> Source code in <code>docqa/core/chunking.py</code> <pre><code>def chunk_content(\n    content: str, single_threshold: int = 100, composite_threshold: int = 200\n) -&gt; list[str]:\n    \"\"\"\n    Generate a list of content chunks from a given string.\n    The function will only split at a new line and never in the middle of a sentence.\n    Which means it tries its best to preserve the structure of the original text.\n\n    Args:\n        content (str): The input string to be chunked.\n        single_threshold (int, optional): The minimum length of a single chunk.\n            Defaults to 100.\n        composite_threshold (int, optional): The maximum length of a composite chunk.\n            Defaults to 200.\n\n    Returns:\n        list[str]: A list of content chunks.\n\n    Description:\n        - This function takes a string `content` and splits it into smaller chunks based\n        on the specified thresholds.\n        - It first splits the string into parts using the newline and carriage return\n        characters as delimiters.\n        - It then iterates over each part and checks if the length of the part exceeds\n        the `single_threshold`.\n        - If it does, it is considered a paragraph and added as a separate chunk.\n        - If the length of the current chunk exceeds the `composite_threshold`, it is\n        also added as a separate chunk.\n        - Finally, the function returns a list of all the generated chunks.\n\n    Example:\n        ```python\n        content = (\n            # long paragraph\n            \"Lorem ipsum dolor sit amet, consectetur adipiscing elit ... \\\\n\"\n            \"A quick brown fox jumps over the lazy dog.\n        )\n        chunk_content(content)\n        ```\n        ```bash\n        ['Lorem ipsum dolor sit amet, consectetur adipiscing elit ...',\n        'A quick brown fox jumps over the lazy dog.']\n        ```\n    \"\"\"\n    if not content:\n        return []\n\n    parts = re.split(r\"[\\n\\r]+\", content)\n\n    chunks = []\n    cur_chunk: deque = deque([])\n    cur_length = 0\n\n    for text in parts:\n        text_length = len(text.split())\n        # if found a text big enough, this could be a paragraph\n        if text_length &gt; single_threshold:\n            # add the current chunk\n            if cur_chunk:\n                chunks.append(\"\\n\".join(cur_chunk))\n            cur_chunk = deque([])\n            cur_length = 0\n            # then add the found paragraph\n            chunks.append(text)\n        else:\n            # extend the current chunk\n            cur_chunk.append(text)\n            cur_length += text_length\n            # if the current chunk is big enough\n            if cur_length &gt; composite_threshold:\n                chunks.append(\"\\n\".join(cur_chunk))\n                # then shorten it down from the left\n                while cur_length &gt; single_threshold:\n                    discard = cur_chunk.popleft()\n                    cur_length -= len(discard.split())\n\n    # the last chunk\n    if cur_chunk:\n        chunks.append(\"\\n\".join(cur_chunk))\n\n    return chunks\n</code></pre>"},{"location":"reference/core/chunking/#core.chunking.chunk_size_stats","title":"chunk_size_stats","text":"<pre><code>chunk_size_stats(sections)\n</code></pre> <p>Calculates the statistics of the chunk sizes in the given list of sections.</p> Description <p>This function calculates the statistics of the chunk sizes in the given list of sections. It iterates through each section and splits the content into paragraphs using \"\\n\\n\" as the delimiter. It then calculates the length of each paragraph by splitting it into words and stores them in the <code>paragraph_lengths</code> list. After that, it filters out the paragraph lengths that are less than or equal to 100.</p> <p>Next, it prints the average paragraph length by calculating the sum of all paragraph lengths and dividing it by the number of paragraph lengths. It then prints the 90th percentile paragraph length by sorting the paragraph lengths in ascending order and selecting the index that corresponds to 90% of the length of the list.</p> <p>The function then initializes an empty dictionary <code>sections_details</code> to store the details of each section. It iterates through each section and checks if the heading matches any of the predefined keywords. If it does, it initializes an empty list <code>chunks</code>, otherwise it calls the <code>chunk_content</code> function to chunk the content and assigns the result to <code>chunks</code>. It then adds the details of the section to the <code>sections_details</code> dictionary.</p> <p>Finally, it prints the total number of chunks by summing the lengths of the <code>chunks</code> list for each section in <code>sections_details</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples containing a heading and content for each section. The content is a string.</p> required Source code in <code>docqa/core/chunking.py</code> <pre><code>def chunk_size_stats(sections: list[tuple[str, str]]):\n    \"\"\"Calculates the statistics of the chunk sizes in the given list of sections.\n\n    Description:\n        This function calculates the statistics of the chunk sizes in the given list of\n        sections. It iterates through each section and splits the content into\n        paragraphs using \"\\\\n\\\\n\" as the delimiter. It then calculates the length of\n        each paragraph by splitting it into words and stores them in the\n        `paragraph_lengths` list. After that, it filters out the paragraph lengths\n        that are less than or equal to 100.\n\n        Next, it prints the average paragraph length by calculating the sum of all\n        paragraph lengths and dividing it by the number of paragraph lengths. It then\n        prints the 90th percentile paragraph length by sorting the paragraph lengths in\n        ascending order and selecting the index that corresponds to 90% of the length\n        of the list.\n\n        The function then initializes an empty dictionary `sections_details` to store\n        the details of each section. It iterates through each section and checks if the\n        heading matches any of the predefined keywords. If it does, it initializes an\n        empty list `chunks`, otherwise it calls the `chunk_content` function to chunk\n        the content and assigns the result to `chunks`. It then adds the details of the\n        section to the `sections_details` dictionary.\n\n        Finally, it prints the total number of chunks by summing the lengths of the\n        `chunks` list for each section in `sections_details`.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples containing a heading and\n            content for each section. The content is a string.\n\n    \"\"\"\n    paragraph_lengths = []\n    for heading, content in sections:\n        paragraphs = content.split(\"\\n\\n\")\n        paragraph_lengths.extend([len(p.split()) for p in paragraphs])\n\n    paragraph_lengths = [length for length in paragraph_lengths if length &gt; 100]\n    print(\"average paragraph length:\", sum(paragraph_lengths) / len(paragraph_lengths))\n    print(\n        \"90 percentile paragraph length:\",\n        sorted(paragraph_lengths)[int(len(paragraph_lengths) * 0.9)],\n    )\n\n    sections_details = {}\n    for heading, content in sections:\n        if re.sub(r\"[^a-zA-Z0-9]\", \"\", heading.lower()) in (\n            \"reference\",\n            \"references\",\n            \"acknowledgement\",\n            \"acknowledgements\",\n        ):\n            chunks = []\n        else:\n            chunks = chunk_content(content)\n        sections_details[heading] = {\n            \"content\": content,\n            \"chunks\": chunks,\n        }\n    print(\n        \"Total number of chunks:\",\n        sum(len(sec[\"chunks\"]) for sec in sections_details.values()),\n    )\n</code></pre>"},{"location":"reference/core/data_generation/","title":"Data generation","text":""},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator","title":"QAPairGenerator","text":"<p>             Bases: <code>BaseModel</code></p> <p>Generates questions and answers for sections and subsections of a document.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The API key for OpenAI.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required Source code in <code>docqa/core/data_generation.py</code> <pre><code>class QAPairGenerator(BaseModel):\n    \"\"\"\n    Generates questions and answers for sections and subsections of a document.\n\n    Args:\n        openai_key (str): The API key for OpenAI.\n        openai_model (str): The name of the OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    output_format: str = (\n        \"Present your questions along with the detailed answers in the following JSON\"\n        ' format: [{\"question\": str, \"answer\": str}, ...].'\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def question_types(self) -&gt; dict[str, dict[str, str]]:\n        return {\n            \"sparse\": {\n                \"system_message\": (\n                    \"You are a professional examiner. Your job is to give questions to\"\n                    \" test people's understanding of a given document.\"\n                ),\n                \"instruction\": (\n                    \"You are given a document, your goal is:\\n- construct a list of\"\n                    \" different complex questions that can be answered based **solely**\"\n                    \" on the given text.\\n- make sure to cover all of the topics\"\n                    \" described in the document.\\n- include the answer for each\"\n                    \" question, the answers should be as detailed as\"\n                    \" possible.\\n\"\n                ),\n            },\n            \"dense\": {\n                \"system_message\": \"You are a top university professor.\",\n                \"instruction\": (\n                    \"You are a top university professor. You have the below text and\"\n                    \" you want to test the student's understanding of it. If you can\"\n                    \" only ask {num_questions} question(s) but must cover all of the\"\n                    \" content and the answer(s) to those questions must contain\"\n                    \" **solely** the information presented in the given text, what\"\n                    \" would you ask?\\n\"\n                ),\n            },\n        }\n\n    @staticmethod\n    def sanitize_output_format(output: dict | list) -&gt; list[dict]:\n        \"\"\"This static method takes in an `output` of type `dict` or `list` and returns\n            a sanitized `list[dict]` output.\n\n        Args:\n            output (dict | list): The input `output` that needs to be sanitized.\n\n        Returns:\n            list[dict]: The sanitized output as a list of dictionaries.\n\n        Raises:\n            ValueError: If the `output` format is invalid.\n\n        \"\"\"\n        if isinstance(output, dict):\n            if list(output.keys()) == [\"questions\"]:\n                if isinstance(output[\"questions\"], list):\n                    output = output[\"questions\"]\n            else:\n                output = [output]\n        elif isinstance(output, list):\n            if list(output[0].keys()) == [\"questions\"]:\n                output = output[0][\"questions\"]\n        else:\n            raise ValueError(f\"Invalid output format: {type(output)}\")\n\n        return output  # type: ignore[return-value]\n\n    def process(\n        self,\n        document: str,\n        temperature: float = 1.0,\n        question_type: str = \"dense\",\n        num_questions: int = 5,\n    ) -&gt; tuple[list[dict[str, str]], list[dict]]:\n        \"\"\"\n        Process the given document to generate a list of questions and answers.\n\n        Args:\n            document (str): The text document to process.\n            temperature (float, optional): The temperature parameter for controlling\n                the randomness of the output. Defaults to 1.0.\n            question_type (str, optional): The type of questions to generate.\n                Defaults to \"dense\".\n            num_questions (int, optional): The number of questions to generate.\n                Defaults to 5.\n\n        Returns:\n            tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of\n                questions and answers and a list of metadata.\n\n        Raises:\n            ValueError: If an invalid question type is provided.\n        \"\"\"\n        if question_type not in self.question_types:\n            raise ValueError(f\"Invalid question type: {question_type}\")\n\n        system_message = self.question_types[question_type][\"system_message\"]\n        instruction = self.question_types[question_type][\"instruction\"].format(\n            num_questions=num_questions\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\n                \"role\": \"user\",\n                \"content\": instruction + self.output_format,\n            },\n            {\"role\": \"user\", \"content\": \"\\nHere is the given text:\\n\\n\" + document},\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n            response_format={\"type\": \"json_object\"},\n        )\n        text = response.choices[0].message.content.strip()\n\n        output = json.loads(text)\n        output = self.sanitize_output_format(output)\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = response.usage.model_dump()\n\n        return output, metadata  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator.sanitize_output_format","title":"sanitize_output_format  <code>staticmethod</code>","text":"<pre><code>sanitize_output_format(output)\n</code></pre> <p>This static method takes in an <code>output</code> of type <code>dict</code> or <code>list</code> and returns     a sanitized <code>list[dict]</code> output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>dict | list</code> <p>The input <code>output</code> that needs to be sanitized.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The sanitized output as a list of dictionaries.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>output</code> format is invalid.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>@staticmethod\ndef sanitize_output_format(output: dict | list) -&gt; list[dict]:\n    \"\"\"This static method takes in an `output` of type `dict` or `list` and returns\n        a sanitized `list[dict]` output.\n\n    Args:\n        output (dict | list): The input `output` that needs to be sanitized.\n\n    Returns:\n        list[dict]: The sanitized output as a list of dictionaries.\n\n    Raises:\n        ValueError: If the `output` format is invalid.\n\n    \"\"\"\n    if isinstance(output, dict):\n        if list(output.keys()) == [\"questions\"]:\n            if isinstance(output[\"questions\"], list):\n                output = output[\"questions\"]\n        else:\n            output = [output]\n    elif isinstance(output, list):\n        if list(output[0].keys()) == [\"questions\"]:\n            output = output[0][\"questions\"]\n    else:\n        raise ValueError(f\"Invalid output format: {type(output)}\")\n\n    return output  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.QAPairGenerator.process","title":"process","text":"<pre><code>process(document, temperature=1.0, question_type='dense', num_questions=5)\n</code></pre> <p>Process the given document to generate a list of questions and answers.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>str</code> <p>The text document to process.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling the randomness of the output. Defaults to 1.0.</p> <code>1.0</code> <code>question_type</code> <code>str</code> <p>The type of questions to generate. Defaults to \"dense\".</p> <code>'dense'</code> <code>num_questions</code> <code>int</code> <p>The number of questions to generate. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple[list[dict[str, str]], list[dict]]</code> <p>tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of questions and answers and a list of metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid question type is provided.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def process(\n    self,\n    document: str,\n    temperature: float = 1.0,\n    question_type: str = \"dense\",\n    num_questions: int = 5,\n) -&gt; tuple[list[dict[str, str]], list[dict]]:\n    \"\"\"\n    Process the given document to generate a list of questions and answers.\n\n    Args:\n        document (str): The text document to process.\n        temperature (float, optional): The temperature parameter for controlling\n            the randomness of the output. Defaults to 1.0.\n        question_type (str, optional): The type of questions to generate.\n            Defaults to \"dense\".\n        num_questions (int, optional): The number of questions to generate.\n            Defaults to 5.\n\n    Returns:\n        tuple[list[dict[str, str]], list[dict]]: A tuple containing a list of\n            questions and answers and a list of metadata.\n\n    Raises:\n        ValueError: If an invalid question type is provided.\n    \"\"\"\n    if question_type not in self.question_types:\n        raise ValueError(f\"Invalid question type: {question_type}\")\n\n    system_message = self.question_types[question_type][\"system_message\"]\n    instruction = self.question_types[question_type][\"instruction\"].format(\n        num_questions=num_questions\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\n            \"role\": \"user\",\n            \"content\": instruction + self.output_format,\n        },\n        {\"role\": \"user\", \"content\": \"\\nHere is the given text:\\n\\n\" + document},\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n        response_format={\"type\": \"json_object\"},\n    )\n    text = response.choices[0].message.content.strip()\n\n    output = json.loads(text)\n    output = self.sanitize_output_format(output)\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = response.usage.model_dump()\n\n    return output, metadata  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.AnswerGenerator","title":"AnswerGenerator","text":"<p>             Bases: <code>BaseModel</code></p> <p>Generate an answer to a question based on a reference.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The OpenAI API key.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required Source code in <code>docqa/core/data_generation.py</code> <pre><code>class AnswerGenerator(BaseModel):\n    \"\"\"Generate an answer to a question based on a reference.\n\n    Args:\n        openai_key (str): The OpenAI API key.\n        openai_model (str): The name of the OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    system_message: str = (\n        \"You are a trusted factual chatbot. You always answer questions based strictly\"\n        \" on the provided reference.\"\n    )\n    instruction: str = (\n        \"Reference(s):\\n\\n{reference}\\n\\nStrictly according to the provided\"\n        \" reference(s), give an answer as detailed as possible to the following\"\n        \" question: {question}\"\n    )\n\n    def process(\n        self,\n        question: str,\n        reference: str,\n        temperature: float = 1.0,\n    ) -&gt; tuple[str, dict]:\n        \"\"\"\n        Process the given question and generate a response using the OpenAI model.\n\n        Parameters:\n            question (str): The question to be processed.\n            reference (str): The reference string for the instruction.\n            temperature (float, optional): The temperature parameter for generating the\n                response. Higher values (e.g., 1.0) make the output more random, while\n                lower values (e.g., 0.2) make it more focused and deterministic.\n                Defaults to 1.0.\n\n        Returns:\n            Tuple[str, dict]: A tuple containing the generated answer and metadata.\n\n        Output dict structure:\n            - answer (str): The generated answer as a string.\n            - metadata (dict): Additional metadata about the response.\n                - finish_reason (str): The reason why the completion finished.\n                - usage (dict): Usage statistics of the completion.\n                    - completed_tokens (int): The number of tokens used for\n                        completion.\n                    - prompt_tokens (int): The number of tokens used for the prompt.\n                    - total_tokens (int): The total number of tokens used.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_message},\n            {\n                \"role\": \"user\",\n                \"content\": self.instruction.format(\n                    reference=reference, question=question\n                ),\n            },\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n        )\n        answer = response.choices[0].message.content.strip()\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = {\n            \"completed_tokens\": response.usage.completion_tokens,\n            \"prompt_tokens\": response.usage.prompt_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n        return answer, metadata\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.AnswerGenerator.process","title":"process","text":"<pre><code>process(question, reference, temperature=1.0)\n</code></pre> <p>Process the given question and generate a response using the OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to be processed.</p> required <code>reference</code> <code>str</code> <p>The reference string for the instruction.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for generating the response. Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple[str, dict]</code> <p>Tuple[str, dict]: A tuple containing the generated answer and metadata.</p> Output dict structure <ul> <li>answer (str): The generated answer as a string.</li> <li>metadata (dict): Additional metadata about the response.<ul> <li>finish_reason (str): The reason why the completion finished.</li> <li>usage (dict): Usage statistics of the completion.<ul> <li>completed_tokens (int): The number of tokens used for     completion.</li> <li>prompt_tokens (int): The number of tokens used for the prompt.</li> <li>total_tokens (int): The total number of tokens used.</li> </ul> </li> </ul> </li> </ul> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def process(\n    self,\n    question: str,\n    reference: str,\n    temperature: float = 1.0,\n) -&gt; tuple[str, dict]:\n    \"\"\"\n    Process the given question and generate a response using the OpenAI model.\n\n    Parameters:\n        question (str): The question to be processed.\n        reference (str): The reference string for the instruction.\n        temperature (float, optional): The temperature parameter for generating the\n            response. Higher values (e.g., 1.0) make the output more random, while\n            lower values (e.g., 0.2) make it more focused and deterministic.\n            Defaults to 1.0.\n\n    Returns:\n        Tuple[str, dict]: A tuple containing the generated answer and metadata.\n\n    Output dict structure:\n        - answer (str): The generated answer as a string.\n        - metadata (dict): Additional metadata about the response.\n            - finish_reason (str): The reason why the completion finished.\n            - usage (dict): Usage statistics of the completion.\n                - completed_tokens (int): The number of tokens used for\n                    completion.\n                - prompt_tokens (int): The number of tokens used for the prompt.\n                - total_tokens (int): The total number of tokens used.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.system_message},\n        {\n            \"role\": \"user\",\n            \"content\": self.instruction.format(\n                reference=reference, question=question\n            ),\n        },\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n    )\n    answer = response.choices[0].message.content.strip()\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = {\n        \"completed_tokens\": response.usage.completion_tokens,\n        \"prompt_tokens\": response.usage.prompt_tokens,\n        \"total_tokens\": response.usage.total_tokens,\n    }\n\n    return answer, metadata\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.generate_top_sections_questions","title":"generate_top_sections_questions","text":"<pre><code>generate_top_sections_questions(doc_tree, output_file, openai_key='', openai_model='', seed=42, temperature=1.0)\n</code></pre> <p>Generate the top sections with questions based on the provided document tree.</p> <p>Parameters:</p> Name Type Description Default <code>doc_tree</code> <code>dict</code> <p>The document tree representing the sections of the document.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the top sections with uestions will be saved.</p> required <code>openai_key</code> <code>str</code> <p>The OpenAI API key. Defaults to an empty string.</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The OpenAI model to use for question generation. Defaults to an empty string.</p> <code>''</code> <code>seed</code> <code>int</code> <p>The seed value for random number generation. Defaults to 42.</p> <code>42</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for question generation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The top sections with questions.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def generate_top_sections_questions(\n    doc_tree: dict,\n    output_file: Path,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    temperature: float = 1.0,\n) -&gt; dict:\n    \"\"\"\n    Generate the top sections with questions based on the provided document tree.\n\n    Args:\n        doc_tree (dict): The document tree representing the sections of the document.\n        output_file (Path): The path to the output file where the top sections with\n            uestions will be saved.\n        openai_key (str, optional): The OpenAI API key. Defaults to an empty string.\n        openai_model (str, optional): The OpenAI model to use for question generation.\n            Defaults to an empty string.\n        seed (int, optional): The seed value for random number generation.\n            Defaults to 42.\n        temperature (float, optional): The temperature parameter for question\n            generation.\n            Defaults to 1.0.\n\n    Returns:\n        dict: The top sections with questions.\n    \"\"\"\n    output_file = Path(output_file)\n    if output_file.exists():\n        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n            top_sections_with_questions = json.load(f)\n        return top_sections_with_questions\n\n    qa_gen = QAPairGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        seed=seed,\n    )\n\n    top_sections_with_questions = {}\n    if doc_tree[\"text\"]:\n        top_sections_with_questions[\"\"] = {\n            \"text\": doc_tree[\"text\"],\n            \"chunks_count\": len(chunk_content(doc_tree[\"text\"])),\n        }\n\n    for section in doc_tree.get(\"child_sections\", []):\n        full_text = get_section_full_text(section)\n        top_sections_with_questions[section[\"heading\"]] = {\n            \"text\": full_text,\n            \"chunks_count\": len(chunk_content(full_text)),\n        }\n\n    for heading, section in top_sections_with_questions.items():\n        print(f\"Generating questions for {heading}\")\n        dense_questions, _ = qa_gen.process(\n            section[\"text\"],\n            question_type=\"dense\",\n            num_questions=section[\"chunks_count\"],\n            temperature=temperature,\n        )\n        sparse_questions, _ = qa_gen.process(\n            section[\"text\"],\n            question_type=\"sparse\",\n            temperature=temperature,\n        )\n        top_sections_with_questions[heading][\"dense_questions\"] = dense_questions\n        top_sections_with_questions[heading][\"sparse_questions\"] = sparse_questions\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(top_sections_with_questions, f, indent=4, ensure_ascii=False)\n\n    return top_sections_with_questions\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.generate_long_answers_for_sections_questions","title":"generate_long_answers_for_sections_questions","text":"<pre><code>generate_long_answers_for_sections_questions(sections_with_questions, output_file, openai_key='', openai_model='', seed=42, temperature=1.0)\n</code></pre> <p>Generate long answers for sections' questions.</p> <p>Parameters:</p> Name Type Description Default <code>sections_with_questions</code> <code>dict</code> <p>A dictionary containing sections with their corresponding questions.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the generated long answers will be stored.</p> required <code>openai_key</code> <code>str</code> <p>The API key for OpenAI. Defaults to an empty string.</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use. Defaults to an empty string.</p> <code>''</code> <code>seed</code> <code>int</code> <p>The seed value for random number generation. Defaults to 42.</p> <code>42</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for generating answers. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing sections with their corresponding questions and generated long answers.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def generate_long_answers_for_sections_questions(\n    sections_with_questions: dict,\n    output_file: Path,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    temperature: float = 1.0,\n) -&gt; dict:\n    \"\"\"\n    Generate long answers for sections' questions.\n\n    Args:\n        sections_with_questions (dict): A dictionary containing sections with their\n            corresponding questions.\n        output_file (Path): The path to the output file where the generated long answers\n            will be stored.\n        openai_key (str, optional): The API key for OpenAI. Defaults to an empty string.\n        openai_model (str, optional): The name of the OpenAI model to use. Defaults to\n            an empty string.\n        seed (int, optional): The seed value for random number generation.\n            Defaults to 42.\n        temperature (float, optional): The temperature parameter for generating answers.\n            Defaults to 1.0.\n\n    Returns:\n        dict: A dictionary containing sections with their corresponding questions and\n            generated long answers.\n    \"\"\"\n    output_file = Path(output_file)\n    if output_file.exists():\n        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n            sections_with_questions_and_long_answers = json.load(f)\n        return sections_with_questions_and_long_answers\n\n    answer_gen = AnswerGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        seed=seed,\n    )\n    for heading, section in sections_with_questions.items():\n        print(f\"Generating long answers for dense questions of {heading}\")\n        reference = f\"===\\n[source: {heading}]\\n{section['text']}\\n===\\n\"\n\n        for question in section[\"dense_questions\"]:\n            answer, _ = answer_gen.process(\n                question=question[\"question\"],\n                reference=reference,\n                temperature=temperature,\n            )\n            question[\"long_answer\"] = answer\n\n        print(f\"Generating long answers for sparse questions of {heading}\")\n        for question in section[\"sparse_questions\"]:\n            answer, _ = answer_gen.process(\n                question=question[\"question\"],\n                reference=reference,\n                temperature=temperature,\n            )\n            question[\"long_answer\"] = answer\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(sections_with_questions, f, indent=4, ensure_ascii=False)\n\n    return sections_with_questions\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.make_simple_sample_for_openai","title":"make_simple_sample_for_openai","text":"<pre><code>make_simple_sample_for_openai(question, answer)\n</code></pre> <p>Generates a simple sample for OpenAI chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The user's question.</p> required <code>answer</code> <code>str</code> <p>The assistant's answer.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the chat conversation sample.</p> Example <p>make_simple_sample_for_openai(\"What is the capital of France?\", \"Paris\")</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def make_simple_sample_for_openai(question: str, answer: str) -&gt; dict:\n    \"\"\"\n    Generates a simple sample for OpenAI chat conversation.\n\n    Args:\n        question (str): The user's question.\n        answer (str): The assistant's answer.\n\n    Returns:\n        dict: A dictionary containing the chat conversation sample.\n\n    Example:\n        make_simple_sample_for_openai(\"What is the capital of France?\", \"Paris\")\n    \"\"\"\n    return {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a trusted factual chatbot that only answers questions\"\n                    \" about generative agents.\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n    }\n</code></pre>"},{"location":"reference/core/data_generation/#core.data_generation.make_instruction_sample_for_openai","title":"make_instruction_sample_for_openai","text":"<pre><code>make_instruction_sample_for_openai(question, answer, references)\n</code></pre> <p>Generates an instruction sample for OpenAI chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to be used in the instruction.</p> required <code>answer</code> <code>str</code> <p>The answer to be used in the instruction.</p> required <code>references</code> <code>list[str]</code> <p>A list of reference texts to be included in the instruction.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the chat conversation sample.</p> Source code in <code>docqa/core/data_generation.py</code> <pre><code>def make_instruction_sample_for_openai(\n    question: str, answer: str, references: list[str]\n) -&gt; dict:\n    \"\"\"\n    Generates an instruction sample for OpenAI chat conversation.\n\n    Args:\n        question (str): The question to be used in the instruction.\n        answer (str): The answer to be used in the instruction.\n        references (list[str]): A list of reference texts to be included in the\n            instruction.\n\n    Returns:\n        dict: A dictionary containing the chat conversation sample.\n    \"\"\"\n    reference_text = \"\\n\\n\".join([\"===\\n\" + ref + \"\\n===\" for ref in references])\n    system_message = AnswerGenerator.model_fields[\"system_message\"].default\n    instruction = AnswerGenerator.model_fields[\"instruction\"].default\n    instruction = instruction.format(reference=reference_text, question=question)\n    return {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": system_message,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": instruction,\n            },\n            {\"role\": \"assistant\", \"content\": answer},\n        ]\n    }\n</code></pre>"},{"location":"reference/core/data_validation/","title":"Data validation","text":"<p>Source: https://cookbook.openai.com/examples/chat_finetuning_data_prep</p>"},{"location":"reference/core/doc_tree/","title":"Doc tree","text":""},{"location":"reference/core/doc_tree/#core.doc_tree.build_doc_tree_from_markdown","title":"build_doc_tree_from_markdown","text":"<pre><code>build_doc_tree_from_markdown(text)\n</code></pre> <p>Takes a string representation of a markdown file as input. Finds the highest level of heading and splits the text into sections accordingly. Returns a list of tuples, each containing the section title and section content.</p> <pre><code>{\n    \"heading\": \"Section 1\",\n    \"text\": \"Section 1 opening text\",\n    \"child_sections\": [\n        {\n            \"heading\": \"Section 1.1\",\n            \"text\": \"Section 1.1 opening text\",\n            \"child_sections\": [\n                ...\n            ]\n        },\n        ...\n    ]\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The content of a markdown file as a single string.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the tree structure of the markdown file.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def build_doc_tree_from_markdown(\n    text: str,\n) -&gt; dict:\n    \"\"\"\n    Takes a string representation of a markdown file as input.\n    Finds the highest level of heading and splits the text into sections accordingly.\n    Returns a list of tuples, each containing the section title and section content.\n\n    ```python\n    {\n        \"heading\": \"Section 1\",\n        \"text\": \"Section 1 opening text\",\n        \"child_sections\": [\n            {\n                \"heading\": \"Section 1.1\",\n                \"text\": \"Section 1.1 opening text\",\n                \"child_sections\": [\n                    ...\n                ]\n            },\n            ...\n        ]\n    }\n    ```\n\n    Args:\n        text (str): The content of a markdown file as a single string.\n\n    Returns:\n        dict: A dictionary containing the tree structure of the markdown file.\n\n    \"\"\"\n    lines = text.strip().split(\"\\n\")\n\n    # Find the highest heading level\n    highest_heading_level = find_highest_markdown_heading_level(lines)\n\n    # If there are no headings, return the text as a single section\n    if highest_heading_level is None:\n        return {\"heading\": \"\", \"text\": text}\n\n    # Construct the heading prefix for splitting\n    headings_prefix = (\"#\" * highest_heading_level) + \" \"\n\n    n = len(lines)\n    i = 0\n    opening_text_lines = []\n    while i &lt; n and not lines[i].startswith(headings_prefix):\n        opening_text_lines.append(lines[i])\n        i += 1\n\n    root = {\n        \"heading\": \"\",\n        \"text\": \"\\n\".join(opening_text_lines).strip(),\n        \"child_sections\": [],\n    }\n\n    current_section_title = \"\"\n    current_section_lines: list[str] = []\n\n    # Split the text at the highest heading level\n    while i &lt; n:\n        line = lines[i]\n        # Check if the line starts with the highest heading level prefix\n        if line.startswith(headings_prefix):\n            # If the current_section is not empty, add it to the sections list\n            if len(current_section_lines) &gt; 0:\n                current_section_body = \"\\n\".join(current_section_lines).strip()\n                child_section = build_doc_tree_from_markdown(current_section_body)\n                child_section[\"heading\"] = current_section_title\n                root[\"child_sections\"].append(child_section)  # type: ignore\n\n            # Update the current_section_title and clear the current_section\n            current_section_title = line.strip()\n            current_section_lines = []\n        else:\n            # Add the line to the current_section\n            current_section_lines.append(line)\n        i += 1\n\n    # Add the last section to the sections list (if not empty)\n    if len(current_section_lines) &gt; 0:\n        current_section_body = \"\\n\".join(current_section_lines).strip()\n        child_section = build_doc_tree_from_markdown(current_section_body)\n        child_section[\"heading\"] = current_section_title\n        root[\"child_sections\"].append(child_section)  # type: ignore[attr-defined]\n\n    return root\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.build_doc_tree_from_pdf","title":"build_doc_tree_from_pdf","text":"<pre><code>build_doc_tree_from_pdf(input_file, output_dir)\n</code></pre> <p>Generate a document tree from a PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>The path to the input PDF file.</p> required <code>output_dir</code> <code>Path</code> <p>The directory where the output files will be saved.</p> required Notes <ul> <li>The function first checks if the marker output file exists in the output     directory.</li> <li>If the marker output file exists, it reads the content of the file.</li> <li>If the marker output file does not exist, it converts the input PDF file to     markdown using the <code>pdf_to_markdown</code> function.</li> <li>The function then checks if the tidy text sections file exists in the output     directory.</li> <li>If the tidy text sections file exists, it reads the content of the file.</li> <li>If the tidy text sections file does not exist, it builds a document tree from     the marker markdown content using the <code>build_doc_tree_from_markdown</code>     function.</li> <li>The function flattens the document tree using the <code>flatten_doc_tree</code> function.</li> <li>It preprocesses the sections using the <code>preprocess_sections</code> function.</li> <li>The function then tidies the markdown sections and retrieves the metadata     using the <code>tidy_markdown_sections</code> function.</li> <li>Finally, it saves the tidy text sections to a file, writes the tidy markdown     content to a file, and saves the metadata to a file.</li> </ul> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The final document tree generated from the PDF.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the marker output file or tidy text sections file does not exist.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def build_doc_tree_from_pdf(input_file: Path, output_dir: Path) -&gt; dict:\n    \"\"\"\n    Generate a document tree from a PDF file.\n\n    Args:\n        input_file (Path): The path to the input PDF file.\n        output_dir (Path): The directory where the output files will be saved.\n\n    Notes:\n        - The function first checks if the marker output file exists in the output\n            directory.\n        - If the marker output file exists, it reads the content of the file.\n        - If the marker output file does not exist, it converts the input PDF file to\n            markdown using the `pdf_to_markdown` function.\n        - The function then checks if the tidy text sections file exists in the output\n            directory.\n        - If the tidy text sections file exists, it reads the content of the file.\n        - If the tidy text sections file does not exist, it builds a document tree from\n            the marker markdown content using the `build_doc_tree_from_markdown`\n            function.\n        - The function flattens the document tree using the `flatten_doc_tree` function.\n        - It preprocesses the sections using the `preprocess_sections` function.\n        - The function then tidies the markdown sections and retrieves the metadata\n            using the `tidy_markdown_sections` function.\n        - Finally, it saves the tidy text sections to a file, writes the tidy markdown\n            content to a file, and saves the metadata to a file.\n\n    Returns:\n        dict: The final document tree generated from the PDF.\n\n    Raises:\n        FileNotFoundError: If the marker output file or tidy text sections file does\n            not exist.\n    \"\"\"\n    marker_output_file = output_dir / \"marker_output.md\"\n\n    if marker_output_file.exists():\n        with open(marker_output_file, \"r\", encoding=\"utf-8\") as f:\n            marker_markdown = f.read()\n    else:\n        cache_dir = output_dir / \"pdf_to_markdown_cache/\"\n        marker_markdown = pdf_to_markdown(\n            input_file, marker_output_file, cache_dir=cache_dir\n        )\n\n    tidy_text_sections_file = output_dir / \"tidy_text_sections.json\"\n    tidy_markdown_file = output_dir / \"tidy_output.md\"\n\n    if tidy_text_sections_file.exists():\n        with open(tidy_text_sections_file, \"r\", encoding=\"utf-8\") as f:\n            tidy_text_sections = json.load(f)\n        tidy_markdown = \"\\n\\n\".join(tidy_text_sections)\n    else:\n        doc_tree = build_doc_tree_from_markdown(marker_markdown)\n        sections = flatten_doc_tree(doc_tree)\n        sections = preprocess_sections(sections)\n        tidy_sections, all_metadata = tidy_markdown_sections(\n            sections,\n            openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n            openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n            seed=int(os.getenv(\"SEED\", 42)),\n        )\n\n        tidy_text_sections = [\n            f\"{heading.strip()}\\n\\n{content.strip()}\".strip()\n            for heading, content in tidy_sections\n        ]\n        tidy_markdown = \"\\n\\n\".join(tidy_text_sections)\n\n        with open(tidy_text_sections_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(tidy_text_sections, f, indent=4)\n\n        with open(tidy_markdown_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(tidy_markdown)\n\n        with open(output_dir / \"tidy_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(all_metadata, f, indent=4)\n\n        print(\n            \"total completion tokens:\",\n            sum([m.get(\"usage\", {}).get(\"total_tokens\", 0) for m in all_metadata]),\n        )\n        print(\n            \"total prompt tokens:\",\n            sum([m.get(\"usage\", {}).get(\"prompt_tokens\", 0) for m in all_metadata]),\n        )\n        print(\n            \"total completed tokens:\",\n            sum([m.get(\"usage\", {}).get(\"completed_tokens\", 0) for m in all_metadata]),\n        )\n\n    final_doc_tree = build_doc_tree_from_markdown(tidy_markdown)\n    doc_tree_file = output_dir / \"doc_tree.json\"\n    with open(doc_tree_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_doc_tree, f, indent=4)\n\n    return final_doc_tree\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.flatten_doc_tree","title":"flatten_doc_tree","text":"<pre><code>flatten_doc_tree(root)\n</code></pre> <p>Recursively flattens a nested dictionary representing a document tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>dict</code> <p>The root node of the document tree.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of tuples representing the flattened document tree. Each tuple contains a heading and its corresponding text.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def flatten_doc_tree(root: dict) -&gt; list:\n    \"\"\"\n    Recursively flattens a nested dictionary representing a document tree.\n\n    Parameters:\n        root (dict): The root node of the document tree.\n\n    Returns:\n        list: A list of tuples representing the flattened document tree. Each tuple\n            contains a heading and its corresponding text.\n    \"\"\"\n    if root[\"heading\"] or root[\"text\"]:\n        sections = [(root[\"heading\"], root[\"text\"])]\n    else:\n        sections = []\n    for section in root.get(\"child_sections\", []):\n        sections.extend(flatten_doc_tree(section))\n    return sections\n</code></pre>"},{"location":"reference/core/doc_tree/#core.doc_tree.get_section_full_text","title":"get_section_full_text","text":"<pre><code>get_section_full_text(section)\n</code></pre> <p>Retrieves the full text of a section from a given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>section</code> <code>dict</code> <p>The section to retrieve the full text from.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full text of the section.</p> Source code in <code>docqa/core/doc_tree.py</code> <pre><code>def get_section_full_text(section: dict) -&gt; str:\n    \"\"\"\n    Retrieves the full text of a section from a given dictionary.\n\n    Args:\n        section (dict): The section to retrieve the full text from.\n\n    Returns:\n        str: The full text of the section.\n    \"\"\"\n    flatten_sections = flatten_doc_tree(section)\n    text_sections = [\n        f\"{heading.strip()}\\n\\n{content.strip()}\".strip()\n        for heading, content in flatten_sections\n    ]\n\n    full_text = \"\\n\\n\".join(text_sections)\n\n    return full_text\n</code></pre>"},{"location":"reference/core/markdown/","title":"Markdown","text":""},{"location":"reference/core/markdown/#core.markdown.MarkdownTidier","title":"MarkdownTidier","text":"<p>             Bases: <code>BaseModel</code></p> <p>Tidies the given markdown text using OpenAI's model.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The OpenAI API key.</p> required <code>openai_model</code> <code>str</code> <p>The OpenAI model to use.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 42.</p> required <code>system_message</code> <code>str</code> <p>The system message for the OpenAI model.</p> required <code>instruction</code> <code>str</code> <p>The instruction for the OpenAI model.</p> required <code>api_client</code> <code>OpenAI</code> <p>The OpenAI client.</p> required Source code in <code>docqa/core/markdown.py</code> <pre><code>class MarkdownTidier(BaseModel):\n    \"\"\"\n    Tidies the given markdown text using OpenAI's model.\n\n    Args:\n        openai_key (str): The OpenAI API key.\n        openai_model (str): The OpenAI model to use.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n        system_message (str, optional): The system message for the OpenAI model.\n        instruction (str, optional): The instruction for the OpenAI model.\n        api_client (OpenAI): The OpenAI client.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    openai_key: str\n    openai_model: str\n    seed: int = 42\n    system_message: str = (\n        \"You are a professional editor. Your job is to reconstruct the broken markdown\"\n        \" text.\"\n    )\n    instruction: str = (\n        \"You are given a markdown text which was converted from pdf and thus has \"\n        \"mixed-up sentences and paragraphs structure, your job is:\\n\"\n        \"- reconstruct the text with proper sentences and paragraphs.\\n\"\n        \"- keep the headings unchanged.\\n\"\n        \"- keep the original content verbatim.\\n\"\n        \"- discard unrelated text.\\n\"\n        \"Answer with **only the reconstructed text**  and nothing else.\\n\"\n    )\n\n    @computed_field  # type: ignore[misc]\n    @property\n    def openai_client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_key)\n\n    def process(self, markdown_text: str, temperature: float = 0.7) -&gt; tuple[str, dict]:\n        \"\"\"\n        Generates a response to a given markdown text using the OpenAI chat model.\n\n        Args:\n            markdown_text (str): The input markdown text to generate a response for.\n            temperature (float, optional): The temperature of the model's output.\n                Higher values make the output more random, while lower values make it\n                more focused and deterministic. Defaults to 0.7.\n\n        Returns:\n            str: The generated response text.\n            dict: Metadata about the completion process, including the finish reason\n                and token usage.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; process(\"Hello, how are you?\")\n            ```\n            ```shel\n            (\n                'I am fine, thank you!',\n                {\n                    'finish_reason': 'stop',\n                    'usage': {\n                        'completed_tokens': 48,\n                        'prompt_tokens': 6,\n                        'total_tokens': 54\n                    }\n                }\n            )\n            ```\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_message},\n            {\"role\": \"user\", \"content\": self.instruction},\n            {\"role\": \"user\", \"content\": markdown_text},\n        ]\n\n        response = self.openai_client.chat.completions.create(\n            model=self.openai_model,\n            messages=messages,\n            temperature=temperature,\n            seed=self.seed,\n        )\n\n        text = response.choices[0].message.content.strip()\n\n        metadata = {}\n        metadata[\"finish_reason\"] = response.choices[0].finish_reason\n        metadata[\"usage\"] = {\n            \"completed_tokens\": response.usage.completion_tokens,\n            \"prompt_tokens\": response.usage.prompt_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n        return text, metadata\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.MarkdownTidier.process","title":"process","text":"<pre><code>process(markdown_text, temperature=0.7)\n</code></pre> <p>Generates a response to a given markdown text using the OpenAI chat model.</p> <p>Parameters:</p> Name Type Description Default <code>markdown_text</code> <code>str</code> <p>The input markdown text to generate a response for.</p> required <code>temperature</code> <code>float</code> <p>The temperature of the model's output. Higher values make the output more random, while lower values make it more focused and deterministic. Defaults to 0.7.</p> <code>0.7</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response text.</p> <code>dict</code> <code>dict</code> <p>Metadata about the completion process, including the finish reason and token usage.</p> Example <p><pre><code>&gt;&gt;&gt; process(\"Hello, how are you?\")\n</code></pre> <pre><code>(\n    'I am fine, thank you!',\n    {\n        'finish_reason': 'stop',\n        'usage': {\n            'completed_tokens': 48,\n            'prompt_tokens': 6,\n            'total_tokens': 54\n        }\n    }\n)\n</code></pre></p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def process(self, markdown_text: str, temperature: float = 0.7) -&gt; tuple[str, dict]:\n    \"\"\"\n    Generates a response to a given markdown text using the OpenAI chat model.\n\n    Args:\n        markdown_text (str): The input markdown text to generate a response for.\n        temperature (float, optional): The temperature of the model's output.\n            Higher values make the output more random, while lower values make it\n            more focused and deterministic. Defaults to 0.7.\n\n    Returns:\n        str: The generated response text.\n        dict: Metadata about the completion process, including the finish reason\n            and token usage.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; process(\"Hello, how are you?\")\n        ```\n        ```shel\n        (\n            'I am fine, thank you!',\n            {\n                'finish_reason': 'stop',\n                'usage': {\n                    'completed_tokens': 48,\n                    'prompt_tokens': 6,\n                    'total_tokens': 54\n                }\n            }\n        )\n        ```\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.system_message},\n        {\"role\": \"user\", \"content\": self.instruction},\n        {\"role\": \"user\", \"content\": markdown_text},\n    ]\n\n    response = self.openai_client.chat.completions.create(\n        model=self.openai_model,\n        messages=messages,\n        temperature=temperature,\n        seed=self.seed,\n    )\n\n    text = response.choices[0].message.content.strip()\n\n    metadata = {}\n    metadata[\"finish_reason\"] = response.choices[0].finish_reason\n    metadata[\"usage\"] = {\n        \"completed_tokens\": response.usage.completion_tokens,\n        \"prompt_tokens\": response.usage.prompt_tokens,\n        \"total_tokens\": response.usage.total_tokens,\n    }\n\n    return text, metadata\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.find_highest_markdown_heading_level","title":"find_highest_markdown_heading_level","text":"<pre><code>find_highest_markdown_heading_level(lines)\n</code></pre> <p>Takes a list of lines representing a markdown file as input. Finds the highest level of heading and returns it as an integer. Returns None if the text contains no headings.</p> Source <p>https://github.com/nestordemeure/question_extractor/blob/main/question_extractor/markdown.py</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list of str</code> <p>A list of lines in the markdown file.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>int | None: The highest heading level as an integer, or None if no headings are found.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def find_highest_markdown_heading_level(lines: list[str]) -&gt; int | None:\n    \"\"\"\n    Takes a list of lines representing a markdown file as input.\n    Finds the highest level of heading and returns it as an integer.\n    Returns None if the text contains no headings.\n\n    Source:\n        https://github.com/nestordemeure/question_extractor/blob/main/question_extractor/markdown.py\n\n    Args:\n        lines (list of str): A list of lines in the markdown file.\n\n    Returns:\n        int | None: The highest heading level as an integer, or None if no headings\n            are found.\n    \"\"\"\n    highest_heading_level = None\n    code_section = False\n\n    # Iterate through the lines in the markdown file\n    for line in lines:\n        \"\"\"\n        Check code section e.g.:\n            ```bash\n            # Trace an IP packet between two Pods\n            antctl trace-packet -S ns1/pod1 -D ns2/pod2\n            # Trace a Service request from a local Pod\n            antctl trace-packet -S ns1/pod1 -D ns2/svc2 -f \"tcp,tcp_dst=80\"\n            # Trace the Service reply packet (assuming \"ns2/pod2\" is the Service\n            # backend Pod)\n            antctl trace-packet -D ns1/pod1 -S ns2/pod2 -f \"tcp,tcp_src=80\"\n            # Trace an IP packet from a Pod to gateway port\n            antctl trace-packet -S ns1/pod1 -D antrea-gw0\n            # Trace a UDP packet from a Pod to an IP address\n            antctl trace-packet -S ns1/pod1 -D 10.1.2.3 -f udp,udp_dst=1234\n            # Trace a UDP packet from an IP address to a Pod\n            antctl trace-packet -D ns1/pod1 -S 10.1.2.3 -f udp,udp_src=1234\n            ```\n        Here # is a code comment not the md level symbole\n        \"\"\"\n        if line.startswith(\"```\"):\n            code_section = not code_section\n        # Check if the line starts with a heading\n        if line.startswith(\"#\") and not code_section:\n            # Calculate the heading level based on the number of '#' characters\n            current_heading_level = len(line.split()[0])\n\n            # Update the highest_heading_level if it is None or if the current_heading_\n            # level is higher\n            if (highest_heading_level is None) or (\n                current_heading_level &lt; highest_heading_level\n            ):\n                highest_heading_level = current_heading_level\n\n    return highest_heading_level\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.pdf_to_markdown","title":"pdf_to_markdown","text":"<pre><code>pdf_to_markdown(pdf_file, output_file, max_pages=None, parallel_factor=1, cache_dir=Path('.cache/pdf_to_markdown/'))\n</code></pre> <p>Converts a PDF file to Markdown format and saves the result to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_file</code> <code>Path</code> <p>The path to the PDF file to be converted.</p> required <code>output_file</code> <code>Path</code> <p>The path to the output file where the converted Markdown will be saved.</p> required <code>max_pages</code> <code>int | None</code> <p>The maximum number of pages to convert. Defaults to None.</p> <code>None</code> <code>parallel_factor</code> <code>int</code> <p>The number of parallel processes to use for conversion. Defaults to 1.</p> <code>1</code> <code>cache_dir</code> <code>Path</code> <p>The directory to use for caching the conversion</p> <code>Path('.cache/pdf_to_markdown/')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The converted Markdown text.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def pdf_to_markdown(\n    pdf_file: Path,\n    output_file: Path,\n    max_pages: int | None = None,\n    parallel_factor: int = 1,\n    cache_dir: Path = Path(\".cache/pdf_to_markdown/\"),\n) -&gt; str:\n    \"\"\"\n    Converts a PDF file to Markdown format and saves the result to an output file.\n\n    Args:\n        pdf_file (Path): The path to the PDF file to be converted.\n        output_file (Path): The path to the output file where the converted Markdown\n            will be saved.\n        max_pages (int | None, optional): The maximum number of pages to convert.\n            Defaults to None.\n        parallel_factor (int, optional): The number of parallel processes to use for\n            conversion. Defaults to 1.\n        cache_dir (Path, optional): The directory to use for caching the conversion\n\n    Returns:\n        str: The converted Markdown text.\n    \"\"\"\n    markdown_text, metadata = convert_single_pdf(\n        pdf_file,\n        model_lst=load_all_models(),\n        max_pages=max_pages,\n        parallel_factor=parallel_factor,\n        cache_dir=cache_dir,\n    )\n\n    if output_file is not None:\n        output_file = Path(output_file)\n        output_file.parent.mkdir(exist_ok=True, parents=True)\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(markdown_text)\n\n    return markdown_text\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.filter_empty_sections","title":"filter_empty_sections","text":"<pre><code>filter_empty_sections(sections)\n</code></pre> <p>Filters out empty sections from a list of tuples.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing sections, where each tuple contains a heading (str) and content (str).</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples representing non-empty sections, where each tuple contains a heading (str) and content (str).</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def filter_empty_sections(sections: list[tuple[str, str]]) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Filters out empty sections from a list of tuples.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing sections, where\n            each tuple contains a heading (str) and content (str).\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples representing non-empty sections, where\n            each tuple contains a heading (str) and content (str).\n    \"\"\"\n    return [(heading, content) for heading, content in sections if heading or content]\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.merge_abstract_with_previous_sections","title":"merge_abstract_with_previous_sections","text":"<pre><code>merge_abstract_with_previous_sections(sections)\n</code></pre> <p>If found an Abstract section then assume it's a research paper and merge it with all previous sections, this is because the authors section might have more column thus messes up the parsed order</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing sections, where each tuple contains a heading (str) and content (str).</p> required <p>Returns:</p> Type Description <p>list[tuple[str, str]]: A list of tuples representing merged sections, where each tuple contains a heading (str) and content (str).</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def merge_abstract_with_previous_sections(sections: list[tuple[str, str]]):\n    \"\"\"\n    If found an Abstract section then assume it's a research paper and merge it with\n    all previous sections, this is because the authors section might have more column\n    thus messes up the parsed order\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing sections, where\n            each tuple contains a heading (str) and content (str).\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples representing merged sections, where\n            each tuple contains a heading (str) and content (str).\n    \"\"\"\n\n    if len(sections) &lt; 2:\n        return sections\n\n    first_heading = sections[0][0]\n    text_sections = [sections[0][1]]\n\n    for i in range(1, len(sections)):\n        heading, content = sections[i]\n        current_text = f\"{heading}\\n\\n{content}\"\n        text_sections.append(current_text)\n        if re.sub(r\"[^a-zA-Z]\", \"\", heading).lower() == \"abstract\":\n            combined_text = \"\\n\\n\".join(text_sections)\n            return [(first_heading, combined_text)] + sections[i + 1 :]\n\n    return sections\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.preprocess_sections","title":"preprocess_sections","text":"<pre><code>preprocess_sections(sections)\n</code></pre> <p>Preprocesses the given list of sections by filtering out any empty sections and merging any abstract sections with their previous sections.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>List[Tuple[str, str]]</code> <p>A list of tuples representing sections. Each tuple contains two strings: the title of the section and the content of the section.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List[Tuple[str, str]]: A list of tuples representing the preprocessed sections. Each tuple contains two strings: the title of the section and the content of the section.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def preprocess_sections(sections: list[tuple[str, str]]) -&gt; list[tuple[str, str]]:\n    \"\"\"\n    Preprocesses the given list of sections by filtering out any empty sections and\n    merging any abstract sections with their previous sections.\n\n    Args:\n        sections (List[Tuple[str, str]]): A list of tuples representing sections.\n            Each tuple contains two strings: the title of the section and the content\n            of the section.\n\n    Returns:\n        List[Tuple[str, str]]: A list of tuples representing the preprocessed sections.\n            Each tuple contains two strings: the title of the section and the content\n            of the section.\n    \"\"\"\n    sections = filter_empty_sections(sections)\n    sections = merge_abstract_with_previous_sections(sections)\n\n    return sections\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.text_similarity_score","title":"text_similarity_score","text":"<pre><code>text_similarity_score(text1, text2)\n</code></pre> <p>Compute the similarity score between two texts.</p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>The first text.</p> required <code>text2</code> <code>str</code> <p>The second text.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The similarity score between the two texts.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def text_similarity_score(text1: str, text2: str) -&gt; float:\n    \"\"\"\n    Compute the similarity score between two texts.\n\n    Args:\n        text1 (str): The first text.\n        text2 (str): The second text.\n\n    Returns:\n        float: The similarity score between the two texts.\n    \"\"\"\n    # remove special characters\n    text1 = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text1.lower())\n    text2 = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text2.lower())\n\n    # split into words, the words are sorted to allow shuffling of content\n    text1_words = sorted(text1.split())\n    text2_words = sorted(text2.split())\n\n    return 1 - editdistance.eval(text1_words, text2_words) / (\n        max(len(text1_words), len(text2_words)) + 1e-6\n    )\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.heading_similarity_score","title":"heading_similarity_score","text":"<pre><code>heading_similarity_score(heading1, heading2)\n</code></pre> <p>Calculate the similarity score between two headings.</p> <p>Parameters:</p> Name Type Description Default <code>heading1</code> <code>str</code> <p>The first heading.</p> required <code>heading2</code> <code>str</code> <p>The second heading.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The similarity score between the two headings.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def heading_similarity_score(heading1: str, heading2: str) -&gt; float:\n    \"\"\"\n    Calculate the similarity score between two headings.\n\n    Parameters:\n        heading1 (str): The first heading.\n        heading2 (str): The second heading.\n\n    Returns:\n        float: The similarity score between the two headings.\n    \"\"\"\n    return 1 - editdistance.eval(heading1, heading2) / (\n        max(len(heading1), len(heading2)) + 1e-6\n    )\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.preserve_content","title":"preserve_content","text":"<pre><code>preserve_content(heading, old_content, new_text, heading_similarity_threshold=0.7, content_similarity_threshold=0.8)\n</code></pre> <p>Calculate the similarity between the given heading and new heading using a threshold. If the similarity score is above the threshold, the new text still contains the heading, so the content after the heading is extracted as the new content. If the similarity score is below the threshold, the new text does not contain the heading, so the entire new text is considered as the new content. Calculate the similarity between the old content and new content using a threshold. If the similarity score is above the threshold, the content is considered preserved and the new content along with its similarity score is returned. If the similarity score is below the threshold, the content has been modified too much and the old content along with its similarity score is returned.</p> <p>Parameters:</p> Name Type Description Default <code>heading</code> <code>str</code> <p>The heading of the old text.</p> required <code>old_content</code> <code>str</code> <p>The content of the old text.</p> required <code>new_text</code> <code>str</code> <p>The new text.</p> required <code>heading_similarity_threshold</code> <code>float</code> <p>The threshold for heading similarity. Defaults to 0.7.</p> <code>0.7</code> <code>content_similarity_threshold</code> <code>float</code> <p>The threshold for content similarity. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[str, float]</code> <p>tuple[str, float]: A tuple containing the new content and its similarity score.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def preserve_content(\n    heading: str,\n    old_content: str,\n    new_text: str,\n    heading_similarity_threshold: float = 0.7,\n    content_similarity_threshold: float = 0.8,\n) -&gt; tuple[str, float]:\n    \"\"\"\n    Calculate the similarity between the given heading and new heading using a\n    threshold. If the similarity score is above the threshold, the new text\n    still contains the heading, so the content after the heading is extracted as the new\n    content. If the similarity score is below the threshold, the new text does\n    not contain the heading, so the entire new text is considered as the new content.\n    Calculate the similarity between the old content and new content using a threshold.\n    If the similarity score is above the threshold, the content is considered preserved\n    and the new content along with its similarity score is returned.\n    If the similarity score is below the threshold, the content has been modified too\n    much and the old content along with its similarity score is returned.\n\n    Args:\n        heading (str): The heading of the old text.\n        old_content (str): The content of the old text.\n        new_text (str): The new text.\n        heading_similarity_threshold (float, optional): The threshold for heading\n            similarity. Defaults to 0.7.\n        content_similarity_threshold (float, optional): The threshold for content\n            similarity. Defaults to 0.8.\n\n    Returns:\n        tuple[str, float]: A tuple containing the new content and its similarity score.\n    \"\"\"\n    parts = new_text.split(\"\\n\")\n    new_heading = parts[0]\n\n    heading_similarity = heading_similarity_score(heading, new_heading)\n    if heading_similarity &gt;= heading_similarity_threshold:\n        # new text still contains heading\n        new_content = \"\\n\".join(parts[1:])\n    else:\n        # new text does not contain heading\n        new_content = new_text\n\n    content_similarity = text_similarity_score(old_content, new_content)\n    if content_similarity &gt;= content_similarity_threshold:\n        # content is still preserved\n        return new_content, content_similarity\n    else:\n        # content has been modified too much\n        return old_content, content_similarity\n</code></pre>"},{"location":"reference/core/markdown/#core.markdown.tidy_markdown_sections","title":"tidy_markdown_sections","text":"<pre><code>tidy_markdown_sections(sections, max_length=4096, openai_key='', openai_model='', seed=42, heading_similarity_threshold=0.7, content_similarity_threshold=0.8)\n</code></pre> <p>Tidies up sections of markdown text by splitting them into heading and content, and then processing each section using the MarkdownTidier class. It takes a list of tuples representing the sections, where each tuple contains a heading and content. The function also accepts optional parameters such as the maximum length of the tidied sections, the OpenAI API key, the OpenAI model to use, a seed value for reproducibility, and thresholds for heading and content similarity.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>list[tuple[str, str]]</code> <p>A list of tuples representing the sections of markdown text. Each tuple contains a heading and content.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the tidied sections. Defaults to 4096.</p> <code>4096</code> <code>openai_key</code> <code>str</code> <p>The OpenAI API key. Defaults to \"\".</p> <code>''</code> <code>openai_model</code> <code>str</code> <p>The OpenAI model to use. Defaults to \"\".</p> <code>''</code> <code>seed</code> <code>int</code> <p>A seed value for reproducibility. Defaults to 42.</p> <code>42</code> <code>heading_similarity_threshold</code> <code>float</code> <p>The threshold for heading similarity. Defaults to 0.7.</p> <code>0.7</code> <code>content_similarity_threshold</code> <code>float</code> <p>The threshold for content similarity. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[list[tuple[str, str]], list[dict]]</code> <p>tuple[list[tuple[str, str]], list[dict]]: A tuple containing the tidied sections and a list of metadata for each section.</p> Source code in <code>docqa/core/markdown.py</code> <pre><code>def tidy_markdown_sections(\n    sections: list[tuple[str, str]],\n    max_length: int = 4096,\n    openai_key: str = \"\",\n    openai_model: str = \"\",\n    seed: int = 42,\n    heading_similarity_threshold: float = 0.7,\n    content_similarity_threshold: float = 0.8,\n) -&gt; tuple[list[tuple[str, str]], list[dict]]:\n    \"\"\"\n    Tidies up sections of markdown text by splitting them into heading and content, and\n    then processing each section using the MarkdownTidier class. It takes a list of\n    tuples representing the sections, where each tuple contains a heading and\n    content. The function also accepts optional parameters such as the maximum\n    length of the tidied sections, the OpenAI API key, the OpenAI model to use, a\n    seed value for reproducibility, and thresholds for heading and content similarity.\n\n    Args:\n        sections (list[tuple[str, str]]): A list of tuples representing the sections of\n            markdown text. Each tuple contains a heading and content.\n        max_length (int, optional): The maximum length of the tidied sections.\n            Defaults to 4096.\n        openai_key (str, optional): The OpenAI API key. Defaults to \"\".\n        openai_model (str, optional): The OpenAI model to use. Defaults to \"\".\n        seed (int, optional): A seed value for reproducibility. Defaults to 42.\n        heading_similarity_threshold (float, optional): The threshold for heading\n            similarity. Defaults to 0.7.\n        content_similarity_threshold (float, optional): The threshold for content\n            similarity. Defaults to 0.8.\n\n    Returns:\n        tuple[list[tuple[str, str]], list[dict]]: A tuple containing the tidied\n            sections and a list of metadata for each section.\n    \"\"\"\n    tidier = MarkdownTidier(openai_key=openai_key, openai_model=openai_model, seed=seed)\n    encoding = tiktoken.encoding_for_model(tidier.openai_model)\n\n    tidy_sections = []\n    all_metadata: list[dict] = []\n    for heading, content in sections:\n        print(\"Tidying:\", heading)\n        section_text = f\"{heading}\\n\\n{content}\"\n        if len(encoding.encode(section_text)) &gt; max_length:\n            tidy_sections.append((heading, content))\n            all_metadata.append({})\n        else:\n            new_section_text, metadata = tidier.process(section_text)\n            new_content, similarty = preserve_content(\n                heading,\n                content,\n                new_section_text,\n                heading_similarity_threshold=heading_similarity_threshold,\n                content_similarity_threshold=content_similarity_threshold,\n            )\n            print(\"\\tcontent similarity:\", similarty)\n            tidy_sections.append((heading, new_content))\n            all_metadata.append(metadata)\n\n    return tidy_sections, all_metadata\n</code></pre>"},{"location":"reference/core/retrieval/","title":"Retrieval","text":""},{"location":"reference/core/retrieval/#core.retrieval.SemanticRetriever","title":"SemanticRetriever","text":"<p>             Bases: <code>BaseModel</code></p> <p>SemanticRetriever class for retrieving documents based on embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>Any</code> <p>The embedding model used to encode the corpus.</p> required <code>vector_db</code> <code>Collection</code> <p>The Chroma vector database.</p> required Source code in <code>docqa/core/retrieval.py</code> <pre><code>class SemanticRetriever(BaseModel):\n    \"\"\"\n    SemanticRetriever class for retrieving documents based on embeddings.\n\n    Args:\n        embedding_model (Any): The embedding model used to encode the corpus.\n        vector_db (chromadb.Collection): The Chroma vector database.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    embedding_model: AnglE\n    vector_db: chromadb.Collection\n\n    def process(\n        self, query: str, top_k: int, metadata_filter: dict | None = None\n    ) -&gt; list[dict]:\n        \"\"\"\n        Process the given query to retrieve the top-k results from the vector database.\n\n        Args:\n            query (str): The query string.\n            top_k (int): The number of results to retrieve.\n            metadata_filter (dict | None, optional): A dictionary specifying metadata\n                filters. Defaults to None.\n\n        Returns:\n            list[dict]: The list of retrieved results.\n        \"\"\"\n        query_embeddings = self.embedding_model.encode({\"text\": query})\n\n        results = self.vector_db.query(\n            query_embeddings=query_embeddings, n_results=top_k, where=metadata_filter\n        )\n\n        output = []\n        for i in range(len(results[\"ids\"][0])):\n            score = 1 - results[\"distances\"][0][i]\n            document = results[\"documents\"][0][i]\n            metadata = results[\"metadatas\"][0][i]\n            output.append({\"score\": score, \"document\": document, \"metadata\": metadata})\n\n        return output\n</code></pre>"},{"location":"reference/core/retrieval/#core.retrieval.SemanticRetriever.process","title":"process","text":"<pre><code>process(query, top_k, metadata_filter=None)\n</code></pre> <p>Process the given query to retrieve the top-k results from the vector database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query string.</p> required <code>top_k</code> <code>int</code> <p>The number of results to retrieve.</p> required <code>metadata_filter</code> <code>dict | None</code> <p>A dictionary specifying metadata filters. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The list of retrieved results.</p> Source code in <code>docqa/core/retrieval.py</code> <pre><code>def process(\n    self, query: str, top_k: int, metadata_filter: dict | None = None\n) -&gt; list[dict]:\n    \"\"\"\n    Process the given query to retrieve the top-k results from the vector database.\n\n    Args:\n        query (str): The query string.\n        top_k (int): The number of results to retrieve.\n        metadata_filter (dict | None, optional): A dictionary specifying metadata\n            filters. Defaults to None.\n\n    Returns:\n        list[dict]: The list of retrieved results.\n    \"\"\"\n    query_embeddings = self.embedding_model.encode({\"text\": query})\n\n    results = self.vector_db.query(\n        query_embeddings=query_embeddings, n_results=top_k, where=metadata_filter\n    )\n\n    output = []\n    for i in range(len(results[\"ids\"][0])):\n        score = 1 - results[\"distances\"][0][i]\n        document = results[\"documents\"][0][i]\n        metadata = results[\"metadatas\"][0][i]\n        output.append({\"score\": score, \"document\": document, \"metadata\": metadata})\n\n    return output\n</code></pre>"},{"location":"reference/demo/","title":"Demo","text":""},{"location":"reference/demo/config/","title":"Config","text":""},{"location":"reference/demo/create_dataset/","title":"Create dataset","text":""},{"location":"reference/demo/create_dataset/#demo.create_dataset.create_openai_dataset","title":"create_openai_dataset","text":"<pre><code>create_openai_dataset(sections_qa_data_flatten, section_type='main', question_type='dense', answer_type='long', prompt_type='instruction')\n</code></pre> <p>Generate a dataset for OpenAI based on the given sections QA data.</p> <p>Parameters:</p> Name Type Description Default <code>sections_qa_data_flatten</code> <code>dict</code> <p>A dictionary containing the flattened sections QA data.</p> required <code>section_type</code> <code>Literal['main', 'summary', 'metadata', 'extra']</code> <p>The type of section to include in the dataset. Defaults to \"main\".</p> <code>'main'</code> <code>question_type</code> <code>Literal['dense', 'sparse']</code> <p>The type of question to include in the dataset. Defaults to \"dense\".</p> <code>'dense'</code> <code>answer_type</code> <code>Literal['long', 'short']</code> <p>The type of answer to include in the dataset. Defaults to \"long\".</p> <code>'long'</code> <code>prompt_type</code> <code>Literal['instruction', 'simple']</code> <p>The type of prompt to use in the dataset. Defaults to \"instruction\".</p> <code>'instruction'</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The generated dataset for OpenAI.</p> Note <ul> <li>The dataset is generated based on the specified parameters.</li> <li>Only sections that exist in the sections QA data will be included in the dataset.</li> <li>For each section, the questions and answers are extracted based on the question type and answer type.</li> <li>Depending on the prompt type, different sample generation functions are used to create the samples.</li> <li>The dataset is a list of dictionaries, where each dictionary represents a sample.</li> </ul> Source code in <code>docqa/demo/create_dataset.py</code> <pre><code>def create_openai_dataset(\n    sections_qa_data_flatten: dict,\n    section_type: Literal[\n        \"main\", \"summary\", \"metadata\", \"extra\"\n    ] = \"main\",  # keys in SECTIONS\n    question_type: Literal[\"dense\", \"sparse\"] = \"dense\",\n    answer_type: Literal[\"long\", \"short\"] = \"long\",\n    prompt_type: Literal[\"instruction\", \"simple\"] = \"instruction\",\n) -&gt; list[dict]:\n    \"\"\"\n    Generate a dataset for OpenAI based on the given sections QA data.\n\n    Parameters:\n        sections_qa_data_flatten (dict): A dictionary containing the flattened sections\n            QA data.\n        section_type (Literal[\"main\", \"summary\", \"metadata\", \"extra\"], optional): The\n            type of section to include in the dataset. Defaults to \"main\".\n        question_type (Literal[\"dense\", \"sparse\"], optional): The type of question to\n            include in the dataset. Defaults to \"dense\".\n        answer_type (Literal[\"long\", \"short\"], optional): The type of answer to include\n            in the dataset. Defaults to \"long\".\n        prompt_type (Literal[\"instruction\", \"simple\"], optional): The type of prompt to\n            use in the dataset. Defaults to \"instruction\".\n\n    Returns:\n        list[dict]: The generated dataset for OpenAI.\n\n    Note:\n        - The dataset is generated based on the specified parameters.\n        - Only sections that exist in the sections QA data will be included in the\n        dataset.\n        - For each section, the questions and answers are extracted based on the\n        question type and answer type.\n        - Depending on the prompt type, different sample generation functions are used\n        to create the samples.\n        - The dataset is a list of dictionaries, where each dictionary represents a\n        sample.\n    \"\"\"\n    dataset = []\n    for heading in SECTIONS[section_type]:\n        if heading not in sections_qa_data_flatten:\n            continue\n        section = sections_qa_data_flatten[heading]\n        qa_list = section[f\"{question_type}_questions\"]\n        for item in qa_list:\n            question = item[\"question\"]\n            answer = item[\"answer\"] if answer_type == \"short\" else item[\"long_answer\"]\n            if prompt_type == \"simple\":\n                sample = make_simple_sample_for_openai(question, answer)\n            elif prompt_type == \"instruction\":\n                reference = f\"[source: {heading}]\\n{section['text']}\\n\"\n                sample = make_instruction_sample_for_openai(\n                    question=question,\n                    answer=answer,\n                    references=[reference],\n                )\n            dataset.append(sample)\n\n    return dataset\n</code></pre>"},{"location":"reference/demo/create_dataset/#demo.create_dataset.pdf_to_qa_data","title":"pdf_to_qa_data","text":"<pre><code>pdf_to_qa_data(output_dir, pdf_file)\n</code></pre> <p>Generates a QA data dictionary from a PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>The directory where the output files will be saved.</p> required <code>pdf_file</code> <code>Path</code> <p>The path to the PDF file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The generated QA data dictionary.</p> Source code in <code>docqa/demo/create_dataset.py</code> <pre><code>def pdf_to_qa_data(output_dir: Path, pdf_file: Path) -&gt; dict:\n    \"\"\"\n    Generates a QA data dictionary from a PDF file.\n\n    Args:\n        output_dir (Path): The directory where the output files will be saved.\n        pdf_file (Path): The path to the PDF file.\n\n    Returns:\n        dict: The generated QA data dictionary.\n    \"\"\"\n    doc_tree_file = output_dir / \"doc_tree.json\"\n\n    if doc_tree_file.exists():\n        with open(doc_tree_file, \"r\", encoding=\"utf-8\") as f:\n            doc_tree = json.load(f)\n    else:\n        doc_tree = build_doc_tree_from_pdf(pdf_file, output_dir=output_dir)\n\n    top_sections_qa_data = generate_top_sections_questions(\n        doc_tree,\n        output_file=output_dir / \"top_sections_qa_data.json\",\n        openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n        openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n        seed=int(os.getenv(\"SEED\", 42)),\n        temperature=1.0,\n    )\n    top_sections_qa_data = generate_long_answers_for_sections_questions(\n        top_sections_qa_data,\n        output_file=output_dir / \"top_sections_qa_data_long_answers.json\",\n        openai_key=os.getenv(\"OPENAI_API_KEY\", \"\"),\n        openai_model=os.getenv(\"OPENAI_MODEL\", \"\"),\n        seed=int(os.getenv(\"SEED\", 42)),\n        temperature=1.0,\n    )\n\n    return top_sections_qa_data\n</code></pre>"},{"location":"reference/demo/finetune_openai/","title":"Finetune openai","text":""},{"location":"reference/demo/pipeline/","title":"Pipeline","text":""},{"location":"reference/demo/pipeline/#demo.pipeline.Pipeline","title":"Pipeline","text":"<p>             Bases: <code>BaseModel</code></p> <p>Pipeline class for the demo.</p> <p>Parameters:</p> Name Type Description Default <code>retriever</code> <code>SemanticRetriever</code> <p>The semantic retriever.</p> required <code>answerer</code> <code>AnswerGenerator</code> <p>The answer generator.</p> required <code>sections_map</code> <code>dict</code> <p>The mapping of section headings to their content.</p> required Source code in <code>docqa/demo/pipeline.py</code> <pre><code>class Pipeline(BaseModel):\n    \"\"\"\n    Pipeline class for the demo.\n\n    Args:\n        retriever (SemanticRetriever): The semantic retriever.\n        answerer (AnswerGenerator): The answer generator.\n        sections_map (dict): The mapping of section headings to their content.\n    \"\"\"\n\n    retriever: SemanticRetriever\n    answerer: AnswerGenerator\n    sections_map: dict\n\n    def process(\n        self,\n        question: str,\n        certainty_threshold: float = 0.9,\n        uncertainty_threshold: float = 0.6,\n        temperature: float = 1.0,\n    ) -&gt; PipelineOutput:\n        \"\"\"\n        Processes a question and returns the answer along with related references and\n            metadata.\n\n        Args:\n            question (str): The question to process.\n            certainty_threshold (float, optional): The threshold for considering a\n                question as certain. Defaults to 0.9.\n            uncertainty_threshold (float, optional): The threshold for considering a\n                question as uncertain. Defaults to 0.6.\n            temperature (float, optional): The temperature parameter for generating the\n                answer. Defaults to 1.0.\n\n        Returns:\n            dict: A dictionary containing the answer, references, and metadata.\n\n        Output dict format:\n            - answer (str): The answer to the question.\n            - references (list): A list of dictionaries containing the related\n                references.\n                - source (str): The source of the reference.\n                - content (str): The content of the reference.\n            - metadata (dict): Additional metadata associated with the answer.\n        \"\"\"\n        similar_questions = self.retriever.process(\n            question, top_k=1, metadata_filter={\"type\": \"question\"}\n        )\n        question_similarity = similar_questions[0][\"score\"]\n\n        if question_similarity &gt; certainty_threshold:\n            related_section = similar_questions[0][\"metadata\"][\"source\"]\n            related_content = self.sections_map[related_section]\n            return PipelineOutput(\n                answer=similar_questions[0][\"metadata\"][\"answer\"],\n                references=[{\"source\": related_section, \"content\": related_content}],\n            )\n\n        related_chunks = self.retriever.process(\n            question, top_k=3, metadata_filter={\"type\": \"chunk\"}\n        )\n        chunks_similarity = np.mean([each[\"score\"] for each in related_chunks])\n\n        if (\n            question_similarity &lt; uncertainty_threshold\n            and chunks_similarity &lt; uncertainty_threshold\n        ):\n            references = []\n            references_text = \"No related references found.\"\n        elif question_similarity &gt;= chunks_similarity:\n            related_section = similar_questions[0][\"metadata\"][\"source\"]\n            related_content = self.sections_map[related_section]\n            references = [{\"source\": related_section, \"content\": related_content}]\n            references_text = f\"{related_section}\\n\\n{related_content}\"\n        else:\n            references = [\n                {\"source\": each[\"metadata\"][\"source\"], \"content\": each[\"document\"]}\n                for each in related_chunks\n            ]\n            references_text = (\"-\" * 6).join(\n                [\n                    f\"From: {each['source']}\\n...\\n{each['content']}\\n...\\n\"\n                    for each in references\n                ]\n            )\n\n        answer, metadata = self.answerer.process(\n            question, references_text, temperature=temperature\n        )\n\n        return PipelineOutput(\n            answer=answer,\n            references=[RetrievalReference.model_validate(each) for each in references],\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.Pipeline.process","title":"process","text":"<pre><code>process(question, certainty_threshold=0.9, uncertainty_threshold=0.6, temperature=1.0)\n</code></pre> <p>Processes a question and returns the answer along with related references and     metadata.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to process.</p> required <code>certainty_threshold</code> <code>float</code> <p>The threshold for considering a question as certain. Defaults to 0.9.</p> <code>0.9</code> <code>uncertainty_threshold</code> <code>float</code> <p>The threshold for considering a question as uncertain. Defaults to 0.6.</p> <code>0.6</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for generating the answer. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>PipelineOutput</code> <p>A dictionary containing the answer, references, and metadata.</p> Output dict format <ul> <li>answer (str): The answer to the question.</li> <li>references (list): A list of dictionaries containing the related     references.<ul> <li>source (str): The source of the reference.</li> <li>content (str): The content of the reference.</li> </ul> </li> <li>metadata (dict): Additional metadata associated with the answer.</li> </ul> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def process(\n    self,\n    question: str,\n    certainty_threshold: float = 0.9,\n    uncertainty_threshold: float = 0.6,\n    temperature: float = 1.0,\n) -&gt; PipelineOutput:\n    \"\"\"\n    Processes a question and returns the answer along with related references and\n        metadata.\n\n    Args:\n        question (str): The question to process.\n        certainty_threshold (float, optional): The threshold for considering a\n            question as certain. Defaults to 0.9.\n        uncertainty_threshold (float, optional): The threshold for considering a\n            question as uncertain. Defaults to 0.6.\n        temperature (float, optional): The temperature parameter for generating the\n            answer. Defaults to 1.0.\n\n    Returns:\n        dict: A dictionary containing the answer, references, and metadata.\n\n    Output dict format:\n        - answer (str): The answer to the question.\n        - references (list): A list of dictionaries containing the related\n            references.\n            - source (str): The source of the reference.\n            - content (str): The content of the reference.\n        - metadata (dict): Additional metadata associated with the answer.\n    \"\"\"\n    similar_questions = self.retriever.process(\n        question, top_k=1, metadata_filter={\"type\": \"question\"}\n    )\n    question_similarity = similar_questions[0][\"score\"]\n\n    if question_similarity &gt; certainty_threshold:\n        related_section = similar_questions[0][\"metadata\"][\"source\"]\n        related_content = self.sections_map[related_section]\n        return PipelineOutput(\n            answer=similar_questions[0][\"metadata\"][\"answer\"],\n            references=[{\"source\": related_section, \"content\": related_content}],\n        )\n\n    related_chunks = self.retriever.process(\n        question, top_k=3, metadata_filter={\"type\": \"chunk\"}\n    )\n    chunks_similarity = np.mean([each[\"score\"] for each in related_chunks])\n\n    if (\n        question_similarity &lt; uncertainty_threshold\n        and chunks_similarity &lt; uncertainty_threshold\n    ):\n        references = []\n        references_text = \"No related references found.\"\n    elif question_similarity &gt;= chunks_similarity:\n        related_section = similar_questions[0][\"metadata\"][\"source\"]\n        related_content = self.sections_map[related_section]\n        references = [{\"source\": related_section, \"content\": related_content}]\n        references_text = f\"{related_section}\\n\\n{related_content}\"\n    else:\n        references = [\n            {\"source\": each[\"metadata\"][\"source\"], \"content\": each[\"document\"]}\n            for each in related_chunks\n        ]\n        references_text = (\"-\" * 6).join(\n            [\n                f\"From: {each['source']}\\n...\\n{each['content']}\\n...\\n\"\n                for each in references\n            ]\n        )\n\n    answer, metadata = self.answerer.process(\n        question, references_text, temperature=temperature\n    )\n\n    return PipelineOutput(\n        answer=answer,\n        references=[RetrievalReference.model_validate(each) for each in references],\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.create_chroma_db","title":"create_chroma_db","text":"<pre><code>create_chroma_db(data_dir, db_dir, collection_name, embedding_model)\n</code></pre> <p>Creates a Chroma database given the data directory, database directory, collection     name, and embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The directory containing the data files.</p> required <code>db_dir</code> <code>Path</code> <p>The directory where the database will be created.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection in the database.</p> required <code>embedding_model</code> <code>Any</code> <p>The embedding model used to encode the corpus.</p> required <p>Returns:</p> Type Description <code>Collection</code> <p>chromadb.Collection: The created Chroma database.</p> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def create_chroma_db(\n    data_dir: Path, db_dir: Path, collection_name: str, embedding_model: Any\n) -&gt; chromadb.Collection:\n    \"\"\"\n    Creates a Chroma database given the data directory, database directory, collection\n        name, and embedding model.\n\n    Args:\n        data_dir (Path): The directory containing the data files.\n        db_dir (Path): The directory where the database will be created.\n        collection_name (str): The name of the collection in the database.\n        embedding_model (Any): The embedding model used to encode the corpus.\n\n    Returns:\n        chromadb.Collection: The created Chroma database.\n    \"\"\"\n    corpus = []\n    metadatas = []\n\n    top_sections_qa_data_file = data_dir / \"top_sections_qa_data.json\"\n    with open(top_sections_qa_data_file) as f:\n        qa_data = json.load(f)\n\n    allowed_sections = set(\n        sum([SECTIONS[section_type] for section_type in [\"main\", \"summary\"]], [])\n    )\n\n    for heading, section in qa_data.items():\n        if heading not in allowed_sections:\n            continue\n        for item in section[\"dense_questions\"] + section[\"sparse_questions\"]:\n            corpus.append(item[\"question\"])\n            metadatas.append(\n                {\"type\": \"question\", \"source\": heading, \"answer\": item[\"answer\"]}\n            )\n\n    doc_tree_file = data_dir / \"doc_tree.json\"\n    with open(doc_tree_file) as f:\n        doc_tree = json.load(f)\n    all_sections_map = {heading: text for heading, text in flatten_doc_tree(doc_tree)}\n\n    for heading in all_sections_map.keys():\n        if heading not in allowed_sections:\n            continue\n        section_chunks = chunk_content(all_sections_map[heading])\n        corpus.extend(section_chunks)\n        metadatas.extend([{\"type\": \"chunk\", \"source\": heading} for _ in section_chunks])\n\n    print(\"Creating chroma db...\")\n    client = chromadb.PersistentClient(\n        str(db_dir), Settings(anonymized_telemetry=False)\n    )\n    db = client.create_collection(\n        name=collection_name, metadata={\"hnsw:space\": \"cosine\"}\n    )\n    print(\"Finish creating chroma db.\")\n\n    print(\"Embedding corpus...\")\n    # one by one because my machine does not have much ram\n    corpus_embeddings = [embedding_model.encode({\"text\": each})[0] for each in corpus]\n    corpus_embeddings = np.vstack(corpus_embeddings)\n    print(\"Finish embedding corpus.\")\n\n    print(\"Populating chroma db...\")\n    db.add(\n        documents=corpus,\n        embeddings=corpus_embeddings,\n        metadatas=metadatas,\n        ids=[str(i) for i in range(len(corpus))],\n    )\n    print(\"Finish populating chroma db.\")\n\n    return db\n</code></pre>"},{"location":"reference/demo/pipeline/#demo.pipeline.get_pipeline","title":"get_pipeline","text":"<pre><code>get_pipeline(data_dir, openai_key, openai_model)\n</code></pre> <p>Initializes and returns a pipeline for processing text-based questions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The directory containing the data files.</p> required <code>openai_key</code> <code>str</code> <p>The API key for OpenAI.</p> required <code>openai_model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>The initialized pipeline for processing text-based questions.</p> Source code in <code>docqa/demo/pipeline.py</code> <pre><code>def get_pipeline(data_dir: Path, openai_key: str, openai_model: str) -&gt; Pipeline:\n    \"\"\"\n    Initializes and returns a pipeline for processing text-based questions.\n\n    Args:\n        data_dir (Path): The directory containing the data files.\n        openai_key (str): The API key for OpenAI.\n        openai_model (str): The name of the OpenAI model to use.\n\n    Returns:\n        Pipeline: The initialized pipeline for processing text-based questions.\n    \"\"\"\n    doc_tree_file = data_dir / \"doc_tree.json\"\n    with open(doc_tree_file) as f:\n        doc_tree = json.load(f)\n    all_sections_map = {heading: text for heading, text in flatten_doc_tree(doc_tree)}\n\n    print(\"Loading embedding model...\")\n    embedding_model = AnglE.from_pretrained(\n        \"WhereIsAI/UAE-Large-V1\", pooling_strategy=\"cls\"\n    )\n    embedding_model.set_prompt(prompt=Prompts.C)\n    print(\"Finish loading embedding model.\")\n\n    db_dir = data_dir / \"chroma\"\n    db_collection_name = \"generative-agents\"\n\n    try:\n        chroma_client = chromadb.PersistentClient(\n            str(db_dir), Settings(anonymized_telemetry=False)\n        )\n        db = chroma_client.get_collection(name=db_collection_name)\n    except ValueError:\n        db = create_chroma_db(data_dir, db_dir, db_collection_name, embedding_model)\n\n    retriever = SemanticRetriever(embedding_model=embedding_model, vector_db=db)\n    answerer = AnswerGenerator(\n        openai_key=openai_key,\n        openai_model=openai_model,\n        instruction=(\n            \"You will be answering questions about the paper called 'Generative\"\n            \" Agents'.\\nInstructions:\\n- Find some references in the paper that related\"\n            \" to the question.\\n- If you found related references, answer the question\"\n            \" as detailed as possible based strictly on that references you found.\\n-\"\n            \" If you can't answer the question using the references, say you can't find\"\n            \" sufficient information to answer the question.\\n- If the question is not\"\n            \" related to the references or there is no reference found, say the\"\n            \" question is irrelevant to the paper and answer the question as\"\n            \" a normal chatbot.\\n\\nReferences you found:\\n\\n{reference}\\n\\nQuestion:\"\n            \" {question}\\nAnswer:\"\n        ),\n    )\n\n    pipeline = Pipeline(\n        retriever=retriever,\n        answerer=answerer,\n        sections_map=all_sections_map,\n    )\n\n    return pipeline\n</code></pre>"}]}